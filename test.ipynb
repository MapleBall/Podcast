{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de93357b-591e-4c95-95ee-04c231e4014d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "import heapq\n",
    "\n",
    "def create_embeddings(use_cpu=False):\n",
    "    device = \"cpu\" if use_cpu else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    return HuggingFaceEmbeddings(\n",
    "        model_name=\"BAAI/bge-m3\",\n",
    "        model_kwargs={'device': device}\n",
    "    )\n",
    "\n",
    "def load_FAISS_vectorstore(vectorstore_path, embeddings):\n",
    "    if os.path.exists(vectorstore_path):\n",
    "        try:\n",
    "            vectorstore = FAISS.load_local(vectorstore_path, embeddings, allow_dangerous_deserialization=True)\n",
    "            print(f\"Loaded vector store from {vectorstore_path}\")\n",
    "            return vectorstore\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading vector store from {vectorstore_path}: {str(e)}\")\n",
    "    else:\n",
    "        print(f\"Vector store not found at {vectorstore_path}\")\n",
    "    return None\n",
    "\n",
    "def load_vectorstores_from_directory(parent_directory, embeddings):\n",
    "    vectorstores = []\n",
    "    for root, dirs, files in os.walk(parent_directory):\n",
    "        if 'index.faiss' in files and 'index.pkl' in files:\n",
    "            vectorstore_path = root\n",
    "            vs = load_FAISS_vectorstore(vectorstore_path, embeddings)\n",
    "            if vs:\n",
    "                vectorstores.append(vs)\n",
    "    return vectorstores\n",
    "\n",
    "def retrieve_from_multiple_stores(vectorstores, query, k=5, fetch_k=20):\n",
    "    all_results = []\n",
    "    for vs in vectorstores:\n",
    "        # 使用 max_marginal_relevance_search 來獲取多樣化的結果\n",
    "        results = vs.max_marginal_relevance_search(query, k=fetch_k, fetch_k=fetch_k)\n",
    "        # 對這些結果進行評分\n",
    "        scored_results = vs.similarity_search_with_score(query, k=len(results))\n",
    "        all_results.extend(scored_results)\n",
    "    \n",
    "    # 根據分數對所有結果進行排序，選擇前 k 個\n",
    "    return sorted(all_results, key=lambda x: x[1])[:k]\n",
    "\n",
    "# def main():\n",
    "#     # 設置參數\n",
    "#     parent_directory = \"/media/starklab/BACKUP/向量庫\"  # 母資料夾路徑 /media/starklab/BACKUP/向量庫\n",
    "#     k = 5  # 總共返回的文檔數量\n",
    "#     fetch_k = 100  # 每個向量庫初始檢索的文檔數量\n",
    "#     use_cpu = True  # 設置為 True 以使用 CPU\n",
    "\n",
    "#     # 創建 embeddings 對象\n",
    "#     embeddings = create_embeddings(use_cpu)\n",
    "\n",
    "#     # 加載母資料夾中的所有向量存儲\n",
    "#     vectorstores = load_vectorstores_from_directory(parent_directory, embeddings)\n",
    "#     if not vectorstores:\n",
    "#         print(\"No vector stores loaded. Exiting.\")\n",
    "#         return\n",
    "\n",
    "#     # 自定問題\n",
    "#     query = \"幫我優化這個問題(請問台美文化的差異？)\" #請問明月堂現在是第幾代接班？\n",
    "\n",
    "#     # 執行檢索\n",
    "#     results = retrieve_from_multiple_stores(vectorstores, query, k, fetch_k)\n",
    "\n",
    "#     # 輸出檢索結果\n",
    "#     print(f\"\\n檢索結果 for query: '{query}'\\n\")\n",
    "#     for idx, (doc, score) in enumerate(results):\n",
    "#         retrieval_filename = f\"{os.path.splitext(doc.metadata['episode_name'])[0]}_節目名稱{doc.metadata['Podcast_name']}\"\n",
    "#         print(f\"Result {idx+1}:\")\n",
    "#         print(f\"  檔案名稱: {retrieval_filename}\")\n",
    "#         print(f\"  相似度分數: {score:.4f}\")\n",
    "#         print(f\"  內容摘要: {doc.page_content[:100]}...\\n\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50838e12-aa27-4b48-aca7-0be448691d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded vector store from /media/starklab/BACKUP/向量庫/科技浪 Techwav\n",
      "Loaded vector store from /media/starklab/BACKUP/向量庫/史塔克實驗室\n",
      "Loaded vector store from /media/starklab/BACKUP/向量庫/老高與小茉 Mr & Mrs Gao\n",
      "Loaded vector store from /media/starklab/BACKUP/向量庫/寧可當吃貨\n",
      "Loaded vector store from /media/starklab/BACKUP/向量庫/週報時光機（生活歷史、冷知識)\n",
      "Loaded vector store from /media/starklab/BACKUP/向量庫/古今中歪\n",
      "Loaded vector store from /media/starklab/BACKUP/向量庫/屎作勇者\n",
      "Loaded vector store from /media/starklab/BACKUP/向量庫/跳脫Do式圈\n",
      "Loaded vector store from /media/starklab/BACKUP/向量庫/A’s 對話日誌\n",
      "Loaded vector store from /media/starklab/BACKUP/向量庫/Joe &amp; Jet 未過濾的 with Jason\n",
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://a2cdbc98b19eae6ad0.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://a2cdbc98b19eae6ad0.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import gradio as gr\n",
    "from langchain.vectorstores import Chroma, FAISS\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import heapq\n",
    "from langchain.schema import BaseRetriever\n",
    "from pydantic import Field\n",
    "\n",
    "# 設置環境變數以禁用 tokenizers 的並行處理\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "k = 5\n",
    "fetch_k = 100\n",
    "\n",
    "def create_embeddings(use_cpu=False):\n",
    "    device = \"cpu\" if use_cpu else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    return HuggingFaceEmbeddings(\n",
    "        model_name=\"BAAI/bge-m3\",\n",
    "        model_kwargs={'device': device}\n",
    "    )\n",
    "\n",
    "def load_FAISS_vectorstore(vectorstore_path, embeddings):\n",
    "    if os.path.exists(vectorstore_path):\n",
    "        try:\n",
    "            vectorstore = FAISS.load_local(vectorstore_path, embeddings, allow_dangerous_deserialization=True)\n",
    "            print(f\"Loaded vector store from {vectorstore_path}\")\n",
    "            return vectorstore\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading vector store from {vectorstore_path}: {str(e)}\")\n",
    "    else:\n",
    "        print(f\"Vector store not found at {vectorstore_path}\")\n",
    "    return None\n",
    "\n",
    "def load_vectorstores_from_directory(parent_directory, embeddings):\n",
    "    vectorstores = []\n",
    "    for root, dirs, files in os.walk(parent_directory):\n",
    "        if 'index.faiss' in files and 'index.pkl' in files:\n",
    "            vectorstore_path = root\n",
    "            vs = load_FAISS_vectorstore(vectorstore_path, embeddings)\n",
    "            if vs:\n",
    "                vectorstores.append(vs)\n",
    "    return vectorstores\n",
    "\n",
    "def retrieve_from_multiple_stores(vectorstores, query, k=5, fetch_k=100):\n",
    "    all_results = []\n",
    "    for vs in vectorstores:\n",
    "        results = vs.max_marginal_relevance_search(query, k=fetch_k, fetch_k=fetch_k)\n",
    "        scored_results = vs.similarity_search_with_score(query, k=len(results))\n",
    "        all_results.extend(scored_results)\n",
    "    \n",
    "    return [doc for doc, score in heapq.nsmallest(k, all_results, key=lambda x: x[1])]\n",
    "\n",
    "def setup_qa_chain(use_cpu=False):\n",
    "    groq_api_key = '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'\n",
    "    #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\n",
    "    #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\n",
    "    model = 'llama-3.1-8b-instant'\n",
    "    groq_chat = ChatGroq(groq_api_key=groq_api_key, model_name=model)\n",
    "    memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "    \n",
    "    # 使用新的FAISS檢索邏輯，並傳入 use_cpu 參數\n",
    "    parent_directory = \"/media/starklab/BACKUP/向量庫\"  # 更新為您的向量庫目錄\n",
    "    embeddings = create_embeddings(use_cpu)\n",
    "    vectorstores = load_vectorstores_from_directory(parent_directory, embeddings)\n",
    "    \n",
    "    class CustomRetriever(BaseRetriever):\n",
    "        vectorstores: list = Field(default_factory=list)\n",
    "    \n",
    "        def __init__(self, vectorstores):\n",
    "            super().__init__()\n",
    "            self.vectorstores = vectorstores\n",
    "    \n",
    "        def _get_relevant_documents(self, query):\n",
    "            results = retrieve_from_multiple_stores(self.vectorstores, query, k=k, fetch_k=fetch_k)\n",
    "            return results  # 這裡應該直接返回文檔列表，而不是元組\n",
    "\n",
    "    custom_retriever = CustomRetriever(vectorstores)\n",
    "\n",
    "\n",
    "\n",
    "    template = \"\"\"我將作為您的Podcast搜尋引擎。當您向我詢問有關特定Podcast節目或內容的問題時，我將使用RAG（檢索增強生成）技術來回答您的問題。請注意，如果RAG檢索庫中沒有您所需的內容，我將告知您「RAG資料庫內沒有您所需的內容」。我希望您根據這些條件提問。\n",
    "\n",
    "您的第一句話是「嗨」。\n",
    "\n",
    "檢索資料信息（包括節目標題）：\n",
    "{context}\n",
    "\n",
    "聊天歷史：\n",
    "{chat_history}\n",
    "\n",
    "當前問題：\n",
    "{question}\n",
    "\n",
    "回答指南：\n",
    "1. **問題處理**：首先對當前問題進行清晰的 prompt engineering，確保理解問題的核心需求。\n",
    "2. **信息使用**：僅使用檢索資料中的信息來回答問題。如果資料不足以回答問題，請直接回答「RAG 資料庫沒有您想要的資料」。\n",
    "3. **回答內容**：\n",
    "   - **具體內容要點**：回答應包括具體的內容要點。\n",
    "   - **時間戳**：每個內容要點應附上對應的時間戳。請使用完整的格式，例如（MM:SS~MM:SS）。如果只有一個時間點，則使用（MM:SS）。\n",
    "   - **節目標題**：最後應提供節目標題（格式：（節目標題：[完整標題]））。\n",
    "4. **回答格式示例**：\n",
    "   - 「根據檢索資料，[內容摘要1]（時間戳）。此外，[內容摘要2]（時間戳）。[如有更多內容，繼續列舉]。（節目標題：[完整標題]）」\n",
    "5. **回答語言和風格**：回答要清楚詳細，使用繁體中文。\n",
    "6. **資訊限制**：不要添加任何檢索資料中沒有的信息。\n",
    "7. **格式問題**: 請不要使用刪除線或任何其他特殊格式標記在你的回答中。\n",
    "8. **記憶**: 如果使用者希望接續前面的問答再次提問，系統應該能夠檢索並提供對話紀錄（chat_history），並根據這些紀錄回答使用者的問題。\n",
    "請根據上述指南回答問題：\n",
    "\"\"\"\n",
    "\n",
    "    document_prompt = PromptTemplate(\n",
    "        input_variables=[\"page_content\", \"episode_name\", \"Podcast_name\"],\n",
    "        template=\"內容: {page_content}\\n來源: {episode_name}, {Podcast_name}\"\n",
    "    )\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=groq_chat,\n",
    "        retriever=custom_retriever,\n",
    "        memory=memory,\n",
    "        combine_docs_chain_kwargs={\n",
    "            \"prompt\": prompt,\n",
    "            \"document_variable_name\": \"context\",\n",
    "            \"document_prompt\": document_prompt\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return qa_chain, custom_retriever\n",
    "\n",
    "# 修改主函數以接受 use_cpu 參數\n",
    "def main(use_cpu=False):\n",
    "    qa_chain, retriever = setup_qa_chain(use_cpu)\n",
    "\n",
    "    def get_program_list(folder_path):\n",
    "        try:\n",
    "            programs = [name for name in os.listdir(folder_path) if os.path.isdir(os.path.join(folder_path, name))]\n",
    "            program_list = \"\\n\".join(f\"{i + 1}: {program}\" for i, program in enumerate(programs))\n",
    "            return program_list\n",
    "        except FileNotFoundError:\n",
    "            return \"指定的資料夾不存在。\"\n",
    "        except Exception as e:\n",
    "            return f\"發生錯誤: {e}\"\n",
    "\n",
    "    def chat_function(message, history):\n",
    "        try:\n",
    "            results = retriever.get_relevant_documents(message)\n",
    "            response = qa_chain.invoke({\"question\": message, \"chat_history\": history})\n",
    "            answer = response['answer']\n",
    "    \n",
    "            # 使用集合來存儲唯一的 (episode_name, podcast_name) 組合\n",
    "            unique_sources = set()\n",
    "            for result in results:\n",
    "                episode_name = result.metadata.get('episode_name', 'Unknown Episode')\n",
    "                podcast_name = result.metadata.get('Podcast_name', 'Unknown Podcast')\n",
    "                unique_sources.add((episode_name, podcast_name))\n",
    "    \n",
    "            # 格式化 sources 字符串\n",
    "            sources_str = \"\\n可參考下方節目集數：\\n\"\n",
    "            for idx, (episode_name, podcast_name) in enumerate(unique_sources, 1):\n",
    "                sources_str += f\"Result {idx}: {episode_name}, {podcast_name}\\n\"\n",
    "    \n",
    "            # 將答案和來源信息合併為一個字符串\n",
    "            full_response = f\"{answer}\\n\\n{sources_str}\"\n",
    "    \n",
    "            return full_response\n",
    "    \n",
    "        except Exception as e:\n",
    "            error_message = f\"發生錯誤: {str(e)}\\n很抱歉，我無法處理您的問題。請再試一次或換個問題。\"\n",
    "            return error_message\n",
    "\n",
    "    with gr.Blocks() as iface:\n",
    "        gr.Markdown(f\"## 目前資料庫中的節目有：\\n{get_program_list('/media/starklab/BACKUP/Podcast_project/轉錄文本存放區')}\\n\\n請在下方提問：\")\n",
    "\n",
    "        chatbot = gr.ChatInterface(\n",
    "            chat_function,\n",
    "            title=\"Podcast Q&A Assistant\",\n",
    "            description=\"Ask questions about podcast content, and I'll provide answers based on the retrieved information.\",\n",
    "            theme=\"soft\",\n",
    "            examples=[\n",
    "                \"林書豪這個賽季遇到了什麼困難？\",\n",
    "                \"請告訴我這個節目討論了哪些主題？\",\n",
    "                \"這集節目中有提到哪些重要的觀點？\"\n",
    "            ],\n",
    "            retry_btn=\"重試\",\n",
    "            undo_btn=\"撤銷\",\n",
    "            clear_btn=\"清除\"\n",
    "        )\n",
    "\n",
    "    iface.launch(share=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    use_cpu = True  # 設置為 True 以使用 CPU，False 則使用 GPU（如果可用）\n",
    "    main(use_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d2852efb-e360-446c-a52d-76c3bfc477b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average similarity score: 0.52421535551548\n",
      "Question: 根據林書豪的描述,新北國王隊在本賽季面臨了哪些主要挑戰?\n",
      "Similarity score: 0.4405829608440399\n",
      "\n",
      "Question: 林書豪提到球隊傷病問題如何影響了球隊的整體表現?\n",
      "Similarity score: 0.696298360824585\n",
      "\n",
      "Question: 與往季相比,林書豪認為本賽季在維持球隊狀況方面有什麼不同?\n",
      "Similarity score: 0.6541098356246948\n",
      "\n",
      "Question: 在冠軍賽中,林書豪遇到了什麼個人健康問題?這如何影響了他的表現?\n",
      "Similarity score: 0.5578819513320923\n",
      "\n",
      "Question: 林書豪如何描述球隊在季後賽期間的轉變?\n",
      "Similarity score: 0.403282105922699\n",
      "\n",
      "Question: 根據播客內容,目前科技業的裁員情況如何?\n",
      "Similarity score: 0.5089830756187439\n",
      "\n",
      "Question: 主持人們提到被裁員後,在找新工作時應該如何考慮薪資問題?\n",
      "Similarity score: 0.3919658958911896\n",
      "\n",
      "Question: 播客中提到了哪些可能導致員工想要離職的原因?\n",
      "Similarity score: 0.5406186580657959\n",
      "\n",
      "評估完成，結果已保存到 evaluation_results.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        if isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return super(NumpyEncoder, self).default(obj)\n",
    "\n",
    "# 讀取 JSON 文件\n",
    "with open('/home/starklab/Documents/QA集/qa_dataset.json', 'r', encoding='utf-8') as f:\n",
    "    qa_data = json.load(f)\n",
    "\n",
    "# 初始化 sentence transformer 模型\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def calculate_similarity(text1, text2):\n",
    "    # 計算兩段文本的餘弦相似度\n",
    "    embeddings = model.encode([text1, text2])\n",
    "    return cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
    "\n",
    "results = []\n",
    "\n",
    "for item in qa_data['questions']:\n",
    "    question = item['question']\n",
    "    llm_answer = item['llm_answer']\n",
    "    ground_truth = item['ground_truth']\n",
    "    \n",
    "    # 計算 LLM 回答和 ground truth 的相似度\n",
    "    similarity = calculate_similarity(llm_answer, ground_truth)\n",
    "    \n",
    "    results.append({\n",
    "        'question': question,\n",
    "        'similarity_score': similarity\n",
    "    })\n",
    "\n",
    "# 計算平均相似度分數\n",
    "average_similarity = sum(item['similarity_score'] for item in results) / len(results)\n",
    "\n",
    "# 輸出結果\n",
    "print(f\"Average similarity score: {average_similarity}\")\n",
    "for item in results:\n",
    "    print(f\"Question: {item['question']}\")\n",
    "    print(f\"Similarity score: {item['similarity_score']}\")\n",
    "    print()\n",
    "\n",
    "# 將結果保存到文件，使用自定義的 NumpyEncoder\n",
    "with open('evaluation_results.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump({\n",
    "        'average_similarity': average_similarity,\n",
    "        'results': results\n",
    "    }, f, ensure_ascii=False, indent=4, cls=NumpyEncoder)\n",
    "\n",
    "print(\"評估完成，結果已保存到 evaluation_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ec7007-14f2-4377-80d8-355a8e30b31a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
