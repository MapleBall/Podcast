{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "def sanitize_filename(filename):\n",
    "    # 定義要移除的非法字符集合\n",
    "    illegal_chars = set('/\\\\%*|\"')\n",
    "\n",
    "    # 創建一個映射表，將所有非法字符映射為空字串\n",
    "    translation_table = str.maketrans('', '', ''.join(illegal_chars))\n",
    "    \n",
    "    # 使用 translate 方法來應用映射表，移除非法字符\n",
    "    sanitized_filename = filename.translate(translation_table)\n",
    "    \n",
    "    return sanitized_filename\n",
    "\n",
    "def download_mp3(url, folder_path, filename):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, stream=True, allow_redirects=True)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # 獲取最終的下載 URL\n",
    "        final_url = response.url\n",
    "        response = requests.get(final_url, headers=headers, stream=True)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        filepath = os.path.join(folder_path, filename)\n",
    "        with open(filepath, 'wb') as file:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                file.write(chunk)\n",
    "        print(f\"MP3 檔案已下載並儲存為 {filename}\")\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"下載失敗: {e}\")\n",
    "\n",
    "def get_rss_file(newsurl, test_n, parent_folder_path=None):\n",
    "    def get_title_name(url):\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            title = soup.find(\"title\").text if soup.find(\"title\") else \"Unknown Title\"\n",
    "            return sanitize_filename(title)\n",
    "        return \"Unknown Title\"\n",
    "\n",
    "    # 獲取節目標題，並在母資料夾內建立子資料夾\n",
    "    if newsurl[13:21] == \"firstory\":\n",
    "        response = requests.get(newsurl)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            title = soup.find(\"title\").text if soup.find(\"title\") else \"Unknown Title\"\n",
    "            title = sanitize_filename(title)\n",
    "    else:\n",
    "        title = get_title_name(newsurl)\n",
    "\n",
    "    # 如果指定了母資料夾，則在其內建立子資料夾\n",
    "    folder_path = os.path.join(parent_folder_path, title) if parent_folder_path else title\n",
    "\n",
    "    # 創建資料夾\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    if newsurl[13:21] == \"firstory\":\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        items = soup.find_all(\"item\")\n",
    "        n = len(items)\n",
    "        print(title, \"一共\", n, \"集\")\n",
    "\n",
    "        test_n = n if n <= 2 or test_n == \"all\" else int(test_n)\n",
    "\n",
    "        for i in range(min(test_n, n)):\n",
    "            time.sleep(random.randrange(1, 5))\n",
    "            item = items[i]\n",
    "            mp3_url = item.find('enclosure')['url'] if item.find('enclosure') else None\n",
    "            name = item.find('itunes:title').text if item.find('itunes:title') else f\"Episode {i+1}\"\n",
    "            if mp3_url:\n",
    "                sanitized_name = sanitize_filename(name)\n",
    "                download_mp3(mp3_url, folder_path, sanitized_name + \".mp3\")\n",
    "            else:\n",
    "                print(f\"無法找到第 {i+1} 集的 MP3 URL\")\n",
    "\n",
    "    elif newsurl[14:23] == \"buzzsprout\":\n",
    "        feed = feedparser.parse(newsurl)\n",
    "        title = feed.feed.title\n",
    "        n = len(feed.entries)\n",
    "        print(title, \"一共\", n, \"集\")\n",
    "\n",
    "        test_n = n if n <= 2 or test_n == \"all\" else int(test_n)\n",
    "\n",
    "        for i in range(min(test_n, n)):\n",
    "            time.sleep(random.randrange(1, 5))\n",
    "            entry = feed.entries[i]\n",
    "            mp3_url = entry.enclosures[0]['url'] if entry.enclosures else None\n",
    "            name = entry.title\n",
    "            if mp3_url:\n",
    "                sanitized_name = sanitize_filename(name)\n",
    "                download_mp3(mp3_url, folder_path, sanitized_name + \".mp3\")\n",
    "            else:\n",
    "                print(f\"無法找到第 {i+1} 集的 MP3 URL\")\n",
    "\n",
    "    else:\n",
    "        file = feedparser.parse(newsurl)\n",
    "        n = len(file.entries)\n",
    "        print(title, \"一共\", n, \"集\")\n",
    "\n",
    "        test_n = n if n <= 2 or test_n == \"all\" else min(int(test_n), n)\n",
    "\n",
    "        for i in range(test_n):\n",
    "            time.sleep(random.randrange(1, 5))\n",
    "            entry = file.entries[i]\n",
    "            try:\n",
    "                mp3_url = next((link['href'] for link in entry.links if link.get('type', '').startswith('audio/')), None)\n",
    "                if not mp3_url:\n",
    "                    raise ValueError(\"No audio link found\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching MP3 URL for episode {i+1}: {e}\")\n",
    "                continue\n",
    "\n",
    "            name = entry.title\n",
    "            sanitized_name = sanitize_filename(name)\n",
    "            download_mp3(mp3_url, folder_path, sanitized_name + \".mp3\")\n",
    "                    \n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "# 測試字符是否為中文\n",
    "def is_chinese(char):\n",
    "    # 判斷字符的 Unicode 編碼值是否在中文範圍內\n",
    "    if '\\u4e00' <= char <= '\\u9fff':\n",
    "        return True\n",
    "    else:\n",
    "        return False      \n",
    "    \n",
    "def get_all_category():\n",
    "    all_cata=['all',\n",
    "     'Arts',\n",
    "     'Arts / Books',\n",
    "     'Arts / Design',\n",
    "     'Arts / Fashion & Beauty',\n",
    "     'Arts / Food',\n",
    "     'Arts / Performing Arts',\n",
    "     'Arts / Visual Arts',\n",
    "     'Business',\n",
    "     'Business / Careers',\n",
    "     'Business / Entrepreneurship',\n",
    "     'Business / Investing',\n",
    "     'Business / Management',\n",
    "     'Business / Marketing',\n",
    "     'Business / Non-Profit',\n",
    "     'Comedy',\n",
    "     'Comedy / Comedy Interviews',\n",
    "     'Comedy / Improv',\n",
    "     'Comedy / Stand-Up',\n",
    "     'Education',\n",
    "     'Education / Courses',\n",
    "     'Education / How To',\n",
    "     'Education / Language Learning',\n",
    "     'Education / Self-Improvement',\n",
    "     'Fiction',\n",
    "     'Fiction / Comedy Fiction',\n",
    "     'Fiction / Drama',\n",
    "     'Fiction / Science Fiction',\n",
    "     'Government',\n",
    "     'Health & Fitness',\n",
    "     'Health & Fitness / Alternative Health',\n",
    "     'Health & Fitness / Fitness',\n",
    "     'Health & Fitness / Medicine',\n",
    "     'Health & Fitness / Mental Health',\n",
    "     'Health & Fitness / Nutrition',\n",
    "     'Health & Fitness / Sexuality',\n",
    "     'History',\n",
    "     'Kids & Family',\n",
    "     'Kids & Family / Education for Kids',\n",
    "     'Kids & Family / Parenting',\n",
    "     'Kids & Family / Pets & Animals',\n",
    "     'Kids & Family / Stories for Kids',\n",
    "     'Leisure',\n",
    "     'Leisure / Animation & Manga',\n",
    "     'Leisure / Automotive',\n",
    "     'Leisure / Aviation',\n",
    "     'Leisure / Crafts',\n",
    "     'Leisure / Games',\n",
    "     'Leisure / Hobbies',\n",
    "     'Leisure / Home & Garden',\n",
    "     'Leisure / Video Games',\n",
    "     'Music',\n",
    "     'Music / Music Commentary',\n",
    "     'Music / Music History',\n",
    "     'Music / Music Interviews',\n",
    "     'News',\n",
    "     'News / Business News',\n",
    "     'News / Daily News',\n",
    "     'News / Entertainment News',\n",
    "     'News / News Commentary',\n",
    "     'News / Politics',\n",
    "     'News / Sports News',\n",
    "     'News / Tech News',\n",
    "     'Religion & Spirituality',\n",
    "     'Religion & Spirituality / Buddhism',\n",
    "     'Religion & Spirituality / Christianity',\n",
    "     'Religion & Spirituality / Hinduism',\n",
    "     'Religion & Spirituality / Islam',\n",
    "     'Religion & Spirituality / Judaism',\n",
    "     'Religion & Spirituality / Religion',\n",
    "     'Religion & Spirituality / Spirituality',\n",
    "     'Science',\n",
    "     'Science / Astronomy',\n",
    "     'Science / Chemistry',\n",
    "     'Science / Earth Sciences',\n",
    "     'Science / Life Sciences',\n",
    "     'Science / Mathematics',\n",
    "     'Science / Natural Sciences',\n",
    "     'Science / Nature',\n",
    "     'Science / Physics',\n",
    "     'Science / Social Sciences',\n",
    "     'Society & Culture',\n",
    "     'Society & Culture / Documentary',\n",
    "     'Society & Culture / Personal Journals',\n",
    "     'Society & Culture / Philosophy',\n",
    "     'Society & Culture / Places & Travel',\n",
    "     'Society & Culture / Relationships',\n",
    "     'Sports',\n",
    "     'Sports / Baseball',\n",
    "     'Sports / Basketball',\n",
    "     'Sports / Cricket',\n",
    "     'Sports / Fantasy Sports',\n",
    "     'Sports / Football',\n",
    "     'Sports / Golf',\n",
    "     'Sports / Hockey',\n",
    "     'Sports / Rugby',\n",
    "     'Sports / Running',\n",
    "     'Sports / Soccer',\n",
    "     'Sports / Swimming',\n",
    "     'Sports / Tennis',\n",
    "     'Sports / Volleyball',\n",
    "     'Sports / Wilderness',\n",
    "     'Sports / Wrestling',\n",
    "     'TV & Film',\n",
    "     'TV & Film / After Shows',\n",
    "     'TV & Film / Film History',\n",
    "     'TV & Film / Film Interviews',\n",
    "     'TV & Film / Film Reviews',\n",
    "     'TV & Film / TV Reviews',\n",
    "     'Technology',\n",
    "     'True Crime']\n",
    "    all_category=[]\n",
    "    for i in range(len(all_cata)):\n",
    "        all_category.append((all_cata[i].replace(\" / \",\"-\").replace(\" & \",\"-\").replace(\" \",\"-\").lower()))\n",
    "    return all_category\n",
    "\n",
    "\n",
    "def get_name_href(category_name):\n",
    "    #抓到總排行榜   全部的網址 跟名稱\n",
    "    rank_list_herf=[]\n",
    "    rank_list_name=[]\n",
    "    # 目標網站的URL\n",
    "\n",
    "    url=\"https://rephonic.com/charts/apple/tw/\"+category_name\n",
    "    # 發送GET請求獲取網頁內容\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # 檢查是否成功獲取網頁內容\n",
    "    if response.status_code == 200:\n",
    "        # 使用BeautifulSoup解析HTML\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # 在這裡編寫你的程式碼來處理解析後的網頁內容\n",
    "\n",
    "        # 以下是一個示例，尋找所有<a>標籤並獲取其連結和文字內容\n",
    "        for link in soup.find_all('a'):\n",
    "            href = link.get('href')\n",
    "            text = link.get_text()\n",
    "            if len(text)>0 and is_chinese(text)==True:\n",
    "                #print(f\"連結: {href}\\t文字內容: {text}\")\n",
    "                rank_list_herf.append(\"https://rephonic.com\"+href)\n",
    "                rank_list_name.append(text)\n",
    "            else:\n",
    "                pass\n",
    "    else:\n",
    "        print(\"無法獲取網頁內容\")\n",
    "    return  rank_list_herf,rank_list_name\n",
    "\n",
    "\n",
    "\n",
    "def get_rss(url):\n",
    "    # 目標網頁的URL\n",
    "    #url = \"https://rephonic.com/podcasts/li-jing-lei-de-chen-jing-shi-jian\"\n",
    "\n",
    "    # 發送GET請求獲取網頁內容\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # 檢查是否成功獲取網頁內容\n",
    "    if response.status_code == 200:\n",
    "        # 使用BeautifulSoup解析HTML\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # 尋找包含RSS網址的元素\n",
    "        find_rss=soup.find_all(\"\",text=re.compile(\"@context\"))\n",
    "\n",
    "        # 要解析的 JSON 字串\n",
    "        json_str = find_rss[0]\n",
    "        # 解析 JSON 字串\n",
    "        data = json.loads(json_str)\n",
    "        # 提取 identifier 後面的文字\n",
    "        identifier_text = data[\"identifier\"]\n",
    "        print(identifier_text)\n",
    "\n",
    "    else:\n",
    "        print(\"無法獲取網頁內容\")\n",
    "    return identifier_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 選擇要爬蟲的節目 (RSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 存放mp3檔路徑\n",
    "folder_path = r\"/media/starklab/BACKUP/Podcast_project/mp3//\"\n",
    "\n",
    "# #貼上rss即可下載。＃新資料夾\n",
    "# 百靈果（近100集）\n",
    "# get_rss_file(\"https://feeds.buzzsprout.com/1974862.rss\",\"all\")\n",
    "# #史塔克實驗室\n",
    "# get_rss_file(\"https://feeds.soundon.fm/podcasts/e4f101be-289a-4101-bb11-59fc61e5c88b.xml\",\"all\")\n",
    "# #跳脫Do式圈\n",
    "# get_rss_file(\"https://feeds.soundon.fm/podcasts/22505944-fec2-4417-b277-649ce5d3a491.xml\",\"all\")\n",
    "#週報時光機 (還沒)\n",
    "# get_rss_file(\"https://feed.firstory.me/rss/user/ckcnhs4x0yuqw0918kkui2pjw\",\"all\",folder_path)\n",
    "#寧可當吃貨 \n",
    "# get_rss_file(\"https://feed.firstory.me/rss/user/cklase4t37jae0872lm9x1xmv\",\"all\",folder_path)\n",
    "#科技浪 \n",
    "# get_rss_file(\"https://feeds.soundon.fm/podcasts/03f4a53e-80cf-4a20-ad2c-bdb31a76c7b3.xml\",\"all\",folder_path)\n",
    "#老高 \n",
    "# get_rss_file(\"https://anchor.fm/s/3ba51528/podcast/rss\",\"all\")\n",
    "#Joe & Jet 未過濾的 with Jason\n",
    "# get_rss_file(\"https://feeds.soundon.fm/podcasts/78a91a6a-5c6b-43cc-aaac-7918f792e5ae.xml\",\"all\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 沒有用到"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    #先得到所有分類的名字\n",
    "    category_name=get_all_category()\n",
    "    #使用者輸入想跑多少分類\n",
    "    category_n=input(\"你想要跑多少種分類？ 輸入數字，或者all抓取全部\")\n",
    "    if category_n==\"all\":       \n",
    "        category_n=len(category_name)\n",
    "    else:\n",
    "        category_n=int(category_n)\n",
    "    \n",
    "    for i in range(category_n):      #這邊的迴圈  是指定跑幾個分類    \n",
    "        #得到該分類的所有節目名稱跟網址\n",
    "        rank_list_herf,rank_list_name=get_name_href(category_name[i])\n",
    "        #使用者輸入想跑多少節目\n",
    "        rank_list_n=5        #(\"一個分類想要抓取多少節目？ 輸入數字，或者all抓取全部\")\n",
    "        if rank_list_n==\"all\":    \n",
    "            rank_list_n=len(rank_list_herf)\n",
    "        else:\n",
    "            rank_list_n=int(rank_list_n)\n",
    "        \n",
    "        for j in range(rank_list_n):     #這邊的迴圈  是指定跑幾個節目\n",
    "            \n",
    "            #得到該節目的rss\n",
    "            rss_url=get_rss(rank_list_herf[j])\n",
    "            #下載所有節目\n",
    "            get_rss_file(rss_url,3)    #後面的後面的數字   是測試用的時候。要下載幾集。  ＃要全部的集數。 輸入\"all\" \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bojyun",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
