(00:00~01:00) 【科技浪】 哈囉大家好,歡迎收聽科技浪,我是主持人哈利 科技浪是一個白話跟你聊科技的Podcast 希望可以用簡單易懂,但是又深入的方式 帶你了解時下最火的科技話題 本期節目由鼎鑫電腦贊助播出 今天要跟大家介紹的是鼎鑫電腦的知識管理平台Chatfile 在我開始介紹Chatfile之前 我先給大家一些背景知識 首先就是我們知道 LLM也就是大型圓模型 有兩個很大的問題嘛 嚴格來說他們有很多問題啦 但是有兩個問題是最直接影響到他的實用性的 第一個就是他會一本正經的瞎掰 第二個就是他沒有自己訓練資料以外的知識 那這兩個痛點呢就讓我們在使用這些大型圓模型的時候 變得有一點搏手搏腳 因為我們很常問他一個問題 我們也不確定他講的到底是不是對的 還要自己去Google再去Fact Check一次 然後我們有一些自己的資料 可能是公司內部的資料
(01:00~02:00) 或者是有一些非常新的文件 可能昨天才剛出來的一些文件 然後你想要用大型圓模型來幫你解讀 這件事情你也做不到 因為這些資料沒有出現在這個LLM的訓練資料當中嘛 那你如果是科技量的聽眾呢 你聽到這裡你應該知道我接下來要講什麼了 對,因為我常常講到說 業界現在有個可以解決這兩個問題 或者你說大幅的降低這兩個問題影響的一個方法 叫做RAG RAG在做的事情就是 讓你的LLM外接一個資料庫 讓他在回答的時候可以參考這個資料庫裡面的資料進行回答 那這個時候LLM可以直接看著這個文件回答你的問題 他講錯話的機率當然就大幅下降 然後同時他現在也等於是有了他訓練資料以外的知識了 那我們今天要介紹的這個頂新電腦的Chatfile 就是一個RAG的產品 它的使用方法很簡單 它就是一個非常簡潔的一個聊天的介面嘛 然後在這個聊天的介面旁邊呢 你可以上傳你的文件 你可以上傳很多個文件五個以上 然後所有常見的格式都可以
(02:00~03:00) 就是包括PDF、Word、TXC File等等 然後你上傳完了之後呢 你就可以直接跟這個Chatfile的機器人聊天 聊天過程中你可以問關於你上傳的文件的任何一個問題 然後這個Chatfile的機器人呢 就會從你上傳的所有文件中找出最相關的那個片段 然後用那個片段來進行回答 然後這邊我也有特別去問他們這個產品背後的技術細節 因為我們科技量講到這個技術的話題都會講得很深嘛 然後鼎鑫那邊人也很好 他們就找了他們的Product來跟我聊一聊 那我自己聽下來 它就是一個非常典型的Rag的架構啊 就是你上傳了這些文件之後 他們會把你這些文件變成一個向量的形式 然後存在一個向量資料庫 然後之後你每次在問問題的時候 它就會根據你這個問題產生出來的向量 跟它向量資料庫裡面所有的向量進行一個對比 找出最相關的那些文章片段 讓它接下來的回答可以有一個參考 會使用向量是因為向量才可以做到什麼樣的東西 因為向量才可以真正體現文字的意義 它背後的意涵 你如果單純的使用文字搜索的話 你搜尋狗 熱狗也會跳出來
(03:00~04:00) 對吧 但是熱狗的向量跟狗的向量就會差非常多啊 那這邊Chapfile有一個特點就是 他們有自己特別訓練一個中文 包括簡體中文跟繁體中文的Embedding Model Embedding Model就是把你的文字變成向量的AI模型 那你聽這個Rag的架構你就會知道了嘛 就是這個Embedding Model它的好壞 可以很大程度的影響你最後這個Rag的flow產生出來的結果的好壞嘛 因為你一個很糟糕的Embedding Model 你抓出了一些很不相關的文件 你當然沒有辦法好好的回答 所以鼎鑫電腦這個專門為中文設計的Embedding Model 我覺得是非常好的 然後呢他們的聊天機器人本身 他們的LM本身就是ChatGPT 3.5 這個他們自己在官網也有講了 然後他們的後端呢全部都是使用微軟的Azure 所以你如果相信微軟的話 你的資料就是安全的 然後我覺得你應該是可以相信微軟啦 微軟都做B2B的聲音做了這麼久了 那麼這個ChatFile呢鼎鑫他們也是有提供給我使用啊
(04:00~05:00) 那大家都知道嘛 我自己在測試這種Rag的System的時候 我都會做一個科技量測試 科技量測試就是拿科技量的文字稿 科技量的竹字稿全部丟進去 然後問它裡面一些非常細節的問題 像是我問它說請問哈利在台北喜歡騎什麼 這個我就只有在第八集的一開頭可能講了三句話 兩句話三句話這樣 然後是把全部十四集還是十幾集的竹字稿全部丟進去 然後這題ChatFile是有回答出來 就是他說哈利喜歡騎Wemo跟腳踏車這樣 然後另外我有問一題就是請比較一下Optimus跟Atlas 這個是第八集的內容 也一樣是第八集 我也不知道為什麼我都在問第八集 反正這個Optimus是特斯拉的人形機器人 然後Atlas是Boston Dynamics的人形機器人 然後我第八集有做一個比較 然後我這個比較都是我自己寫的 在網路上找不到一模一樣的內容 跟我用一模一樣的維度 我覺得是不可能
(05:00~06:00) 應該說一定很多人做過類似的比較 但是跟我一模一樣是不太可能的 但是這個ChatFile就有正確的回答出 我那時候比較的時候我使用的方法我使用的維度 所以我覺得你的工作如果會需要讀 會需要研究很大量的資料 不管你是分析師還是律師之類的 我覺得ChatFile就是一個很好的助手 就是你可以把它想像成一個 一年365天隨時on call 而且超級聰明 而且反應超級快 而且又超級便宜的一個研究助理 那當然ChatGPT其實也可以做到類似的事情 就是假設你有買他們的Plus會員的話 你可以使用他們的GPT功能 你可以建一個GPT來間接做到這個REG的flow 但他就真的沒有像ChatFile這麼直接跟單純 就是根據你要這個GPT回答的文件 你要去建一個自己的客製化GPT出來 然後你要在那邊搞什麼Prom Engineering的東西 然後假設你今天有一些文件是行政文件 有一些文件是法律文件
(06:00~07:02) 有一些文件是這個機器學習相關的文件 那你要全部放在一個GPT當中嗎? 感覺也是有點怪 然後還有就是這個ChatGPT的Plus是比較貴的喔 就是他們的個人版是一個月600塊台幣 就是20塊美金這樣 但是頂新的ChatFile個人版是一個月300塊台幣 所以說是直接半價的概念 當然啦我這邊也不是說ChatFile一定比ChatGPT好 這邊就是大家自己各取所需 就自己去判斷一下 如果你需要的客製化程度比較高 然後會想要一些OpenAI的Beta功能 那你可能可以選擇ChatGPT 那你如果是想要非常單純的想要解讀你的文件 尤其是中文的文件 然後你不想要那麼多太多複雜的功能 也不想一個月付600塊的話 你真的可以考慮一下ChatFile 你如果想要訂閱ChatFile個人版的話 本集資訊欄裡面有一個優惠碼 我這邊就不唸了 這是非常複雜的優惠碼 你就自己複製貼上 然後這樣你在訂閱6個月或是12個月方案的時候 你第一個月可以直接免費 立刻省300塊 有興趣的朋友可以點進去看一下 那最後我也要說一下
(07:02~08:02) 就是LLM的系統是非常機率的 這我剛剛已經講過 但我還是要強調 所以它是非常機率的 它是非常機率的 所以說不代表 它有了參考資料之後 它就完全不會犯錯 它也是有可能會犯錯 所以雖然這些產品 我相信一定可以產生很大的價值 但是你還是要小心使用 好那本集業配就到這邊結束 謝謝鼎鑫電腦的贊助 好那我們今天這一集呢 就是要講一個AI界的大新聞 Google的Gemini 那我知道Gemini這個新聞呢 是在禮拜三的晚上 差不多11點那個時候 我那時候一打開YouTube 我就看到一系列來自Google的影片 然後標題裡面都有Gemini 什麼Testing Gemini Hands on with Gemini之類的 我看到真的是又驚又喜啊 哇Google終於釋出了他們的Gemini模型了嗎 然後我就趕快上X了 我每次遇到這種大事 我第一件事就是去X上面看 然後果不其然的 上面我已經直接被Google Gemini洗版了 要知道在這個時候 那些YouTube影片才出來大概30分鐘吧 就是觀看數可能還只有幾千而已
(08:02~09:02) 但是我的X已經完全被Google Gemini洗版了 由此可見這件事是多麼的重大 那Google Gemini就是Google最新的AI模型 那它之所以會激起這麼多的討論 是因為我們對它的期待非常的高 那我記得Google第一次講到Gemini這個詞 就是它第一次說他們有在開發這個產品 是在今年的Io Google Io就是Google開發者大會嘛 那應該是在今年五月左右的時候吧 它講了這個他們有在開發Gemini 那Google那時候是都沒有講什麼細節嘛 他們就是說這個模型會超級強之類的 就是一些形容詞這樣 那這就給了網路鄉民很多的幻想空間嘛 對吧 所以說在接下來這幾個月內 這半年之中呢 Google Gemini的謠言真的是太多了 有些人說Gemini已經接近AGI啦 有些人說這個Gemini會比GPD-4強大好幾倍啊 然後我覺得其中謠言最有名的就是 Semi-Analysis的一篇文章
(09:02~10:03) Semi-Analysis是一個在研究半導體產業的公司 那個Semi就是Semi-Conductor的Semi嘛 那這間公司我記得我應該有在之前的集數講過啦 就是它算是一間很浮誇的公司 就是它很喜歡投一些震撼彈啦 下一些很驚悚的標題 然後賺很多點閱這樣 但是確實它的寫手叫做Dylan Patel 他確實是就是在這個半導體產業 他有一些人脈啊 然後也有蠻多的知識的 所以他們確實是有幾篇文章就真的是紅到大家都看過啊 那他們其中有一篇文章的標題呢就是 Google Gemini Eats the World Gemini smashes GPT-4 by 5X 就是Google Gemini吞噬全世界 然後Gemini會比GPD-4強五倍 直接碾壓GPD-4這樣 那這篇文章呢就是他們最爆紅的一篇文章之一啊 他們這篇文章剛出來的那兩個禮拜呢 每個人都在讀這篇文章 一堆人在轉發一堆人在討論 甚至連OpenAI的CEO Sam Almond 都在都發了一個推文在討論這篇文章
(10:03~11:04) 應該說他是在嘴Dylan Patel 那整篇文章超級長 然後還有一部分是要付費才可以看到 但是他全部都在講一個概念而已 就是Google有超級無敵多的GPU啊TPU跟GPU 所以他們train出來的模型 一定會比GPD-4強非常多 那我記得我應該有跟大家講過嘛 就是我對於Dylan Patel的看法就是 我覺得他是一個很浮誇的人啊 而且他是一個非常硬體的人 就是他我覺得他不太懂軟體 他都從一個硬體的角度出發在看事情 但是決定一個AI模型強度的 並不是只有硬體而已啊 又不是有算力就贏 他很喜歡講這種話就是 有算力就贏 但決定一個模型的強度的因素 除了算力以外 很重要的還有資料嘛 然後還有你的演算法嘛 所以說所以我真的覺得 事實沒有他想像那麼簡單啊 然後這次Gemini的release也印證了這件事啊 就是先跟大家爆雷一下啦 就是Gemini沒有比GPD-4強五倍好不好 他也沒有碾壓GPD-4好不好
(11:04~12:04) 所以說這全部都是錯誤的 但是他的文章其實我還是會看啊 因為我不知道他從哪裡搞來的 但他總是有一些內部的消息 好啦扯的有點遠了 反正我想講的就是 大家對於Gemini的期待都非常高 OK尤其很大一部分 是從這個Semai Analysis的這篇文章出來的 帶起了這一波大家對於Gemini的幻想 這波討論 然後原本Google就是預計在今年年底的時候 就是接近現在這個時間 要發布Gemini嘛 那大家都很期待 但是我們一直等到12月都還沒有出來 然後前一陣子好像有聽 有一些消息是說 他們好像要延後到明年年初 再來release Gemini 所以我們大家就想說 包括我啦我也是這樣想 就想說OK 那可能就要等到明年年初了 所以我這禮拜三的晚上 看到這個Google Gemini的釋出 真的是又驚又喜啊 哇這個等待終於結束了 雖然說這禮拜的科技浪呢 我其實有一些我老早就想講的主題
(12:04~13:04) 就是我不知道我有沒有跟大家講過 我有很多科技浪的主題 都會在那邊都沒有辦法講 因為動不動就有一些突然的大事出來嘛 像是這個Gemini 像是上上禮拜的這個OpenAI的事件嘛 我都一直沒有辦法講這些主題 然後我都很怕他們已經 到我講的時候都已經要過時了 現在應該是不會啦 應該算是在一個月內啦 但是就是這次又沒有辦法講到 是有點可惜 不過我覺得我們終於可以 揭開這個Gemini的神秘面紗 也是很好的好不好 我也是感到非常的興奮 好那接下來呢 我就先簡單跟大家介紹一下 Gemini這個模型它究竟是什麼 它可以做到什麼事情 然後為什麼有一些人說 Google lied to us Google騙我們 就是這個Gemini呢 其實是有一些爭議點的喔 這邊我覺得蠻有趣的我們可以講一講 然後除此之外呢 我也會帶大家看一些我覺得Gemini有的亮點 然後它跟OpenAI的GPD-4比起來 到底誰贏誰輸 OpenAI在AI產業的領先位置 是否終於要讓位了呢
(13:04~14:04) 好那首先是這個Gemini的簡介 我先講一下這個Gemini 究竟是什麼樣的一個模型 它其實呢就是一個LLM 一個大型音樂模型 也就是說它跟這個GPD-4 GPD-3 Anthropic的Clawed 臉書的LAMI 一樣它就是一個LLM 它一樣就是一個在做Next Word Prediction 或是你更精確來說應該是 Next Token Prediction的一個機器 也就是說它們的架構其實是一模一樣的 它們都是採用一個叫做 Transformer的神經網路架構 具體來說這個Transformer究竟是什麼 我就不多加介紹了 這邊比較技術一點啦 好那反正它們架構是一樣的 只不過唯一不一樣的地方 就是在這個Gemini呢 它是多模態的 模態也就是Modality 再講的就是 像是文字是一種模態 影片是一種模態 圖片是一種模態 聲音是一種模態 有點像是五官的感覺 這些AI的五官 那這個Gemini模型是多模態的 意思就是說 它不只是吃文字然後吐文字而已
(14:04~15:05) 它可以吃四種模態 它可以吃文字、圖片、聲音跟影片 然後它吐出來的 它的Output有兩種模態 有文字跟圖片 所以它真的算是現在所有大型模型當中 最多模態的一個 除了它以外 我們原本有的一個多模態模型 就是OpenAI的GPT-4V 就是它可以吃文字跟圖片的Input 然後Output是只有文字這樣 但是我必須說 Gemini這麼多模態 我必須在聲音跟影片這兩個模態旁邊 打一個問號 因為我還沒看過一個 我覺得是可以相信的一個Demo 是在Demo它的聲音跟 影片的模態能力的 他們確實是有出一些影片 是在Demo這個聲音跟影片的模態 但是我現在不太確定 我能不能相信這些Demo 所以我還是先打個問號 因為我們發現有一些Demo 他們是造假的 這邊我待會會再跟大家說 好那接下來介紹Gemini的Size 這個Gemini有三種不同的大小
(15:05~16:07) 分別是最大的Gemini Ultra 然後中型的Gemini Pro 跟微型的Gemini Nano Gemini Ultra可以媲美GPD-4 Gemini Pro差不多是在GPD 3.5的Level 然後這個Gemini Nano則是可以在手機上面跑的 一個非常微型的模型 這邊我覺得他們有Gemini Ultra跟Pro是很正常的 就是通常一般公司在Release他們的大型的模型 都會有不同的Size 然後通常都有像Ultra這種Size跟Pro這種Size 但是有Nano這種Size的模型是蠻少的 我覺得一直以來都只有Google比較認真在做這件事情而已 我覺得他們推出Nano真的是非常棒 因為這樣做等於是在加速邊緣AI的進展 所謂的邊緣AI就是在邊緣裝置上面跑AI 邊緣裝置就是你平時手邊摸得到的這些3C產品 像是你的手機、你的筆電這種 你自己可以擁有的這些3C裝置 叫做邊緣裝置
(16:07~17:08) 邊緣AI就是在這些裝置上面跑AI模型 當然因為這些裝置裡面的處理器 沒有像是雲端的伺服器這麼強 所以說你能跑的模型就會小非常多 通常這些AI公司或是科技巨頭 推出的這些大型音樂模型 通常都沒有辦法在手機上面跑 幾乎沒有 像是拉馬就不太可能在手機上面跑 雖然說在筆電上面跑是可以的 但是Google總是會推出一個在手機上面也可以跑的模型 就是他們5月的時候在Io他們不是發表了他們的Palm 2模型嗎 那時候他們有發表一個四種不同的size 其中一個size最小的叫做Gecko 也是可以在手機上面跑的 Google這麼做從一個商業的角度來看也是非常合理的 因為他們有Pixel Phone的Product Line 我自己也是很樂見這種情況 因為我覺得手機是不是我們一天會花這麼多時間 在上面的一個裝置 然後我真的覺得現在在上面做很多事情都可以被AI優化 然後都沒有被優化
(17:08~18:09) 所以我覺得總而言之 Google Gemini有一個Nano的size很讚 好那再來Google Gemini究竟有沒有比GPT-4強呢 我覺得細節這邊他們的成績的比較 我們可能留到待會再講 我現在先簡單跟大家講一下我目前的想法 我覺得Google Gemini Ultra應該就是跟GPT-4V差不多而已 我甚至不敢說它有比較好 我真的覺得應該就是差不多 然後Gemini Pro應該就是跟GPT-3.5差不多 就是你使用洽GPT然後你沒有付費的話 你使用的那個版本就是GPT-3.5 然後Google Gemini Pro就是跟它差不多 那我們要去哪裡使用這些Google Gemini的模型呢 首先現在他們Bard後面的模型已經換成了Google Gemini Pro 意思就是說你直接用Google Bard 你就可以跟這個Gemini Pro對話了 然後Bard完全是免費的 你只要有Google帳號你就可以去申請了 所以我覺得大家可以去玩一下 我自己也是稍微測試了一下 玩了一下啦我覺得蠻好玩的 那如果你是想要透過API來使用Google Gemini的話
(18:09~19:11) 你要等到12月13號 其實也快啦你們聽到這個Podcast之後 在兩天應該就API就出來了 然後你也可以透過這個Google AI Studio 跟Google Cloud的這個Vertex AI 把這些Gemini模型整合到你的應用程式當中 那Google Gemini的簡介就差不多到這邊 那最後再補充一點就是 他們還有同步釋出一個Gemini微調出來的一個Coding模型 叫做Alpha Code 2 也就是他們使用這些Coding的資料集呢 去再把這個Google Gemini模型進行再訓練 訓練出了一個叫做Alpha Code 2的模型 那這個Alpha Code 2的模型呢 我覺得大家討論很少啊 就是大家都在看Gemini本身嘛 但其實我覺得這個Alpha Code 2 有很多值得討論的點喔 我覺得算是一個亮點之一 這邊我們可能晚一點再介紹 好那接下來呢我們就來聊一點有掛的內容 就是Google究竟騙了我們什麼 最近這幾天國外的科技媒體已經開始報了嘛
(19:11~20:11) 就是他們有報說Google的Demo影片是騙人的 我不確定台灣的媒體有沒有報這個新聞啊 應該是有啦因為他們都超國外的嘛 他們可能玩個幾小時吧 但他們都會超國外的 但我覺得在講他們這個Demo影片做假之前呢 我覺得Google整個Demo就是不只是影片 包括他的網站包括他的Blog文 我覺得都有一點點浮誇的感覺 就是我可以很強烈的感覺到 他們是有在刻意的想要營造出大家對於這個模型的想法 當然啦每間公司都會這樣做 都會包裝一下 但我覺得Google Gemini他們這次包裝的有點過頭了 首先你進入他們的網站 你第一句話你會看到就是 Google Gemini是Multi Model的 包括Text,Images,Video,Audio跟Code 我剛剛這句話我第一個想法是想說 蛤?Code?你把Code算成一種模態是什麼意思? Code不是一種新的模態 Code就是Text Code就是文字 他們是同一種模態 如果Code也算是一種模態的話
(20:11~21:11) 那不是所有的大學園模型都是多模態模型嗎? 因為所有大學園模型都可以產生文字 就是一般的文章 跟Code啊 這是第一個浮誇點 第二個浮誇點就是你網站往下滑 你會看到一張圖是在顯示 Google Gemini在MMLU這個測試上面的成績 MMLU就是拿來測試這個大學園模型的一種測試 這邊我待會會再詳細講 那這張圖呢就是顯示了這個 Google Gemini跟Human Expert就是一般的人類 然後以及這個GPT-4他們的成績比起來的一個比較的圖片 GPT-4就是在最下面 他的成績是86.4% 再來是Human Expert他的成績是89.8% 那在最上面的是Google Gemini Ultra 他是90% 那我第一次看到這張圖呢 我是在我手機上面看到 然後那時候看到整個圖的比例是完全不make sense的 就是Google Gemini在超爆上面 然後他離Human Expert的成績是差超級多的
(21:11~22:12) 他是在一個非常高的地方 但其實你看實際數字 90%跟98.8%根本只差0.2%而已 然後你跟這個Human Expert還有GPT-4的這個距離比一下 你就知道那個比例是完全不make sense的 把這個Google Gemini呢放在非常高的地方 但是後來我發現是 這個比較是手機上面顯示出來的樣子啊 電腦上面呢是比較正常的一個比例 但我還是去偷偷量過了 然後發現這個比例還是不太正確 就是他離實際正確的比例沒有差非常多啦 但他們還是有稍稍的調一下這樣 然後至於手機上面的圖呢 我後來發現他們也改掉了 就是他們應該找前端工程師去fix過了 所以說目前看來是沒問題的 反正從這一點呢我們是可以看得出來 Google是想要透過這個稍微微調這個圖呈現的樣子 讓Gemini看起來是真的是高高在上 但這邊我自己是覺得還行啦 不算是騙
(22:12~23:13) 因為我自己以前也是在過瘾的待過嘛 然後我上一份工作也有很多跟這個客戶要溝通data的部分 這邊在做這個資料視覺化的時候 我覺得多多少少都是會選擇一個 可能對於我們比較有利的呈現模式啊 所以說這邊我是可以理解 但是接下來的這一點我覺得真的就是騙了 cross the line 就是Google有出一個Gemini的demo影片 6分鐘的 嚴格來說他們出非常多的影片啊 但是6分鐘的那個hands-on demo影片 是最多人觀看的 我剛剛check了 到現在已經有200多次的觀看次數了 那這個影片呢我建議大家可以點進去看一下 我會把連結放在這個本集的資訊欄裡面 在這個影片當中呢 哇Gemini看起來真的是像是活著的一樣 就是像是有人類的靈魂一樣 哇真的是非常生動 像是這個測試者呢有做一個這個 剪刀石頭布的動作給Gemini看 然後問Gemini說我在幹嘛 就是他是用影片的形式呈現喔 就是他手這樣
(23:13~24:14) 甩兩下然後剪刀甩兩下然後石頭甩兩下 然後問Gemini說我現在在幹嘛 而且他是用聲音問喔 他不是打字問Gemini喔 他說What am I doing 然後Gemini就說喔你現在在玩剪刀石頭布 然後還有一段是 我覺得我看我自己第一遍看那個影片的時候 我被震驚到最多的時候 就是我看到那段我是真的覺得天啊 現在AI真的可以做到這種程度嗎 太扯了 就是他們有一個片段就是 這個測試者呢桌上放一顆這個紙球 然後他拿三個杯子 其中一個杯子把這個紙球給蓋住 然後這個時候Gemini就說 好喔你是想要我猜這個紙球現在在哪個杯子下面嗎 好我接受挑戰 然後這個測試者呢就開始移動這些杯子 就是應該說對於一個人類來說 其實不會很難的事情 就是你兩隻眼睛直直的盯著中間那個杯子 然後看他被移到哪裡就好了 對於人類來說應該是可以蠻簡單回答出來的 但是他移完之後Gemini直接說 To the left就是左邊那個然後是正確的 我整個嚇傻耶
(24:14~25:14) 因為我那時候看到這個demo我心裡在想的是 他在做的事情應該是 他把這個他變魔術 這算變魔術嗎還是算是一個猜謎 反正他把這個移動杯子的遊戲的影片給錄下來 然後把這個影片當作input直接丟給Gemini 然後Gemini就猜出說 在這個移動完之後呢球應該在左邊的杯子下面 意思就是說Gemini單純靠這個影片 就猜出說這個球在哪裡 我覺得這件事情依我現在對於這些大型音樂模型的了解 我覺得要做到是非常非常困難的一件事 就是在Gemini之前我們最先進的這個多麼泰模型就是GPT-4V嘛 就是他懂他有一些視覺知識 在文字知識以外他還有視覺知識 那對於GPT-4V來說他的視覺知識的程度就只有到一個程度 就是他可以了解什麼是杯子什麼是球 然後他對於世界上的物體是怎麼樣運作的 他有一個非常粗淺的了解
(25:14~26:16) 就是他知道假設這台車子他的背景看起來有點模糊 這就代表他正在很快很高速的行駛中 但是你今天要了解一個變魔術的影片 不是圖片喔 你要對於世界上的所有物體怎麼運作 有一個非常高程度的理解 就是你要知道現在這顆球被這個杯子蓋住 然後這個杯子一旦移動了下面這個球會跟著動 這種程度的了解跟GPT-4V對於世界的了解是完全不同level的 你如果要從一個技術的角度來解釋的話 這個就是三維跟四維的差別 直接多一個維度出來耶 因為一張鏡子的圖片是三維的嘛 就是他有長、寬跟顏色嘛這三個維度 那影片是四維的 他是長、寬、顏色跟時間 因為一部影片就是很多張圖片疊在一起嘛 所以說就加了一個時間的維度進去這樣 反正我覺得今天一個大型元模型要做到這件事情 絕對可能 我覺得是有可能的 就是我們從一些現在很流行的一些Video Diffusion Model嘛 就是這邊可能比較技術一點啦
(26:16~27:16) 反正就是最近在Machine Learning的圈子很流行一些就是 可以產生影片的模型 可以產生動畫的模型 就是你輸入一串文字 或者是你給他一張靜止的圖片 他就讓這張圖片開始動起來 或者是產生出你文字敘述的影片這樣 那這些模型呢 我自己覺得他們對於這個世界物體怎麼運作的理解呢 已經是比GPT-4V對於圖片的理解更強了 就是他們會知道一些非常基本的就是 在風吹的情況之下 頭髮要怎麼樣飄之類的 所以這些Video Diffusion Model就已經證實了 就是AI是可以理解影片的這件事情 只不過他們對於影片的理解呢 目前還是在一個非常淺的階段 就是他們只能產生可能4秒左右的影片 不過我覺得哪一天AI對於這個長影片的了解到 Gemini Demo的這種程度 我覺得是非常有可能的 絕對做得到的 所以我那時候看到Google Gemini Demo這個影片 我就想說 哇!They did it! 他們做到了
(27:16~28:16) 結果呢 唉沒想到全部都是騙人的 就是他們其實有發一個部落格文 很詳細的在講說 他們這個Demo裡面的所有操作呢 具體來說他們是怎麼做到的 他們是寫了哪些Prompt進這個模型當中 然後完全沒有任何Video 從頭到尾都是靜止的影片跟文字而已 所以也沒有Audio嘛 所以說他影片中秀出那個影片 以及他在用聲音在講話 完全都是假的 他就是打字 跟這個靜止的圖片而已 然後我們來看一下他這個杯子長球的實驗 究竟是怎麼做的 他們的做法呢 就是文字跟靜止圖片的Fuse Shot Prompting Fuse Shot Prompting在講的就是 你在問這個AI問題之前呢 你先給他兩個例子 幾個例子啦 不一定是兩個可能五個例子 十個例子八個例子 但他這邊是使用兩個例子 然後他最一開始呢 是有先把設定跟Gemini講 就他跟Gemini說 第三個杯子下面現在有一顆球 光是這邊就跟Demo不一樣了嘛
(28:16~29:17) 對吧 在Demo的影片中 他是使用者什麼話都沒講喔 他直接放一顆 默默的放一顆球到桌上 然後把三個杯子放到桌上 其中一個杯子蓋住那個球 然後Gemini這邊就直接講話 他Gemini就說 啊你是想要我猜哪顆球 現在在哪個杯子下面對不對 狗屁啦Gemini哪有這樣講 明明就是你跟他說的 然後再來是 因為Fuse Shot Prompting嘛 他就給這個Gemini兩個範例 他給什麼範例呢 他一個範例就是三張圖片 跟兩句話 一張圖片是顯示 使用者把左手跟右手放在這個 球杯上面的這個樣子 那第二張圖片呢是顯示 這個使用者對調了這兩個杯子之後 他手的姿勢 手的姿勢會反過來嘛 就有點不一樣這樣 然後第三張圖片呢就是 三個杯子然後沒有手的圖片 然後下面的兩句話呢 第一句話就是寫說 Swap 2 and 3 就是我現在正在進行 第二個跟第三個杯子的對調 然後第二句話呢就是在講說
(29:17~30:20) Current state empty ball empty 也就是在跟Gemini說 目前呢對調完了之後呢 現在三個杯子的狀況是 空的然後有球然後空的 然後他給了Gemini兩個例子之後呢 他就給他三張圖片 然後一張圖片就是 他要他抓著兩個杯子嘛 就是要換那兩個杯子 第二張圖片就是 那兩個杯子換完之後 他手的樣子 然後第三張圖片就是 三個杯子沒有手 然後接下來就讓Gemini 自己把話接下去 Gemini就說 Swap 1 and 3 你是在換1跟3的杯子 然後Current state是 Empty empty ball 然後耶!答對了 我跟你講這件事情 我沒有測試過 但我保證GPT-4V也做得到 好不好 唉!跟我想像中真的是差太多了 不只是整個模態變了 就是不是影片跟聲音 而是純粹的這個圖片跟文字 而且他們還使用非常引導式的問答 就是用Fuse shot的方式嘛 然後這個答案組合又這麼少 你就三種可能嘛
(30:20~31:21) 杯子球只能在左邊中間或右邊嘛 那這種情況下就算他亂講 也可能講對啊 總之啦我覺得這些Demo的影片 你Cherry pick是很正常的 Cherry pick就是你 從很多的範例中 你挑出最好的那些範例 我覺得這是很正常 大家都會做這件事情 但我覺得Google這次的Demo 他不只是Cherry pick而已 他有誤導人的意思 他又誤導他們是使用Video跟Audio 在進行prompting 那這完全就是錯誤的 那這種就是我不能接受的Demo 就是我覺得你這樣已經不是誇示了 你是完全是在說謊 那了解到了這件事情之後呢 他還有其他一些影片是在Demo 專門Demo他的這個Audio的modality 就是他可以理解人講話的語氣之間的一些微妙差異 甚至是中文的聲調差異 這邊呢我都先畫上了一個問號 所以說我一開始才會跟大家說 他四種模態 文字、圖片、聲音跟影片 聲音跟影片這兩個模態
(31:21~32:21) 我要畫上一個問號 我在沒有自己親自測試過之前 我都會先畫上一個問號 但是好像投資人沒有被影響太多啦 其實Google在發表了Gemini之後 他們的股價飆了將近5%吧我記得 然後在這件造假事情爆出來之後呢 他們股價是跌了一點回去 但沒有說跌很多 那大部分的媒體呢應該就是講到這邊而已 就是他們講到說 Google這個六分鐘的Demo影片基本上都是假的 就這樣而已 但是故事並沒有就此結束 就是我有自己去看他們的technical report 他們有出一個60頁的technical report 然後我去詳細的看了一下 他們究竟是怎麼樣去衡量Google Gemini 跟其他GPD-4的成績的比較的 那這邊呢我又看出了一些他們浮誇的地方 首先我們在衡量這些大型元模型的時候 我們會用很多種不同的benchmark 這邊benchmark我在講的其實就是一種測驗啦 對我們會用很多種不同的測驗
(32:21~33:23) 來測試這些大型元模型各方面的能力 包括它的樹理能力 它的普遍知識能力等等 那在這些benchmark當中呢 Google最愛說嘴的兩個數據就是 Gemini在32個benchmark裡面 它其中有30個都是state of the art state of the art就是全世界第一的意思 就是他們是第一名的模型 然後另外一個數據呢是他們在MMLU MMLU剛剛我有講過嘛 就是它是其中一個benchmark 那這個benchmark是最常用來 衡量一個大型元模型 它對於世界的普遍知識的 因為這個benchmark裡面 基本上它就是很多很多的問題 那這些問題涵蓋了57個科目 就是從比較一般的人文、社會、科學、數學 到非常專精的像是法律跟倫理等等 然後它的題目難度呢從小學等級、初階等級 到最難的這個高階的職業等級全部都有 所以它是對於一個大型元模型
(33:23~34:26) 它一般知識的一個非常全面性的一個測驗 那Google最愛說嘴的一個資料就是 他們在MMLU的成績呢是90% 意思就是說他們答對了90%的問題 與此同時呢GPT-4是86.4% 所以看起來好像是Gemini比較好喔 然後好了大概3%多喔 但是他們在這兩個成績下面有寫一個小字 他們在Gemini的90%下面呢 它寫了一個小字寫說COTAT32 然後在這個GPT-4的成績下面呢 它一樣寫了一串小字寫說FiveShot 這個小字的意思是在講它這個成績 是透過怎麼樣的測驗而取得的 所以說他們測試Gemini的方式 跟他們測試GPT-4是不一樣的 Gemini是用COT的方式得到了這個90分 然後GPT-4則是用FiveShot的方式得到了56分 然後我點進他們這個Technical Report裡面看 它這個詳細的數據
(34:26~35:26) 我發現當Gemini一樣用FiveShot的方式進行測試的時候 就是跟GPT-4一模一樣的方式進行測試的時候 它的成績是83.78% 所以說它比GPT-4還爛 這樣真的是很賊啊對不對 你真正Apple to Apple的比較的時候 你還是輸GPT-4嘛 對吧 那你之所以會贏完全是因為這個COT的方法比FiveShot的方法更好 那我這邊就來解釋一下COT跟FiveShot是什麼好了 首先FiveShot就是 其實就是我剛剛講的FiveShot Prompting 那它是用FiveShot就是五個例子這樣 意思就是說在你的AI在回答問題之前呢 你先給它五個類似問題的例子 例子就包括問題是什麼答案是什麼 問題是什麼答案是什麼 然後最後你再問你的問題 然後讓它給出答案這樣 那麼這個我已經講過很多次了啦 但就是這些神經網路這些大型圓模型 它在做的事情就是模式辨認而已 所以說你先給它一些非常接近的模式
(35:26~36:28) 然後直接讓它Copy這個模式 然後辨認出其中的規律 這樣是會大幅增加這些大型圓模型的成效的 就是比起你完全不給它任何例子 你直接問它一個問題來說 你給它一些例子它可以答得更正確 那這個就是FiveShot嘛 那COT又是什麼呢 COT是升級版的FiveShot Prompting COT的全名是Chain of Thought Prompting 就是怎麼講呢 邏輯鏈的Prompting 就是它一樣在Prompt的時候 它會給這個LM一些例子參考 但它不是簡單的給這個問題跟答案的例子 它一樣是給一個問題 問題是一樣的 但它在答案這邊它會做一些更改 它並不是直接寫一個答案而已 它是把這個答案怎麼得出的整個邏輯串全部都寫出來 那因為這些大型圓模型就在做模式辨認而已嘛 它有時候如果比較邏輯比較複雜的問題 它可能沒有辦法從小小的五個example裡面 就單純從這個問題答案的Pair找出裡面的規律
(36:28~37:30) 但你如果把在答案這邊把整個邏輯串都寫出來的話 它有更多可以follow的這些線索 更多可以follow這些規律 然後它就可以更好的找出正確的答案這樣 所以Chain of Thought完全就是升級版的FiveShot Prompting 然後你拿Chain of Thought跟FiveShot Prompting比 一定是Chain of Thought會比較好 這是不公平的比較 而且你知道嗎 Google做的不只這樣 它不是使用一般的Chain of Thought而已 它是使用Chain of Thought at 32 這個又是一個升級版的Chain of Thought 就是它並不是透過這個Chain of Thought的一些範例 然後產出一個答案 那個就是正確答案 它是使用這個Chain of Thought的過程產出32個不同的答案 然後在這32個不同的答案裡面 看說哪一個回答是占多數 然後占多數的那個回答才會被當成最終回答 所以他們是使用了FiveShot Prompting的進階版Chain of Thought 然後又使用了Chain of Thought的進階版Chain of Thought at 32 但其實這還不是全部
(37:30~38:30) 就是他們其實是使用Chain of Thought at 32的進階版 叫做Chain of Thought at 32 括號Uncertainty Routed 這個Uncertainty Routed是Google自己開發出來的一個方法 就是Chain of Thought不是Google開發的 那個很久以前的論文就有了 然後FiveShot Prompting也不是 但他們自己找了這個最適合Gemini的一個叫做Uncertainty Routed的方式 這個方式就是在講說 我剛不是說Chain of Thought at 32 就是它同一個問題答32次 然後投票嘛 然後最多回答的那個答案就是正確答案 那有加上Uncertainty Routed的意思就是說 它會設定一個門檻 就是假設這個最多數的票沒有通過這個門檻的話 那它就直接把Chain of Thought的這個process shut down 它直接讓這個Gemini對於問題做一個直接的回答這樣 那他們這樣為什麼會比較好?不知道 反正他們測出來就是這個特殊的方法 在Gemini上面就是可以發揮 就是可以讓Gemini成績刷到90分
(38:30~39:32) 就是這樣 所以這多麼的hypercritical 就是你看GPT-4 人家是用Five Shot Prompting 在做這個MMLU 然後做到86分 那你咧? 你用Five Shot Prompting的進階版COT 然後COT的進階版COT at 32 又進階版COT at 32 Uncertainty Routed 然後做到90分 然後你把這個數字寫得這麼大 這樣你良心過得去嗎? 不過這邊有個細節我也是要講一下 就是Technical Report裡面也有寫出說 GPT-4用Chain of Thought at 32的方式進行prompting的成績 那他們的成績是87.29% 所以說也是沒有到90% 但是它下面的括號 有一個括號叫做VIA API 意思就是說這個不是OpenAI做的測試 也不是其他第三方做的測試 這是Google他們自己使用GPT-4的API做出來的測試 所以你覺得有沒有可能是Cherry Pick? In this case是Pick Bad Cherry
(39:32~40:32) 就是挑GPT-4比較爛的成績 有可能啊 就你拿Chain of Thought at 32 跟Five Shot在比這種事都幹得出來了 他們一定也敢Cherry Pick 然後我剛剛說除了這個MMLU 是他們常講的一個數據以外 另外一個數據就是 他們聲稱在32個benchmark當中 他們有30個benchmark是State of the Art 但你實際去看這些數據你就會發現 它贏是有贏啦 但大部分都贏不到3% 然後很多其實甚至贏不到1% 然後很多這些測試呢 其實也都有掛號VIA API 也就是這些都是Google他們自家做的測試 所以這些測試都是你們自己做的 然後差距又這麼小 會不會是Cherry Pick的結果 我覺得很有可能 我也不是說就是只要是他們自己做的測試 我就不信 我也不是這樣 假設今天它跟GPT-4的成績差距是10% 那我絕對相信 我覺得10%是不太可能Cherry Pick出來的結果 但是他們差這麼小 我覺得對我來說
(40:32~41:33) 它還在那個誤差值之中 就是它是可能是一個統計的誤差 所以說我不會覺得Gemini有贏過GPT-4 所以我一開始才會下那樣的結論 就是我覺得Gemini Ultra就是等於GPT-4V這樣 然後大家也不要會錯意 我並不是在說Gemini很爛 我只是說我們原本對於Gemini的期待很高 然後後來發現可能它沒有那麼強 然後同時他們也有一些可能比較浮誇不實的廣告 但是一個跟GPT-4V同等級的模型已經是非常強了 已經是一項成就了 因為在這之前真的沒有人打得贏GPT-4V 但是同時啊 這也並不代表就是未來的這些最高端的LLM的市場 會被這個GPT-4V跟Gemini評分掉 難說難說 這邊我其實有準備很多的內容 但是我看這個錄音時間啊 應該是沒有辦法在這一集講完 但這邊我覺得我不想要爛尾 就我不想要草草帶過 然後我也不想要都不講
(41:33~42:34) 所以說我決定這邊的部分留到下禮拜的集數再講 這邊的大主題呢就是 Google究竟能不能靠Gemini領先OpenAI 然後它的子問題呢就會像是 OpenAI究竟有哪一些優勢 Google究竟有哪一些優勢 他們這樣一比究竟是誰會勝出 然後除了這個Google VS OpenAI的話題啊 其實也有很多Gemini的亮點是我今天沒有講到的 我覺得我今天好像都是花時間在揭露這個Gemini的惡行 但是呢其實Gemini是有很多亮點的 就是像是Gemini本身它的模型 它是一個Dance Model 然後GPT-4是一個Sparse Model 這邊是有一點差異的 但這邊很技術啊 就我覺得這個我下禮拜再來講 那同時呢 Google在訓練Gemini的硬體也是蠻不一樣的喔 這邊我也是在他們的Technical Report裡面看到 他們有蠻詳細的記載 別家公司通常都是使用業界的標竿 都是使用NVIDIA的GPU 要嘛是A100或是H100 或者是未來的H200嘛
(42:34~43:35) 但是Google這是在訓練Gemini 他們是使用自家的TPU 這並不是因為他們沒有NVIDIA的GPU喔 他們絕對有他們絕對有一大堆 但是他們卻是使用自己的TPU 那這邊也是有很多有趣的細節啦 就是他們究竟是怎麼讓這個TPU的訓練 做得跟NVIDIAGPU的訓練一樣高效的 這個是我覺得我們可以詳細聊聊的 然後除了硬體設備以外 另外一個大亮點就是我一開始在 這集一開始有講的就是Alpha Code 2 講錯Alpha Code 2 這個由Gemini Fine Tune出來的一個Coding模型呢 它其實有個亮點 它跟一般的那種 它跟一般的那種像是Lama的CodeLama不一樣 它並不是一個單純的Coding模型而已 它是有配合一些額外的一些幫助思考的系統 我覺得這邊是非常有趣值得討論的 因為我覺得Alpha Code 2很可能就是Google的QSTAR 大家還記得嗎? 就是在應該是在兩集之前吧
(43:35~44:37) 我有講一下這個OpenAI流出的一個神秘技術 叫做QSTAR 雖然說OpenAI官方沒有說這個QSTAR是什麼 但是我們都有很多的猜測 然後我們從各地的線索呢 可以大概拼湊出這個QSTAR應該是什麼樣子的一個技術 有興趣的可以去複習一下EP16 反正我覺得這個Alpha Code 2呢 真的很有可能就是Google的QSTAR 那麼以上這些全部的話題呢 就是包括Gemini一些我還沒介紹到的亮點 然後還有Google跟OpenAI究竟誰贏誰輸 這一部分的討論呢 我們就全部流到下禮拜的科技浪 我再來詳細跟大家解說 因為這些東西講一講講完了應該也是一整集啊 裡面有許多重要然後又有趣的資訊 我覺得大家可以期待一下 那在這集結束之前呢 也是跟大家最後提醒一下就是 我們剛剛講到的這些拿來測試原模型的這些benchmark 這些所有benchmark都不是完美的指標 這邊我之前也強調過了嘛 就是這些benchmark真的有非常多的缺陷
(44:37~45:37) 其中一個最大的就是這些benchmark多多少少 應該都有混入訓練資料當中 像是我們剛剛講到的Gemini拿90分的這個MMLU嘛 這個MMLU2020年的9月就出來了 它已經在網路上活躍了三年了 你覺得Google在網路上爬訓練資料的時候 它沒有爬到MMLU嗎 它一定有爬到MMLU 所以我覺得大家在詮釋這些benchmark的成績的時候 也要抱持著一絲懷疑之心啊 我看現在很多媒體真的就直接拿這個成績 就直接拿它當作絕對的指標來比較這些模型的好壞 我覺得這樣子不太好啊 那更好的做法有什麼 第一個就像是XAI在做的 XAI就是馬斯克的AI公司嘛 它有個聊天機器人叫做Grock 那他們在測試Grock的時候呢 他們也是有跑這些傳統的benchmark 但是他們跑完了然後結果寫完了之後 他們下面又加了一段 他們寫說 At this point這些benchmark都已經mixed into the training data了 不好意思 剛好我在回想那個原文
(45:37~46:38) 然後邊想邊講 然後就一直中音夾雜 反正它的意思就是說 我們知道這些benchmark應該多多少少有混到我們的訓練資料當中 所以我們用一個全新的測試來測試這些模型 這個全新的測試呢就是 他們在這個Grock推出的大概幾個禮拜前還是一兩個月前 在匈牙利的一間高中的一個段考考題 我記得是這樣 細節有可能是大學我也忘了 反正他們就是拿一個才剛出爐的一個測試題 來分析這些大型元模型的能力 我覺得這樣子是有比較好一點 但是當然這樣子也麻煩很多啦 所以說大部分公司都沒有做這種事 對了我今天早上看到一個超好笑的 就是有人問Grock一個可能比較危險的問題 然後Grock就回覆說 根據OpenAI的policy我們沒有辦法回答這個問題 意思就是說他以為他自己是ChatGBT 所以看來這個Grock的訓練資料呢
(46:38~47:39) 不只是參了很多的benchmark 他們還參了很多的ChatGBT output 好這題外話 反正你要檢測這些大型元模型 你要嘛就是我覺得你應該要做一個比較新的測驗 這樣子會比較好 然後另外一個方法呢就是使用Chatbot Arena的方式 這個Chatbot Arena是一些Berkeley的人做的一個專案 他們就是有一個LM的排名LM的leaderboard 那這個排名的決定方式呢 並不是靠一些benchmark的成績 而是靠他們的elo rating elo rating通常是我們拿來鑑定一個西洋棋手他的能力的一個指標 他的核心概念就是 每個人都會有一個代表他自己的分數 那這個分數會經過他每一次的比賽進行調整 調整的方式就是很直覺 你如果下贏了比你還強的人 那你的分數就會多很多 那你如果下贏了就是一般的人 或是甚至是比你弱的人 那你的分數可能沒什麼改變 然後反之亦然啊 就是你輸給很弱的對手的話 你的分數就會降很多 那你輸給比你強的對手的話
(47:39~48:40) 你的分數也不會改變太多 那這個elo rating呢對於chess來說是一個最公平的一個指標 因為你不可能說你今天賽道不小心下贏了世界第一的棋王 你就立刻變世界第一 不可能這樣嘛 你一定是根據你每一盤棋的結果 然後你每一盤棋的對手 慢慢的調整你自己的分數 然後最後會達到一個平衡點 然後同時這個分數也會是動態的 就是你如果一直在變強 你的分數就會跟著慢慢變強 因為你慢慢可以打贏越來越多人嘛 那這個chap bar arena也是一樣的概念 只不過這邊的輸贏是讓使用者盲測決定的 就是使用者同時會跟兩個不同的LM講話 但使用者不會知道這兩個LM是哪一個模型這樣 然後使用者要純粹從他得到的這兩個LM給他的答案裡面 挑出一個他覺得比較好的 那被挑中的那個LM當然就是那一局的贏家 然後他們真的是用這個方法在很多人身上跑的這個測試 應該說很多人是自主進行這個測試啦
(48:40~49:26) 然後他們就算出了一個所有的LM他們的elo rating 那這邊就是簡單跟大家介紹到這邊啦 就是跟大家說你不要只看這些benchmark 其實有其他東西可以看 好那這禮拜呢其實非常的開心 因為我們科技狼久違的又進入Apple Podcast綜合排行的前五名了 雖然說我們錄音的這個時候呢我已經掉出前十了 然後呢我會進入前五呢 也純粹是因為我有一個IG影片在推廣這個我的podcast 但是真的是久違的又看到這麼多新的觀眾來聽我們的podcast 真的是非常的開心 然後也謝謝大家持續的在各個平台呢幫科技狼留言跟評分 我看了也都是非常的開心 那最後也祝大家有個愉快的一周 在2023年的最後一個月也要好好加油
