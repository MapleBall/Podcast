(00:00~01:02) 【音樂】 哈囉大家好,歡迎收聽科技浪,我是主持人哈利 科技浪是一個白話跟你聊科技的Podcast 希望可以用簡單易懂,但是又深入的方式 帶你了解時下最火的科技話題 本期節目由知識衛星贊助播出 那今天呢,要跟大家推薦一堂Lydia老師的UX商業價值實現課 這個UX呢,當然指的就是User Experience 那我覺得,擁有好的UX思維真的是極其重要的一件事情 因為一個產品在市場上的表現,很大程度是UX決定的 一個產品它UX好壞,會直接影響許多跟營收相關的重要指標喔 包括什麼,包括轉化率,包括用戶流失率,客戶滿意度跟忠誠度等等 那你今天呢,如果只專注於開發產品的功能跟性能 而沒有真正去了解你的消費者,並且做出好的UX 你很可能就沒有辦法成為消費者的第一選擇
(01:02~02:05) 那我覺得,如果你有在看一些Y Combinator的內容 這個Y Combinator就是一個世界頂級的新創加速器 那你有在看他們的內容,你就知道他們一天到晚都在說UX跟客戶中心思維是非常重要的 那我今天要推薦的這堂UX商業價值實現課程的老師Lydia 她是一個資深的UX大師喔 她有非常豐富的業界經驗 她曾經任職於設計顧問公司Frog Design 就是賈伯斯指定設計第一個Mac的那一間公司喔 然後她也曾是Yahoo國際產品設計的負責人 那除此之外,她也有一些UX顧問的經驗 她曾經是新展銀行、中華電信、福特汽車、招商銀行等知名企業的指定顧問 那在這堂UX商業價值實現課中呢 Lydia老師她會分享她15年來在國際企業的UX設計跟企業顧問的經驗 然後集結必備的UX研究工具百科 來教大家如何定義問題、設計解決方案到商業驗證 最終創造出更好的使用者體驗
(02:05~03:07) 那課程中她也會拆解很多業界實踐UX的成敗關鍵點 來教你如何將使用者價值轉換為商業成功的策略 那同時呢,這堂課裡面也會有一些質牙分享 Lydia老師她會分享她作為跨國設計總監的一些求職跟身千技巧 那我覺得這堂課不只適用於UI、UX的從業人員 也非常適合電商行銷、產品經理、甚至軟體開發等職場上 因為不管你是不是直接做UX的工作 一個好的UX思維總是可以提升你的職場價值的 那這堂課的連結呢,我就放在本集的資訊欄裡面 建議大家點進去看一看 那你最後如果決定想要購買的話呢 建議你越早買會越划算 因為從即日起到9月1號呢 他們會有限時優惠低於4折 那同時呢結帳的時候你可以輸入我的優惠碼 WAV350 WAV350 就可以再折扣350塊 有興趣的朋友就趕緊到本集的資訊欄裡面點連結看看吧 那本集業配就到這邊結束 謝謝知識衛星的贊助 那有在關注我IG的人都知道
(03:07~04:09) 這禮拜哈利說的IG發生了一件大事 就是我們的頻道被倒了 那當然就是我們有在四個小時內搶救回來 然後目前帳號也完全是安全的 但是非常悲慘的一件事情呢 就是過去的影片全部都被刪掉了 那我覺得這禮拜當然很明顯啦 就我當然過得不是很好嘛 那有非常非常多負面的情緒 就是從憤怒啊到傷心啊難過啊 各種就是負面的情緒這樣子 很抱歉就是開頭就跟大家講這麼depressing的東西 但我覺得我必須還是得get this out of the way 因為我想跟大家說的就是 這些影片我們接下來的一兩個禮拜 就會想辦法全部重新上傳 然後會非常安靜的重新上傳 就是不會讓大家一直滑到 但是想回去看影片的人也都找得到 就想辦法做到這一點 那當然能不能做到我們就再看 接下來我們會做一些小規模的實驗
(04:09~05:09) 但希望可以做到這一點這樣 那這件事我覺得今天我就先不comment太多 因為我現在還沒有辦法好好的把這件事情講完 因為我一講我就會覺得超生氣 然後超難過各種情緒 所以我覺得這件事情先這樣 那上禮拜我是跟大家說 那個禮拜基本上都沒有什麼科技的大事發生 所以我就挑了一個比特幣的話題來跟大家聊一下 那大家其實也是蠻喜歡的嘛 那我覺得其實我自己講起來是蠻開心的 我也不想要每一天都在跟大家講AI Google Meta OpenAI他們怎麼新模型 怎麼樣誰比較強GPD-4 我真的不想每一天都講AI 雖然說知道AI的最新發展是重要的 然後我也會想辦法讓大家隨時跟上最新最重要的資訊 然後確實在目前的時代 大部分這種科技業的最新最重要資訊 最重要資訊都是AI資訊
(05:09~06:11) 所以我們頻道大概八成都是在講AI 這個是無可避免的嘛 但我覺得至少我們要有20%來講一些其他的科技 讓我自己跟各位都換換口味 那上禮拜這集比特幣呢 我看是得到了蠻多觀眾的好評 所以呢區塊鏈這項技術的這個系列呢 續集已經確定被訂下來了 但是之後下一集會在什麼時候上映 我覺得就再看吧 看有哪一週沒什麼大新聞 我們就來講講這個區塊鏈的續集 那上禮拜蠻好笑的當然就是 我Podcast裡面一錄完說 我覺得這禮拜都沒有什麼大事發生 然後馬上就有一件大事發生了 就是Windows大當機的事件嘛 那當然我 因為當然是在錄音之後發生了 所以我也沒有辦法把這個事情再補錄進去之類的 然後我週末也是有自己的行程 然後這禮拜呢 又有很多有趣的新聞可以聊聊 我覺得是蠻多AI領域的重大發展的
(06:11~07:11) 所以我覺得這個Windows大當機的事件 我就當作一個遺珠之憾吧 我覺得比起它 我們今天更應該來聊聊這個禮拜的AI重大發展 那我覺得這個禮拜主要有三個蠻重要的AI發展 第一個呢也是最大的一個 就是我們的潮南Zuck 他終於釋出了他 由始以來最大的開源AI模型 這個405 billion的LAMA 3.1模型 那這邊當然我們 我待會會詳細的介紹他究竟是什麼 整集的重點也會放在這個時間 但另外兩件事呢 我覺得也是挺有趣的 首先有這個OpenAI呢宣布要推出他們的AI搜尋引擎 叫做Search GPT 然後還有這個Google DeepMind 他們發表了他們能夠 在今年的數學20P拿到銀牌的兩個數理超強AI 包括Alpha Proof跟Alpha Geometry 2 那我覺得數學20PAI的這個新聞呢
(07:11~08:11) 我覺得我們在今天講LAMA的部分 其實應該會有一部分會帶到 所以就是我們可以在那個時候一併聊一聊 那OpenAI的這個Search GPT AI搜尋引擎呢 我相信你有在關注AI的 一定都對於這個工具不陌生啦 這是很早以前大家就有在使用的一個概念 反正就是你輸入一個問題之後呢 他會先去幫你爬一些可能網路上相關的一些文章一些文本 然後再使用GPT4O根據這些文本撰寫一個直接的回答給你 就等於是一個進階版的Google Search 就你不用Search完了之後 你再點進去一些網站找你的答案是什麼 而是這個AI會直接把答案從這些網站裡面找出來給你的概念 那當然我說這件事情不行嘛 就是因為我們知道Perplexity這間公司就是在做這件事情嘛 一個最近非常紅的一間新創嘛 我自己本人也有在使用 那Google Search他們自己也有推出這個功能
(08:11~09:12) 他們是把它叫做AI Overview 現在你在做一些Google搜尋的時候呢 你會先得到一個AI幫你整理的一個答案這樣 就是一模一樣的事情 那OpenAI自己以前也有在做類似的事情 就是他們有把ChatGPT連到Being Search嘛 就是你問他一些可能會需要用到網路的問題的時候 就比如說告訴我最近台積電的財報表現如何之類的 他就會先跳出說正在搜尋網頁 然後最後跳出說搜尋到四個網站 然後根據那四個網站的文本呢給你一個回答 那OpenAI這次要推出的SearchGPT呢 它的概念上應該是差不多啦 反正一定就是從網路上找出一些文本 然後用LLM去解釋 一定都是這個概念 但他們這次把它包裝成一個新的功能 或甚至是一個新的產品 我覺得是因為他們在使用者體驗上面可能做了很多的優化 他們想要做出一個完全不一樣的使用者體驗
(09:12~10:13) 因為目前的這個ChatGPT的對話框呢 它的UI確實沒有為Search做最優化嘛 那我們知道有在做優化的就像是Perplexity 那你看Perplexity它的整個UI 它的整個使用者體驗就跟ChatGPT的對話框是蠻不一樣的嘛 那同時我們也知道 一個使用者在使用這些搜尋引擎時候的體驗 真的會很大程度的影響他使用這個東西的頻率 以及它的黏著度嘛 這個Perplexity的CEO Aravind 他前一陣子上Lex Freeman Podcast他就有講到這一點 就是說Perplexity網站的每一個小細節 這個按鈕要放在哪裡 然後related question要放哪一些 要怎麼放出來 使用者要怎麼滑 要怎麼完成這個句子 每一個小細節他們都是精心設計過的 因為每一個小細節真的都會有很大的影響 所以我真的覺得OpenAI的SearchGPT呢 這次應該真的是為了Search這個產品呢 做出一套專屬的UX UI
(10:13~11:15) 然後想辦法把這個東西optimize到最好 然後把它融入ChatGPT當中 那如果退一步想他們為什麼要這麼做呢 很明顯的現在就是要把所有的客戶全部都留在ChatGPT嘛 因為現在有些人遇到這種需要一些 有資料支持的Search的問題的時候呢 會跑到Perplexity 像我,我就是這樣 我今天想要快速驗證某一件事情 然後這件事情可能是在 這個OpenAI的訓練資料的knowledge cutoff之後 就是像是GPT-4O的knowledge cutoff 好像就是去年年底吧 可能去年年底11月12月之類的 那我如果想問的問題是 今年年初的問題 或者是最近這兩個月的問題 當然我就會需要一個Search的功能來輔助 那這個時候我就會使用Perplexity 因為我長期使用使用慣了 雖然說它Perplexity當然也是會講錯話 但它整個體驗我是蠻喜歡的 那假設今天ChatGPT的網頁呢 也有這個Search的體驗 然後也是非常好的話
(11:15~12:15) 我可能真的就持續的留在ChatGPT這邊了 那同時呢 每次看到這種OpenAI釋出 為了賺錢的feature呢 我都會覺得有點感慨啦 因為像這個SearchGPT呢 它純粹就是為了讓OpenAI賺更多的subscription revenue 讓更多人留在ChatGPT那邊 然後付每個月20塊 那這個純粹就是為了賺錢的feature呢 當然不是說不好 甚至對於消費者來說是更好的一件事情 因為企業之間的競爭越多 消費者的選擇越多 然後能夠得到更好的服務嘛 但總是會覺得有點感慨 就是因為 哇OpenAI真的是離它當初創立的初衷是越來越遠了 最一開始是一個為了做出AGI的一個research lab 那現在是一個盈利導向的AI公司 因為你看這個SearchGPT 它跟AGI有沒有什麼關係 根本沒有傻屌關係 它就是為了抓住更多使用者賺更多錢的東西 就是非常單純
(12:15~13:15) 那當然有這種感慨也不是第一次了啦 我們都知道從這個SearchGPT出來之後呢 Sam Almond就一直把OpenAI往這個方向帶嘛 那今天我們要講的這個主題呢 潮男祖克柏的AI就是完全不一樣的方向 那我們都知道Mera它有推出一系列的大型原模型的AI嘛 然後他們是把它稱作拉馬 中文是叫做落馬 那今年四月的時候呢 他們就推出了最新的拉馬3家族的模型 有三個不同大小的模型 分別是8B、70B、405B 那這邊的B呢當然指的就是Billion、10億 就是指它的參數量有多少啦 假設一個8B的模型呢 它就是有80億的參數量 那參數量越多就代表這個模型的腦容量越大 可以學會更多的知識 那今年四月拉馬3剛出來的時候呢 我當然有錄一集科技樣來跟大家聊聊 我記得是EP37嘛 那時候就有跟大家說 這個405B的模型 他們那個時候還不會釋出
(13:15~14:15) 他們只有釋出8B的跟70B的 因為這個405B的模型 他們說還在訓練當中 他們只有拿其中一個訓練到一半的版本 去大概跑一些測試 跟大家看它的成績是多少這樣 那這個禮拜拉馬3.1的釋出呢 他們就是真正釋出了這個405B的模型 由來最大的開源AI模型 然後同時呢 這個比較小的8B跟70B的模型呢 也被tune得更強了一點了 那同時呢Meta也有推出一篇 超爆長資訊量超級豐富的一個技術論文 總共92頁 然後裡面把這個拉馬3整個模型 它的模型架構、模型的訓練的過程 各個步驟、他們訓練的硬體設施 他們的安全措施 全部寫得超詳細 我真的覺得我好愛祖克柏喔 不管他是西元還是外星人什麼的 我真的好愛他喔 尤其他現在變得那麼潮 還會帶GoChain對不對
(14:15~15:16) 真的有夠帥的 他這間公司花了幾億甚至幾十億 在build這些硬體設施 在train這個team 在train這些模型 然後全部東西都寫在論文裡面 公開給大家看 免費看 然後模型也免費使用 我覺得真的太佛了好不好 怎麼會有這麼好的事情 那我週末真的是 我沒有當然沒有一字一句的全部看完啦 但我有挑重點把整個論文基本上都 我想看的地方都看過了 很多地方精讀 有些地方掃過這樣子 那今天就整理了很多的我的想法 然後我看到的一些亮點來跟大家討論 這是我們待會要講的 那當然除了這個以外呢 祖克柏還有寫一封信 跟大家說 為什麼他們他這麼相信開源AI 為什麼他們會花這麼多錢train這些AI 然後免費送給大家 為什麼他們覺得這個 這些拉馬這些模型呢 會是AI的未來 那我覺得這邊當然也是非常多可以聊的 所以這邊我們就放在最後講
(15:16~16:16) 那在我們開始深入解析論文之前呢 我還是先給大家一個拉馬3.1 或者說拉馬模型的一個overview 好怕大家沒有跟上這個新聞 那首先這些模型的表現如何呢 他們究竟多強 首先大家最關心的應該都是這個405B的模型 因為這個是第一次推出的 然後也是有史以來最大的開源社群模型 有4050億的參數喔 那他的表現呢也沒有讓人失望 他已經基本上跟OpenAI跟Anthropic最強的模型 也就是GPD-4O跟Claw 3.5 Sonnet 已經是差不多同等級的模型了 你如果單純看benchmark的話 也就是這些各個領域的標準化測試 像這個MMLU這種基本知識的測試啊 或者是Human Eval這種Coding的測試啊 GSMAK這種數學的測試啊 在各個領域呢 他基本上都跟GPD-4O跟Claw 3.5 Sonnet是 在同一個等級的
(16:16~17:16) 甚至在某一些領域他還做得更好 然後再來更重要的是人類評測的成績嘛 因為我們知道那些benchmark他 其實有很多的問題啦 那那個數字只能大概做一個參考 所以人類的評分呢 我覺得是比較有參考價值的 那臉書他們自己有做一個內部的human evaluation 那評出來的結果呢就是 405B跟Claw 3.5 Sonnet是表現一模一樣的 幾乎一模一樣啦 甚至微微贏0.幾%這樣 那跟GPD-4O比起來呢 是輸了一點點這樣子 就是他們在大概20%的問題之中呢 是贏過了GPD-4O 但是有30%是輸給GPD-4O的 那剩下的50%呢就是平手 那剩下比較小的這兩個模型呢 8B跟70B他們也沒有讓人失望 他們基本上是屌打同樣大小的其他模型 完全把人家吊著打 就是基本上在每一項benchmark上面他們都贏
(17:16~18:18) 而且在有些benchmark上面還贏非常多 所以我覺得這次405B模型的推出 它真的是AI領域的一個重大里程碑 因為它顯示了開源社群的AI發展 已經跟幣源社群只差兩個月了 它已經基本上已經要追上了 為什麼是兩個月呢 因為GPD-4O是5月推出的嘛 然後Claw 3.5 Sonnet甚至可能 什麼時候啊6月還是7月初的時候才推出 那現在到了7月405B竟然已經跟這兩個模型一樣好了 我覺得真的是非常的驚人啊 因為我們都有目睹從LAMA的第一代出來 LAMA的第二代出來到第三代出來的整個過程 那從LAMA第一代出來的時候呢 那個時候GPD-4根本是遙不可及啊 那個時候大家都感覺開源社群跟幣源社群的差距呢 可能差了一年甚至兩年都有 就是非常落後這樣子 但是在接下來的一年之中呢 OpenAI的幣源模型並沒有進步的那麼快
(18:18~19:18) 但是LAMA卻持續的飛速進步 從LAMA1到LAMA2到LAMA3 到現在LAMA3.1 405B 竟然把跟幣源社群的差距縮小到了兩個月 然後同時祖克柏也說 他們現在已經在訓練LAMA4了 那當這個LAMA4訓練好可能是今年年底或是明年年初 到時候開源社群就會超過幣源社群了 全天下最厲害的模型是一個完全透明 而且你可以免費下載使用的 那當然我很期盼當天的到來 但我們也知道OpenAI跟Anthropic不會坐以待斃 Google也不會 雖然說在這次LAMA的Paper裡面 他們甚至完全不把Google放在眼裡 根本就沒有把Gemini放上來比較 只有比GPD-4跟Claw 3.5 Sonnet 那這些LAMA3.1的模型呢 他們的模型架構也是非常的單純 就是我們所謂的Dense Decoder-Only Transformer 所謂的Dense在講的就是
(19:18~20:20) 它不是一個MOE的模型 因為我們知道最近很多AI的公司呢 他們會出一些叫做MOE的模型 就是把很多很多個小模型把它組起來 然後在處理每一個字 在算每一個字的時候呢 都選擇用一個不一樣的模型去算這樣 這樣子的話就感覺好像你擴充了你的參數量的感覺 擴充你的腦容量 因為你現在變成有可能8個小模型嘛 那8個模型的參數量這樣 但你在使用的時候不會耗用這麼大樣的算力 因為你在算每一個字的時候 你一次可能只會動用兩個小模型這樣 所以說就不會浪費算力那麼多 但這個LAMA3的模型呢 都是Dense Model 就連最大的這個405B的模型 也完全是一個Dense Model 它就是一個超級龐大的模型 每一個字丟進去都是同一套權重在算 再來我說它是一個Decoder Only的Transformer 這個就是非常業界標準的一個做法
(20:20~21:22) 所有的Transformer現在都是Decoder Only Transformer 至少在大型語言模型的這一塊 就是我知道現在很多人講到Transformer 都會拿2017年Attention Is All You Need 這篇Paper裡面的那個圖來講Transformer 但這個也不算是最精準的講法 因為那個時候的論文發表的Transformer 是Encoder跟Decoder都有的Transformer 但現在所有的大型語言模型 都是Decoder Only的Transformer 就是你看這張圖它有兩塊 大家可以自己現在去Google一下 就是你GoogleAttention Is All You Need Transformer 你就會看到這張圖這個模型的架構這樣 基本上就是兩串蔥烤牛肉捲 左邊小捲的那一串是Encoder 這個都是現在都已經省去的部分 我們現在沒有在使用這個東西了 至少LLM沒有在使用這樣 都剩下一個右邊那一串 比較大串的蔥烤牛肉捲Decoder的部分
(21:22~22:22) 所以現在所有的LLM都是Decoder Only Transformer 所以我真的不知道很多人是為了要致敬 Attention Is All You Need的這個原圖 還是說大家覺得就真的不懂說現在都剩下Decoder Only了 很多人還是會用這張圖在講LLM 大家要知道這張圖不是最精準的 只有右邊的烤肉串 所以Lama 3.1就是這些Dense Decoder Only Transformer 其實你這個架構大家都一樣 但裡面有一些東西是可以微調的 就是你可能可以替換裡面的數學函式之類的 但這次Lama 3跟Lama 3.1都沒有做什麼調整 唯一有一個比較不一樣的地方呢 就是他們這次使用了一個比較大的Tokenizer 就是一個128K的Tokenizer 你如果不知道是什麼意思的話 白話來講基本上就是他們這次Lama 3的模型 跟Lama 3.1的模型 他們是有更豐富的詞彙量了 當然他們的詞彙跟我們人類的詞彙定義也不太一樣
(22:22~23:23) 他們的詞彙是Token 他們認得了更多不同的Token的種類了 這邊其實我EP37就有跟大家聊到了 接下來我在講解論文的時候 看有沒有可以提到這一點的部分 沒有提到也沒關係 大家就去EP37聽 很快帶過其他的重點 就包括他們這次的License有做一個改變 他們現在解除了一個限制 讓大家可以使用Lama 3模型的Output 來訓練自己的模型 這一點我覺得很明顯是為了408B這個模型設計的 因為408B的模型一般人真的沒有辦法使用 誰家電腦有超過200GB的RAM 甚至200GB是這個模型被量化到4BIT的大小 它原本大小800多GB 你是誰能用得了這個模型 但祖克柏就一直很強調說 我們知道一般人是沒有辦法輕易的使用這個模型 但一個用法就是
(23:23~24:26) 你可以用405B的模型產出一些很好的訓練資料 來訓練你自己的模型 你自己一個比較小的 比如說你的Lama 70B或Lama 8B的模型 就有點像是這個405B的模型當老師 然後你的8B模型當學生 這405B的模型教這個8B的模型 這樣子的一個概念 我們通常比較技術的詞會把這個動作稱作Distillation 中文可能可以翻成真六 這件事情臉書也是第一次開放 他們同時也有說 如果你真的使用Lama 405B去訓練你自己的其他的模型 就算不是Lama好了 其他的開源模型 比如說你的Base可能是Mistral的模型好了 在你的模型名字前面 你就要加上Lama 所以說我們接下來 在Hugging Face上面真的會被Lama模型給洗版 最後就是你可以在哪裡使用這些模型呢 首先就是Meta他們自己有一個網站叫做Meta.ai 你可以直接在這個網站上面使用這些模型 包括405B
(24:26~25:28) 不過這邊就是你應該還是要VPN到美國才可以使用 除此之外 Meta也有跟很多的這些雲端服務提供商 這些Inference Engine Company去簽訂很多的合約 在他們的伺服器上面部署Lama 3.1的模型 當然包括405B的模型 當然你如果要自己去部署的話 也可以你在Hugging Face上面可以很輕易的下載到這些模型的權重 它會叫你這個要agree一些這些條款什麼有的沒的 你全部都打勾之後 過了五分鐘它就會讓你下載了 最小的8B這個模型 一般的電競筆電就可以跑得動了 或是Mac也跑得動 那70B的模型可能就會比較困難一點 那405B的模型一般人就是想也不用想了 但是有些公司可能真的是想要自己host這些Inference的話 確實在一個Server Rack裡面也都可以放得下 只要你有足夠的VRAM
(25:28~26:28) 好的接下來我們來講講這篇92頁的論文 我當時禮拜六下午的時候我在看這篇論文 我真的是越看越不相信自己的眼睛 因為他們真的是所有能講的東西都講了 而且都講到非常細節的部分 我覺得如果你想要很完整的了解一間公司 要怎麼樣train出一個像LAMA這樣子的模型的話 你看這篇論文就夠了 這篇論文把模型的架構 模型的整個訓練的配方跟訓練的過程 以及整個訓練的設施 然後甚至訓練完之後的一些安全措施 全部都跟教科書一樣寫給你看了 而且你也要知道這是非常好的一個教材 因為他們訓練出來的這些LAMA3.1模型 已經是快要是全世界最強的模型了 跟全世界最強的模型只有一步之差了 所以這真的是可以說是業界的best practice 直接秀給你看 那整篇論文講最多的地方就三個 模型訓練的配方、模型訓練的硬體設施 以及一些安全措施、一些safety的東西
(26:28~27:28) 那模型訓練的配方就包括 它整個訓練的過程是怎麼樣 以及訓練過程中的這些資料是怎麼filter 怎麼樣拿到的 那我覺得我們就先從模型訓練的配方這邊開始聊好了 那首先先給大家一個基本概念 我相信蠻多人應該都已經知道這些了 但是為了那些不知道的人 我們簡單講一下就是 你訓練一個大型元模型的過程 有主要兩個步驟 pre-training跟post-training pre-training在做的事情就是 直接丟一大堆的資料給這個模型 讓他一直看一直看一直看 看到他對於世界有一定程度的基本認知 然後各個領域的基本知識都有這樣 那接下來呢就會進入post-training的部分 那這個部分呢就是 拿比較小的資料集去tune這個模型 讓他對於某一些特定領域的知識 或者是某一些特定能力變得更強 比如說更強的coding能力啊 更強的使用工具的能力啊 然後同時也修正他的價值觀
(27:28~28:28) 讓他更接近人類 就是讓他知道什麼話可以講啊 什麼話不能講啊 然後在回答某一些問題的時候 要用什麼樣的形式會比較好啊之類的 那首先在pre-training這邊呢 LAMA3的模型有一個最有名的地方 就是他們用極大量的資料去進行pre-training 他們在LAMA2的時候呢 他們是使用1.8 trillion的資料 但他們在LAMA3是使用15 trillion的資料 所以真的是量是足足多了8倍啊 但同時呢他們也沒有忽略掉品質的部分 他們這次呢有進行更多 更好更嚴密的一些資料的過濾資料的清洗 所以說呢這個去年資料在值跟量的同時提升 就是這次LAMA為什麼可以表現這麼好的主要原因之一 另外一個大原因呢 當然就是post-training那邊的一些動作 但是這個是非常主要的原因之一 那大家有興趣的話呢 可以再去論文裡面看他們是怎麼樣去清洗
(28:28~29:31) 怎麼樣去篩選這些資料的 就他們有做很多刪除重複的資料啊 刪除一些不常見的文本啊 就比如說可能他的distribution 他的token distribution跟一般正常的token distribution 真的差太多了那種 然後他們也有刪除掉一些像是限制級的內容啊之類的 他們都寫得非常詳細 連他們怎麼做 他們用KL divergence去比 他們用cosine similarity去比 全部都寫出來了 真的是非常的佛系 那在pre-training這個階段呢 當然除了這個資料的處理資料的清洗以外呢 一個非常重要的事情 就是你要決定你要訓練的這個模型有多少的參數量 以及你要用多少的訓練資料去訓練它 這兩件事情你一定要事先決定好 而且是一個非常重大的一個決定 因為你在訓練每一個模型的時候 你都有一定的這個運算資源的預算嘛 你的compute budget 那你模型的參數量以及訓練資料量的大小
(29:31~30:31) 就是會影響你的compute budget的最主要兩個因素嘛 那你一個training run跑下去 一次跑就是好幾個月 然後就是可能好幾million的花費 所以這真的是非常重大的一個決定 那一般公司在決定這件事情的時候呢 他們通常都會參考一些所謂的scaling law 所謂的scaling law呢就是一些公司 他們觀察到模型的參數以及訓練資料量 怎麼樣的調整會造成怎麼樣的訓練結果 這是大家觀察到的一些現象這樣 那當然law這個字你直翻它可能是一個定律嘛對不對 就是可能物理上面的一些定律啊 牛頓的三大運動定律啊對不對 它聽起來是一個很確定的東西 其實不是它是一個我們觀察到的現象 那我們可以透過這個觀察到現象大概猜說接下來會怎麼走 但是一切都不一定 有點像是摩爾定律啦 摩爾定律就是這種概念 確實好像美國一年半美國一年半
(30:31~31:32) 這個電晶體的數量都會double double 但我們現在就看到它slow down啦對不對 所以說這個scaling law各家的講法也不太一樣 那這邊給大家一個簡單的scaling law的歷史的recap 反正最初代的這種neural scaling law 大家是只focus在模型的權重 就是大家就發現說我們今天把模型做得越來越大 它就會變得越來越聰明越來越強耶 所以我們就要持續把這個模型做大而已 那確實呢這件事情是真的 我們從GBT2到GBT3就是根據這個scaling law 我們就把參數塞爆多 這個GBT3有175 billion的參數耶 就算在現在的這個大型元模型的世界當中呢 也算是蠻大的嘛 但是後來呢Google有一批人呢他們發現說 欸不行喔不是模型的參數變多而已 這個訓練資料量也要等比例的上升 你才可以最妥善的運用你的運算資源 他們是把這個 然後他們就根據這件事情推出了一個新的scaling law
(31:32~32:35) 把它叫做Chinchilla的scaling law 那同時呢這件事情也就解釋了 為什麼GBT3175 billion的參數但這麼笨 因為它的訓練資料量太少了嘛 假設今天它把這個模型的 就是用同樣的compute budget 但它把模型的參數變小一點 然後訓練資料量變大一點 你用同樣的compute budget喔 你可以train出一個更強的模型 但是臉書在這次的論文之中呢 欸他們又發明了一個新的scaling law 就是他們沒有把它取名還怎樣 他們就是自己內部測試出來的一個scaling law 因為他們要的這個scaling law 因為他們要決定這個最大的這個模型呢 究竟要多少參數 然後要用多少訓練資料嘛 那他們覺得Chinchilla scaling law 雖然說不錯 但是它還忽略了一些重要的點 就是包括他們在判斷模型好壞的時候 是使用所謂的training laws 去比較這些模型的好壞 那這個training laws呢 就是這個模型它的output 跟它的訓練資料的這個distribution的相似程度
(32:35~33:35) 就是它能多高程度的模仿它的訓練資料這樣子 那很能模仿你的訓練資料 並不代表這個模型在處理日常生活中的一些任務的時候 會處理得更好 有更強的能力 對吧 然後同時呢 Meta也覺得Chinchilla的這篇論文 他們拿來實驗的這些模型真的都太小了 他們的這個實驗的運算資源太少了 所以他們的這個訓練資料 所以說他們確實做了 就是用更多的運算資源 然後同時呢 也不是用laws 而是用他們所謂downstream task的這個capability 來去判斷這些模型的好壞 那最後他們根據他們的這個scaling law呢 他們發現了一些事情 首先第一件事情呢是 他們在這個LAMA3推出的時候 就有跟大家說的就是 他們在這個LAMA3推出的時候 他們在這個LAMA3推出的時候 他們有發現說 就算你的訓練資料已經多到 超過Chinchilla suggests的量了 Chinchilla的optimal point
(33:35~34:39) 你就持續增加越來越多的訓練資料上去 然後模型權重大小都沒有改變 你如果繼續增加這個訓練資料量 這個模型還是會持續的變好 它變好的這個速率可能會開始變慢 但是它是無止盡的會繼續變好的 那這也是為什麼Meta最後決定 要用15 trillion的訓練資料 就是非常非常對於這些小模型來說 是非常非常over trained的資料量 然後同時Meta也發現另外一件事情就是說 當你的compute budget大到一定程度的時候 你這個模型權重跟資料量之間的trade off 會越來越小 意思就是說 你今天的運算資源多到 你再考慮要train一個可能405B的模型 這個時候你可能要去做一個 這個時候你可能少一點點參數 把這些參數換成訓練資料 或者是多一點點 或者是少一點點訓練資料 把這些訓練資料換成模型參數 其實都沒什麼太大的差別 所以說這也是為什麼他們最後選擇是使用
(34:39~35:40) 15 trillion的token來訓練一個405B參數的模型 其實照理來說可能它的token數 要再更多一點會比較好嗎 畢竟他們連train8B的訓練資料 畢竟他們連train8B的模型 都是使用15 trillion tokens 他們可能再多一點會更好 但他們發現說其實這個trade off沒有那麼明顯 那我們就把這個模型做大一點沒關係 我覺得這邊有些人會問說 為什麼要這麼大 為什麼要大到大家都跑不動呢 其實他們這邊的想法就是 首先第一個他們確實 真的是想要一拼frontier model的排名 他們真的想要打敗OpenAI跟CloD 他們最強的這些模型 然後成為排行榜中的第一名 他們真的想要一拼這樣子 所以說他們基本上就是在目前訓練資料量 以及運算資源的限制之下 能做多大就做多大 就做405B這樣 反正到了這個階段資料量跟模型的參數量的trade off 也沒有那麼明顯了嘛對不對
(35:40~36:41) 再來另外一點就是 其實405B的模型 它勉強還是可以塞得進 一個8個H100的compute node裡面 所以雖然說對於一般人來說是不太可能使用 但對於資料中心來說 勉強還是沒有那麼難用啦 就不至於你今天host一個模型 你也要做很多的parallelization 就是要把這個server rack裡面放一半模型 另外一半還要放到另外一個rack裡面 至少不用做到這件事情 然後最後一個考量就是 當然他們這個模型做出來 他們也不是想要成為大家的everyday model 大家平常每天使用的模型 當然就是8B、70B 那這個模型呢 它的任務就是為這些比較小的模型 產生出高品質的訓練資料 去distill他們、去教他們 從竹克伯他最近的一些interview 像是他上Emily Zhang的Bloomberg的那個叫什麼 The Circuit的那個節目 他就是有講到這一點 順便大推一下
(36:41~37:42) 我覺得The Circuit是一個優質節目 真的超棒的 他是一個彭博的資深科技記者 Emily Zhang 他在YouTube上開了一系列 跟CEO訪談的節目 然後他都做得超漂亮的 真的這個好想要有這種剪輯的技巧 不對 應該是說好想要有能夠 可以跟竹克伯一起去surfing的機會 真的是不行啊 不能再講了 再講下去 我真的要被這個曹南索克給掰彎了 他真的是太帥了 pre-training這邊基本上就是這樣 就是你決定了你的模型參數量 你的訓練資料量 然後你要怎麼樣清洗這些資料 就可以開始訓練 基本上就是這樣子 還有就是你當然也要規劃一下 就是你一開始訓練的context length 跟之後的context length 因為這些context的能力 是慢慢提升的 你一開始訓練這個模型是 可能8k的context length
(37:42~38:44) 你之後再慢慢訓練到128k 這個就是拉馬3.1現在的context length 就是可能要規劃一下 你每個階段要訓練多少這樣 但反正pre-training大概就是這樣 模型就塞一堆資料進去給他讀 接下來post-training的部分 其實主要也分成兩個步驟 這兩個步驟會不斷的重複這樣 這兩個步驟就包括 supervised fine-tuning 跟direct preference optimization 這些技巧在被Chad GPT帶起來之後 基本上大家都在使用 很多人有討論 你有興趣的話可以自己去看論文 或者是自己去查一下 問Chad GPT也好 就知道這個過程是怎樣 當然DPO這邊有些人會做PPO 反正大概就是這樣子 今天我就不講技術細節的部分了 如果要很簡單很高層次的跟你講 他是在做什麼的話 就大概是兩種事情 一種就是你先準備好一個
(38:44~39:45) 你想要的問題跟回答 直接拿這個問題跟回答去訓練他 跟他說你看到這個問題 就是要這樣回答 另外一種方法就是 你叫他根據一個問題 你叫他做出一個回答 叫他做出很多種不同的回答 你去裡面選一個你覺得最讚的回答 然後透過不斷的重複這些過程 慢慢讓他知道你的喜好 或是在某一些領域變得更強 我覺得這邊論文之中 有一個蠻有趣的可以討論的點就是 他們在整個過程中運用到 很大量的生成資料 Synthetic data 或者我覺得中文應該翻成合成資料比較好 反正這些資料就是AI產生的資料 也就是說這個LAMA 3.1的模型 他們都是由AI在調教的 人類只是在很小一部分 給很小一部分的這些答案 或者是幫忙選一下哪一個 你們比較喜歡 就非常少部分的參與
(39:45~40:46) 但大部分的這些資料是來自AI自己產生的 或者就算不是AI產生的 也是AI標註的 AI選的 所以目前看來Synthetic data的發展 感覺是蠻順利的 我覺得 因為一開始很多人會在那邊說什麼 這個網路上的資料就這麼多而已 我們都已經快用完了 之後會不會有資料荒的情況 不會啊 我們可以有這種AI自己生成的資料去訓練 同時也有人說 不行喔 AI生成的資料訓練模型 模型會越訓練越笨之類的 或者是會走上歧途之類的 變得很奇怪 目前看來這個LAMA 3.1 感覺還是沒有這些問題出現 然後在一些那種 很好判斷一個回答對錯的領域 Synthetic data尤其好用 像是在Coding這個領域 像是LAMA 3.1這次在Post Training這邊 就用到很多的Coding的Synthetic data 因為Coding的合成資料真的非常好合成 因為當模型產生出一個回答之後
(40:46~41:47) 你可以非常快的 不需要人為介入的 去驗證這個回答是否是好的 你就直接執行它 就直接讓一個Python interpreter執行它 看它會不會報錯 如果報錯了 那這個就是不好的 這次LAMA真的就是這樣做 他們就是讓這個模型產生出 一些Code 然後執行一下 如果報錯的話 就一樣把這個報錯的資訊 丟回去給這個模型 叫它說你重新再做一個 然後它就再產生出一個Code 然後再跑 再報錯的話 就再丟進去再產生出一個 直到它丟出一個Code 是可以完全正常執行的 那這個時候它就成功產生出了 一筆OK的訓練資料 那它就反覆的這樣子做 一直做做做 產生出幾百萬筆 然後就可以拿這些優質的訓練資料 來訓練模型 那我一開始也有說就是 這禮拜還有一個很大的新聞就是 Google它有兩個模型 差點拿到了這一次數學奧林匹亞的金牌 它不是拿到了銀牌
(41:47~42:47) 然後好像差一分還是兩分 就變成金牌了 這兩個模型是 Alpha Proof跟Alpha Geometry 2 那這兩個模型它背後的運作原理 其實也是差不多的概念 他們之所以可以解數學題解得這麼強呢 就是因為數學是一個 很好去判斷對錯的一件事情 像這個Alpha Proof 它就可以把所有的這些數學問題 轉換成一個叫做Lin的語言 那在這個Lin的語言之中呢 它就可以很好的去證明 你現在是否推導成功 那能做到這件事情之後呢 你就可以不斷的產生新的練習題 然後不斷的練習 然後就自己變得越來越強這樣 這個就跟2016年的Alpha Go、Alpha Zero 是一模一樣的概念 就是你自己跟自己下棋 越下越強越下越強 為什麼? 因為一盤棋是很好判斷輸贏的 就像是一個數學題 很好判斷它有沒有被解出來 就跟一篇code 是很好判斷它能不能被執行的 所以其實很多人會覺得
(42:47~43:48) 下棋這件事情這麼難 AI竟然能夠可以Master AI真的是太厲害了 或者數學奧林匹亞這麼難的東西 AI竟然可以解得這麼好 真的好厲害 但其實AI能做得好 單純就是因為 它們的結果是非常好判斷對錯的 這個AI看似是很聰明 好像邏輯推理能力很強對不對 但它們沒有辦法把這些推理能力 應用在其他的問題上面 因為其他問題沒有辦法判斷對錯 所以這個就是為什麼你會看到 有一些AI已經可以解數學奧林匹亞的題目了 但是你問它 9.9跟9.11誰比較大 它會答錯 或者是你問它 Will Smith的兒子是Jayden Smith 那Jayden Smith的爸爸是誰 他也會跟你講一個完全不知道的人 其實這個模型訓練配方這邊 還有很多可以聊的 它們真的是 真的在訓練這個模型 就像是哈利波特的Potion Class一樣 就是一個Coldron裡面 放一湯匙的什麼東西 半顆什麼東西
(43:48~44:52) 然後攪了幾下 然後再放什麼東西之類的 你去看一下 它們真的是 真的是一個 用非常複雜的配方去訓練這樣 這邊很有趣 但我覺得今天沒有辦法Cover全部 因為我們要趕快來講下一個部分 就是訓練的硬體設施的部分 因為我覺得這邊也真的是很有趣 然後我覺得很多的台灣的聽眾 可能也會比較在意這一部分 也會比較在意這一部分 畢竟大家現在要投資的話 應該也都是投資AI的硬體 我們前幾集有聊過了 現在錢都只能流往硬體這邊 因為應用還沒出來 所以究竟NVIDIA 跟這些其他的 Intel、AMD的未來如何呢 我們來看一下Meta怎麼訓練他們的模型 首先Meta這次真的是誠意滿滿 它們LAMA1跟LAMA2的模型 是用Meta的AI Research Supercluster在訓練的 他們專門為訓練AI而做的
(44:52~45:54) 一個資料中心裡面的GPU集群 但是LAMA3他們是把它移到 Meta的Production Cluster上面去訓練 當然我不確定 沒有辦法確切知道Production Cluster是什麼 但聽名字 我覺得應該是在跟可能 Riels的演算法 IG的演算法去搶資源了 他們會這樣做是因為Production Cluster 可能有比較大的算力 同時也比較可靠 因為他們是為了Production做準備的 一定是最可靠的 我們最大的405B的模型 它是在16000個H100上面訓練出來的 那一個H100現在的價格可能3萬塊美金左右吧 所以大家可以大概算一下 他們這個硬體花了多少錢 那這篇論文在硬體設施這邊的介紹 也是非常的詳細 其實我覺得我從來沒有看過 任何一篇對於LLM的Training Cluster 做這麼詳細的介紹的文章 幾乎是沒有
(45:54~46:54) 你可能從一些可能像是Embue 這個Lambda, Coreweaves, Cerebra, Scrock 這些公司他們的一些小Kino 你可能可以聽到一些這些東西的介紹 但是我們從來沒有一間科技巨頭 出來這麼透明公開的 跟大家說他的整個Training Cluster的Stack是怎麼樣 那這次Meta就做到了這件事情 全部攤在論文裡面給大家看了 我覺得你若是對硬體很有興趣的 可以建議你去看一下 那我這邊就大概講一些我自己的comment 或是一些我看到的酷的東西 一些重點 那首先當然就是他們拿來訓練的GPU 就是H100的GPU NVIDIA的H100這很明顯 那每八張H100會配兩張的CPU 但他們CPU是用哪一家的 他們就沒有特別說 我剛才說他們超級無敵透明詳細 結果竟然沒有講CPU是什麼 OK,沒關係 這不是重點啦好不好 這個GPU才是主要的Workhorse嘛
(46:54~47:56) 那這些NVIDIA H100的GPU呢 全部都是用NVLink的技術連在一起的 就是在一個Rack裡面他們是用NVLink連在一起 但是Rack to Rack這個比較遠的一些connection呢 他們是用他們自己發明的一個Networking的技術 叫做RDMA Overconverged Ethernet 那這些東西是什麼呢 我在EP43也有大概跟大家介紹到嘛 但反正你要進行這麼大規模的訓練呢 你必須要把很多很多的GPU串連在一起 就像是他把這個我剛講 把16000個H100的GPU串在一起嘛 那當然不可能16000個H100的GPU 全部都塞在一個Rack裡面 你一定就是要很多很多 一個Rack就是一個伺服器的架子嘛 你一定要很多很多這種伺服器的架子 然後想辦法把他們連在一起 用非常高的資料傳輸的速度連在一起這樣 那這個Rack跟Rack之間的連線呢 你如果是使用NVIDIA的系統 就是使用他們的DGX
(47:56~48:56) 就是你去聽NVIDIA的Keynote 他們不是都會有一段動畫是 從一塊小小的晶片慢慢組成越來越大塊 然後把它插到一個Rack裡面 然後再把很多Rack排出來 然後最後再排成一整個Data Center 他們在排到這些Data Center的時候 這些Rack之間的連線 都是使用一種叫做Infinite Band的技術 嚴格來說也不是都是啊 他們有Ethernet的選項 但他們很厲害就是有一種 Infinite Band的技術 那這個Infinite Band的技術 就可以把兩個Rack之間的連線做得超級快 Ethernet就是我們平時用的這種以太網路呢 也可以拿來連接這些Rack 但是就會慢一點這樣 那這個臉書呢他們就是使用Ethernet 因為他們是使用自己設計的這些伺服器架 他們不是使用NVIDIA的系統 那是太貴了 他們是自己設計的 但是他們已經想辦法把以太網路推到最快了 因為他用的這個技術叫做 RDMA Over Converge Ethernet 那這個RDMA就是
(48:56~49:56) Infinite Band為什麼會這麼快的原因 因為RDMA在說的就是 Remote Direct Memory Access 就是兩個GPU之間的記憶體 可以直接溝通資料 不用透過CPU 這就叫做RDMA 那Infinite Band是有原生的支援RDMA的 他原本就是為了這個HPC 做出來的這種通訊協定 那以太網路呢就沒有原生的支援 因為他原本就是為了這種 廣泛的大家使用 做出來的這種Networking協定 所以Meta用的這個RDMA Over Converge Ethernet 就是在以太網路上面實現RDMA的一種技術 然後用這種技術做出這些Fabric 那他們的通訊速度呢是 1秒400Gbps 那當然使用Infinite Band應該是可以到更快啦 但他們就是他們覺得這樣就夠用了嘛 那確實也是夠快了嘛 但你看了我你就會發現 有一個數字是蠻低的喔
(49:56~50:57) 這個數字叫做MFU 叫做Model Flop Utilization 這個你就可以把它想像成 GPU算力的使用率 使用率只有40% 在整個平均下來喔 整個Training的過程 一個GPU的算力只有被用到40% 那你問說其他60%為什麼不用 因為他在等資料來啊 他在等資料流到對的地方 他才可以漏進去才可以開始算嘛 所以說從這個數字 我們就可以很明顯的看出 為什麼NVIDIA吃下這麼大的AI Training的試戰 因為你如果今天把這個GPU 換成是一個AMD的GPU好了 AMD的GPU不是都吹說 他們的MI300X不是吹說 比H100的這個算力還多嗎 但假設你今天換成MI300X來 這個GPU這個MFU的數字可能會掉到10%20%之類的 因為MI300X之間沒有MV Link的連線啊 所以他的這個 他之間的頻寬一定會變得更低
(50:57~51:59) 然後你又要花更久的時間去等這些資料來 你能用到的算力就更少 所以你的算力在那邊多個10%20%30% 其實真的沒有那麼大的用處啦 然後同時呢這篇論文也寫出一些 那種一般人跟小公司在玩AI 永遠都不會遇到的問題 像是他們要進行大量的平行化 他們不是 我之前有介紹過平行化嘛 就可能比較大類分成兩種 Model的Parallelization跟Data的Parallelization 但他們有做四種平行化 他們有用Tensor Parallelism Data Parallelism Context Parallelism 還有Pipeline Parallelism 有一大堆這種平行化 然後四個是同時使用的 然後同時你又要最優化每一個GPU的使用率 你不能說因為平行化 上一個部分還沒算完就不能算下一個部分 你可以先接其他人的工作啊 所以這麼多的平行化同時發生 然後你要做最優化
(51:59~53:01) 真的是超困難超複雜的一件事情 我光想一兩種平行化 我就想不下去了 這個是你要 你要有很高維度的思考能力 你才可以很直覺的去解決這種問題 不然真的非常困難 然後同時Mela也有提到訓練的過程中 一些可能供電啊錯誤率的一些問題啊 就比如說其實這個訓練的過程啊 對於一個電網來說 它是非常Bursty的 意思就是說它的用電量會時不時跳上去 然後又掉下來然後又跳上去 然後跳上去的時候就是 大家都要做Checkpoint的時候 就是你訓練模型訓練到一個階段 你要做一個Checkpoint 就是現在就目前的模型權重去做一個備份的概念 那一種很Bursty的這種用電啊訓練的狀況呢 就會出很多的問題嘛 那種問題都是一般人跟小公司是完全不可能會遇到的 那我們在這篇論文中呢都可以看到非常有趣
(53:01~54:03) 然後最後論文呢還有花一部分的篇幅 在講一些Safety的東西啦 什麼Red Timing啊這些 那你聽到我現在的這個語氣的這個變化你就知道 我他媽覺得這個部分真的超無聊的 我每一次都不想講這個部分都想直接跳過 因為我真的覺得好啦現在的LLM真的沒有什麼威脅啦好不好 我們就正視這件事情可以嗎 就你現在能靠LLM做到的壞事 你靠Google Search都做得到嘛 那當然我不是說這個Safety不是一個issue 它絕對是一個你要放進論文 你絕對是要花時間去研究要有一些成果的東西 但我就是本人就沒什麼興趣啦 就我真的不怕哪一個人問出來GBT某個炸藥的配方是什麼 因為他媽的你去查Google你應該也查得到同樣的事情 然後我也我真的不太care你今天這個模型你講出一個幹之類的 我覺得沒差啊對我來說真的沒差 反正你就寫好我的課 你就Summarize好我的文章就好了
(54:03~55:05) OKObviously這個不是很好啦 小孩子也會使用嘛就不要說幹啦 但我覺得大家應該懂我的點吧 就這些事情我自己個人真的覺得沒有什麼太大興趣 所以這邊我打算全部跳過 那等到哪天這個模型真的有自我變強 或者是一些更厲害的更進階的reasoning的能力的時候 甚至一些自我意識的徵兆 那到時候我就會非常認真的做一整集科技上Podcast 來跟大家解讀Safe的問題 但在那之前呢就先跳過 好那我們錄到這邊其實時間也差不多了啦 其實論文的這邊甚至有蠻多我是想講的但是還沒講到 但時間就已經快要超時了 可見這個論文真的是資訊非常的豐富 那一開始有跟大家說就是我們最後要來聊一下 為什麼主客不會要開源這些模型嘛 為什麼花這麼多錢train的這些模型要免費送給大家 然後開源社群的未來究竟如何呢
(55:05~56:06) 那這個問題呢其實說真的要討論可以討論的非常久 可以討論的很深 但是因為時間的問題呢我這邊就快速帶過我目前的想法 然後你如果想去聽討論的話你可以去聽一些其他的podcast 比如說好像是BG2 或者是嗯... 最近還有誰聊 我不知道我最近有聽過的就是BG2這樣 那如果未來有機會我當然也會再重新深入的討論一下 但目前我先給大家很快的我的想法 那主客伯在他的信裡面其實也有討論到這個話題 就是為什麼Meta要開源這些AI模型 那他寫了很多點啦但是我這邊呢總結出我自己的三點 首先第一點呢就是Meta的business model不是靠AI模型賺錢的 AI模型對Meta來說不是一個賺錢的工具 那對於Google對於OpenAI對於Anthropic來說 他們的AI就是他們賺錢的工具嘛對不對 他們有這個subscription的服務嘛 Chai GPT是一個月20塊然後Google是 我可能也是差不多一個月20塊然後跟Google One包在一起嘛
(56:06~57:06) 還有一些給你一些storage這樣子 反正他們都是靠這些AI模型在賺錢 但是臉書並沒有打算要直接透過AI模型來賺錢 他們就是乖乖賺他們的廣告錢就行了 那講到這裡有些人可能就會問說 那臉書幹嘛要開發AI模型 既然沒辦法靠它賺錢那幹嘛要開發呢 那這邊就來到我們的第二點就是 臉書未來的產品一定用得到AI模型 而且能夠透過AI模型賺到更多的錢 也就是說AI模型變得更好Meta的business就會變得更好 你如果常常聽祖克柏的訪談你就會知道 他對於AI怎麼融入Meta的未來Meta未來的產品呢 他的看法都還蠻一致的 他都會給出幾個大方向 第一個就是未來的創作者都可以透過自己的AI分身 跟他的粉絲互動來scale他engage粉絲的能力 第二個就是許多的企業會想要有一個自己的AI模型 來加快他們做事情的效率
(57:06~58:06) 或者是跟處理克服問題的效率 然後第三點可能就是AI跟ARM Metaverse的一些結合 那針對這三點呢 首先第一點Meta已經做一些非常初步的嘗試了 就是他們有抓一批美國很有名的名人 包括MrBeast、Kendall Jenner 然後為他們做一個AI分身 然後這個AI分身長得跟他們一模一樣 但是取了一個不一樣的名字 然後每一個人都有自己的一個專長 比如說這個MrBeast的AI分身叫做Zack 他就是一個大哥哥的概念 你可以跟他聊一些你會想跟一個大哥哥聊的話題 Kendall Jenner可能就是你的Girl Bestie之類的 反正他們有嘗試去做一些名人的AI分身 但是目前好像大家的反應不怎麼樣 所以說第一點就是我覺得還要再看下去 第二點就是公司建立自己的AI模型
(58:06~59:06) 我覺得這個方向絕對是正確的 尤其是在客服這邊絕對是靠AI在處理的 那這邊當然目前的發展還是非常緩慢 然後Meta有提供一些可能Meta AI Studio之類的初步的嘗試 但目前還沒有什麼太多的舉動 最後一個就是AI在Metaverse這邊的結合 那這邊最明顯的第一部產品就是Meta的Ray-Ban glasses 那我可以說這個是一個蠻成功的例子 就是在眼鏡裡面結合一個AI助手 讓你可以問任何你看到的東西 或者是隨時問一些很快的問題 我覺得是非常棒的一個use case 所以很明顯的Meta現在已經非常積極的 在把AI融入他們目前的產品線當中了 也就是說呢這個AI的技術越來越強 這些模型越來越聰明 這些產品就會越來越好 然後Meta最終就會賺越來越多錢 那我們都知道通常一個技術開源了之後 它會進步的比較快
(59:06~60:06) 因為全天下的人都可以一起幫你找出一些錯誤 幫助你想出進步的方法 所以說開源這個Meta的AI讓它進步的更快 Meta最後也會變得更好 那最後一點開源的原因 我覺得也是非常重要的一個原因 就是Meta不想要再依賴他人的技術了 它不想要在未來使用這些AI模型的時候 是被一個幣源的公司給綁住的 這個我覺得就蠻明顯的 第一個考量可能就是成本 別人可能定價會比較貴一點 但第二個我覺得比較大的一個考量 就是你就是真的是受制於人啊 那一個最明顯的例子呢就是蘋果手機嘛 蘋果手機有一個所謂惡名昭彰的Apple tax 所有iOS App裡面的消費都要被他們抽30% 然後很多這個App他們想要開發的一些新功能 蘋果都不讓他們launch 真的是完全是受制於蘋果 那祖克柏被蘋果踩的最重的那一腳 其實不是Apple tax
(60:06~61:07) 是Apple tracking transparency policy 那時候不是蘋果做了這個App tracking transparency的更新之後呢 使用者在開了一個新的App之後 都會跳出一個通知問他說 你要不要讓這個App去track你的activity這樣子 你在上面進行的一些活動啊之類的 那這件事情就非常大幅的影響到了臉書的廣告收益嘛 所以祖克柏有這種被幣源社群統一到的經驗 然後同時呢他在開源社群這邊也有一些成功的經驗 像是他們推出的這個Open Compute Project 自己把他們的Data Center的一些Rack的設計啊去開源 讓這個supplier全部都可以遵循這個統一的標準 那最後的結果呢就是Open Compute Project變成了業界標準 然後臉書靠這件事情省了好幾個billion 所以總結下來呢為什麼祖克柏要開源這些AI模型呢
(61:07~61:58) 第一個他們開源沒有關係因為他們不是靠AI賺錢 第二個AI變得更好他也會變得更好 那開源AI呢就是讓AI變得更好的一個方式 然後第三個他們不想要再依賴幣源社群了 應該說他們不想要受制於人 那以上呢就是我對於臉書開源的一個快速想法 那我們這集呢就差不多聊到這邊啦 那最後也再次感謝一下本集的贊助商 這個知識衛星的一堂好課Lydia的UX的課程 那在節目一開始呢大家也聽到就是UX真的是非常重要的一個事情 不管你本業就是在做UX設計的 還是你是工程師還是你是PM 你都可以因為一個好的UX思維而提升你的職場價值 你想看看這堂課的話在本集的資訊欄就可以找到連結了 那最後決定要購買的話呢也別忘了使用我的優惠碼喔 那最後呢就祝大家有個愉快的一週
