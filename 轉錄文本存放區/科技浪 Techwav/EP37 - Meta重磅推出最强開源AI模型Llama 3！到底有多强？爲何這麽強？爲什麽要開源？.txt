(00:00~01:01) 【音樂】 哈囉大家好,歡迎收聽科技浪,我是主持人哈利 科技浪是一個白話跟你聊科技的Podcast 希望可以用簡單易懂,但是又深入的方式 帶你了解時下最火的科技話題 本集節目由Speak贊助播出 Speak是一個語言學習的APP 目前可以學英文跟西文 那這個Speak這間公司呢,其實挺特別的 它背後的金主是OpenAI OpenAI有個投資新創的基金叫做 OpenAI Startup Fund 那這個基金呢,SameOwn本人也有說 這個基金它只會投資那些 真的在解決非常重要的問題的新創公司 然後同時呢,他們不會廣撒 他們只會挑幾間最有前途的公司來投資 那我們今天的贊助商Speak就是其中一間 所以說它真的就是一個OpenAI認證的公司喔 那這個Speak顧名思義啊 它就是專門幫你練習口語的一個APP
(01:01~02:02) 它下載是免費的 然後下載下來之後呢,它會先問你一些問題 然後根據你的回答 幫你客製化一些口語的課程這樣 就是他們會一句一句的教你怎麼念 然後讓你不斷的練習 然後除了這些口語課程以外 他們還有一個很大的賣點就是他們的AI家教 這些AI家教呢,就是可以跟你聊任何你想聊的話題 你可以透過跟這些AI家教聊天的過程中呢 來主動的練習你的口語能力 不過這邊呢就是需要付費解鎖的部分了 大家也知道這些LM的運算資源不便宜啊 然後除此之外呢Speak也是高度遊戲化的 它會有一些什麼挑戰啊之類的 有些成就要解鎖 所以說可以持續的激勵你一直來學習 那我覺得我們亞洲學生呢 在英文的這方面 口語能力一直以來都是聽說讀寫裡面最弱的那個 原因很簡單 就是因為口語的訓練資源是相對難取得的 你要訓練這個閱讀能力的話 你就去買一大堆英文小說狂看就可以了 訓練聽力的話 你就狂聽一堆英文Podcast就可以了
(02:02~03:03) 訓練這個寫作能力的話 你就狂寫一堆文章就行了 但是你要訓練口語的話 你要找人練習 那這件事情就是非常困難的 你可能要去找一個英文的口語家教 然後約時間然後見面 然後過程中他還收超級貴的終點費 那Speak出來之後真的就是改變了這一切 你隨時隨地想要練習口語都可以練習 然後這個費用呢 也是跟真人比起來是非常非常低的 你如果想要練習你的英文或是西文口語的話 可以在本集的資訊欄找到Speak這個城市的連結 你可以先免費下載來玩玩看 再決定要不要付費購買他們的AI家教 本集業配就到這邊結束 謝謝Speak的贊助 好那我不知道大家最近有沒有看到就是 社群媒體上有在瘋傳一張照片 就是祖克柏留鬍子的照片 祖克柏當然就是臉書的創辦人跟CEO Mark Zuckerberg 那有一張他留滿了一個烙腮鬍的照片 在社群媒體上瘋傳
(03:03~04:05) 那不得不說他留鬍子是真的蠻帥的 我覺得他原本就是一個仆男 但是他留了鬍子之後我覺得算是一點點帥哥這樣 那這張照片直接在社群媒體上瘋傳 大家都說 因為他在這張照片中他不只是留這個烙腮鬍 他的穿搭也是有進步 就是他穿的衣服的這個剪裁 然後還有他會有戴一個這個銀項鏈 然後這個頭髮呢也感覺有style過 就是有點卷卷的這樣子 那就大家都覺得他進入這個潮男時代了 那我會把這張照片放在本集的資訊欄裡面 大家去看 那我會把這張照片放在本集的資訊欄裡面 大家去看 那我會把這張照片放在本集的資訊欄裡面 大家去看一下這個祖克柏現在究竟變得多潮了 那我就放在這個資訊欄最下面 滑到最下面就可以看到 但其實這張照片是假的啦 就他並沒有鬍子 那穿搭跟髮型都是他真實有的 但是鬍子是別人披上去的這樣 但我必須說
(04:05~05:06) 我覺得祖克柏最近真的有變得越來越潮 然後越來越像人類的趨勢 那大家應該都還記得在前幾年 就是你那時候看祖克柏的 不管是他的一些發表 還是他的一些interview 大家都會覺得他 都會叫他西藝人 或是叫他AI 因為他那個時候 我覺得真的是有一點點 在鏡頭前是有一點點尷尬 有點awkward的一個人 因為就很好笑 明明他是這個全世界最大的社群媒體的擁有者的CEO 結果他自己在這個社群媒體上這麼尷尬這樣 反正之前他就是在這個鏡頭前面 他的臉部表情是非常僵硬的 然後這個眼睛感覺也是有一點點無神 就真的有點像是一個AI的感覺 但最近啊 我發現他真的進步很多 就是他 因為他不是會常常在他自己的這個 instagram的帳號上面 po一些他最近公司的一些update 像這次的LAMA3出來 他有再po一則這個reels 然後那個
(05:06~06:08) 前幾天的這個 HorizonOS的開源他也有po 然後再更早以前 我不知道大家記不記得 就是他還有po一則是在嘴Applevision Pro 那則我真的超愛 我真的超喜歡這種 就是會直接出來defend自己的產品 然後嘴對方產品的這種CEO 酷爆 反正他在這些reels上面大家都可以看到就是 他的穿著有變得越來越潮的趨勢 他現在會穿一些可能 比較有比較奇特的版型 有些oversize之類的 然後他甚至還會帶一些銀項鏈這種東西 然後他講話的這個樣子 也更像人類了 講起來比較順比較自然的 然後也會有一些可能 一些微笑啊什麼之類的表情 所以說我覺得 我不知道是他的公關團隊在幫他訓練還是 他自己就真的是可能突然開竅了 但反正他有變好 所以最近很多人都開始說就是 哇祖克柏終於要開始融入人類社會了這樣 那雖然說他還是有一些這種尷尬影片被傳出來
(06:08~07:10) 最好笑的就是前一陣子的那個meme 就是他在一個UFC的場合 UFC就是美國的一個格鬥賽事的場合這樣 那他在那邊一個很多人的地方 大家走來走去 然後手就一直伸出來要跟別人握手 但沒有人要跟他握 全部人都直接無視他 然後他每次伸出來然後被無視之後 他就裝作就是手慢慢放下來 然後在看別的地方裝作沒有什麼事情都沒有發生這樣 反正超級好笑的 那個大家可以去查一下這個meme 我也把這個meme放到本集的資訊欄裡面好了 那玩笑之外啦 其實祖克柏真的是我一直以來都是非常喜歡 然後非常仰慕的一個tech CEO 那當然最明顯就是他帶Facebook或是你說Meta這20年來 真的經歷了很多事情 有很多這些可能政治的醜聞啊 被放大檢視啊 然後股價重創啊 然後公司要轉型啊這類的 有的沒的一大堆這些事情他都挺過去了 然後也帶領這個Meta走到現在這邊 我覺得是一個很好的位置
(07:10~08:11) 雖然說這個禮拜呢Meta股價還是暴跌 但除了Meta以外啊 就是大家都知道祖克柏有很多所謂的SideQuest 對不對 大家都說他有這些支線任務 就是首先第一個就是他是有在這個訓練格鬥的 他應該是練巴西柔術吧 然後他就是常常去參加這些UFC的這些賽事 然後跟裡面的選手也有些關係這樣 他甚至之前呢 是要真的是要進行這個UFC的職業的訓練 真的是要開始打職業了 但他在開始打職業之前呢 他的這個阿基里事件斷掉了 所以說他被迫休息了很長一段時間這樣 所以就沒有辦法圓夢 他原本的夢想是在40歲以前打一次這個UFC的職業 他就沒有辦法達成 但他已經很厲害了 就是你在經營一間trillion dollar company 然後你同時還在這個訓練這麼多格鬥的技巧 是非常厲害 我現在在後製
(08:11~09:11) 那我發現剛剛有些事情我講錯了 所以我這邊更正一下 那第一件事情講錯了就是 我剛剛說尷尬影片之中呢 祖克柏是想跟人家握手 但他其實是想要拿人家遞給他的衣服 所以說這其實沒有什麼太重要了 但還是更正一下 那另外一件事情講錯了就是 祖克柏他並不是斷阿基里斯件 他是斷他的十字韌帶 他的ACL 所以說這邊更正一下 然後祖克柏還有另外一個side quest就是 他很喜歡烤肉 那他喜歡烤肉到什麼程度呢 他喜歡到他有自己的一個農場在養牛 然後這個農場的最終目的呢 就是產出全世界最高品質的牛肉 他是認真在研究這件事情喔 就是他有在研究要怎麼樣去 讓牛有怎麼樣的生活作息 要給他們吃什麼 要給他們喝什麼樣的啤酒 他們才可以就是變成最高級的牛肉這樣 因為你們知道就是那種和牛阿
(09:11~10:15) 你要讓他真的有這種很美的油花分布 你必須要讓他喝啤酒 因為喝啤酒是可以促進他的胃口的 讓他吃更多東西 然後就更增加他的體脂這樣 所以我覺得他真的是超酷的一個人 那我們今天呢正好就是要來聊他的公司 臉書meta 那為什麼要聊meta呢 就跟我們上一集聊特薩原因是一樣的 就是不只是這個股價的波動很大 他們最近也是有很多很多的重磅新聞 從這個拉馬3模型的出來 對不對 這個羊駝3 不對不是羊駝 是這個洛馬 我po了這個羊駝3這個新聞之後 就是有人在IG上面密我說 哈利這個拉馬其實不是羊駝 他是大羊駝 或者是稱作洛馬 他是不一樣的一種品種這樣 所以現在幫他證明一下洛馬3 除了這個洛馬3模型的出來以外呢 卓克博也發布了他們在元宇宙metaverse這邊的一個很大的更新
(10:15~11:16) 是足以撼動整個元宇宙的產業的更新 雖然說這個產業目前看來還沒有很大 那這更新呢就是他們要把他們自己的這些metaquest的作業系統 叫做horizonOS開源 讓其他的這些頭盔製造商 也可以使用他們的作業系統 其他的這些包括像是這個Lenovo 他們要做自己的這個VR頭盔 他們現在就可以使用meta的horizonOS當作業系統 像是ASUS ROG 他們要做自己的gaming的頭盔 現在也可以使用metaquest的OS 然後在前面這兩個大新聞之後呢 meta也公布了他們最新一季的財報 然後在這個法說會結束之後呢 他們的股價是直接暴跌 市值蒸發了2000億美金這樣 很明顯的 雖然說這個卓克博呢 他講這個metaverse講AI都講得沾沾自喜 但這個投資人呢其實不太買單的 他們一直看這個meta一直丟錢進去 然後都沒有任何的這個revenue出來 他們是感覺有點慌了
(11:16~12:16) 好那所以說今天呢 我們就來好好的聊聊meta這間公司 那最一開始呢 我們當然是要從這個洛陽3 欸這是洛陽還是洛馬 洛馬3這個LAMA3的這些模型開始講起 那我覺得用一句話來概括這個LAMA3模型的釋出呢 我覺得就是大家原本對他的期待 非常的高 他是萬眾矚目的一個模型 然後他最後的結果呢 也沒有讓大家失望 這個模型呢確實非常的厲害 那我比較想跟大家聊的是這個模型呢 他究竟表現有多好 然後他為什麼可以表現得這麼好 以及這個可能我們下一步可以期待什麼 因為這個模型呢他並不是 他們並沒有釋出全部的東西 他們還有一些東西正在準備這樣 但我覺得在那之前我還是先簡單介紹一下 LAMA3的這些模型 給大家一些背景知識這樣 我怕有一些人沒有跟到 好那首先這些LAMA3的模型呢 都是大型語言模型 只能吃純文字的 INPUT是文字 OPPO也是文字
(12:16~13:16) 他們未來可能會有一些多模態的更新 但目前來看就是這樣 那LAMA3的模型呢 目前釋出的有兩種不同的大小 一種是8 billion參數的 一種是70 billion參數的 8 billion就是80億個參數啦 那之後呢 我就直接講billion跟trillion 我相信大家這個基本的換算應該都懂吧 billion就是10億 trillion就早 那他們還有一個405 billion參數的模型 是還沒有釋出的 他還在進行訓練 那其實這個8b、70b 都是我們大家蠻常見的這個模型的尺寸 但這個405b的尺寸 是我們前所未見的 超大的一個模型尺寸 至少在開源社群這邊 我們是前所未見的 所以這是一個算是一個新的嘗試 然後他們現在釋出的這個8b跟70b的模型 都有釋出兩個不同的模型版本 一個版本呢是只有pre-trained完之後的模型 另外一個版本呢 是pre-trained完之後還有post-training
(13:16~14:17) 然後被instruction tuned過的這些模型 那這兩個模型呢 白話來講啦 他們的差異就是 只有pre-trained完的這個模型呢 就是他看過了所有訓練資料之後 只會進行文字接龍的這種模型 那另外一個模型呢 就是instruction tuned過的模型呢 就是臉書還有額外教他 要怎麼樣作為一個聊天機器人 跟人類對打 然後就是要怎麼樣去避免講一些 可能很危險或是很母湯的話 那這邊其實沒什麼特別的啦 就是這個是業界標準操作 大家都會釋出一個instruction tuned的 跟一個只有pre-trained完的模型這樣 那在這個license的部分呢 基本上就是跟LAMA2差不多啦 就是有開放商用跟研究用 你如果是一間小新創 你把LAMA3 build進你的產品當中 是完全沒有問題的 除非你是一個超級無敵大的公司 就是你的 越活躍用戶呢有超過 7億或者是 你是一個這個雲端服務的提供者 就是像你是AWS 或是這個Microsoft Azure 你這個雲端服務呢
(14:17~15:17) 有部署這些LAMA3的模型 出租給其他的用戶使用 如果你是這些人的話 你就要付點錢 然後你要用你自己的資料去fine tune這個LAMA3的模型 也是可以 你可以用台灣的中文的資料 去tune出一個LAMA3台灣模型 或者是你用一個 這個 寶可夢的資料 去fine tune一個寶可夢專家的LAMA3 反正你 可以隨便玩 但重點是你如果要把這個模型公開 發佈在網路上給大家使用的話 你的模型名稱裡面呢 一定要冠上這個LAMA3的名字這樣 好那這個LAMA3的基本資訊 就簡單講到這邊 接下來要開始講一些有趣的了 首先這個LAMA3的表現 究竟有多好 在這個LAMA3出來之前 他們真的是萬眾矚目的模型 因為這個從LAMA3 LAMA2出現到現在 也隔了差不多快要一年了 然後在這一年之中呢 真的有非常多的開源模型超越了LAMA2
(15:17~16:17) 這個碧元模型就不用說了 一直以來都贏LAMA2很多 像是這個OpenAI的模型 這個Anthropic的模型 他們一直以來Google的模型 他們一直以來都是贏LAMA2很多 但是這個開源社群呢 也開始漸漸的贏過了LAMA2 像是這個Mistral他們出了很多 不管是Mistral 7B 還是Mistral 8x7B Mistral 8x22B 這些模型呢都有打贏一些 相對應的LAMA2版本 那它除了Mistral以外呢 Mistral以外呢 還有這個Cohere Command R Plus 也是打贏了一些LAMA2的模型 然後在這次LAMA3出來之前呢 大家對LAMA3都有信心說 他應該會重新贏過這些 其他的開源模型 重新成為這個開源模型的王者 因為臉書這間公司不但有比 其他這些小新創 多超級多的GPU 他們是所謂的GPU rich 小新創是GPU poor 是GPU的貧窮戶這樣
(16:17~17:17) 他們有很多GPU 很多的AI人才 就是其實在可能5到10年之前吧 那個時候在研究AI 應該說那時候還比較AI 那個時候在研究Machine Learning 或者是研究Deep Learning的這些人 要嘛就是去Google 要嘛就是去臉書 Google跟臉書基本上就是一個 寡占所有AI人才的市場 但現在就是最近這幾年 當然有很多其他的新創出來 像有一些人會跑去OpenAI 有一些人跑去Anthropic 然後去一些其他的小AI新創 或是自己創立AI新創這樣 所以說人才當然就沒有這麼集中 但臉書現在還是 掌握了很多AI研究的人才 好那這麼多資源的 一間公司究竟會做出什麼樣子的模型 大家都很期待 然後期望也都很高 然後這些這麼高的期望呢 我覺得臉書不只有達到 還有超過大家的期望 那我們就來講講這個 這個LAMA3的模型究竟有多強 那我們現在在比較這些大型元模型的強弱的時候
(17:17~18:17) 都有兩種方法 一種方法就是使用 大型元模型的Benchmark 這些Benchmark就是所謂的 標準化測驗 基本上就是讓這些大型元模型寫考卷 然後看它答對幾題這樣 另外一種評測這個 大型元模型的方法呢 是Chatbot Arena 那你如果是科技量長期的聽眾 你一定對這個Chatbot Arena的概念是不陌生的 因為我一直以來都跟大家說 我覺得我個人比較信任 這個Chatbot Arena的排名 因為在這個排名之中呢 每個大型元模型的成績是經過 大量的人盲測投票之後的結果 意思就是說 這個大型元模型的成績呢 是反映了大家 使用上真實的喜好 大家使用上是 比較喜歡這個模型 這個模型它的排名就比較高一點 那相對之下呢這些標準化的Benchmark 就不是很好的標準 因為他們這些題目 真的跟一般人使用這些大型元模型 的場景是差的非常遠
(18:17~19:17) 然後過程之中呢也有很多 的這些漏洞 比如說一些Data Contamination Benchmark可能不小心流進 訓練資料當中 這個模型都已經把檔案背起來了這種問題 不過蠻可笑的是九成的媒體 在報這些模型的時候 他們在比較好壞都只會拿 這些Benchmark的成績 然後也把它講得很絕對 高一趴就是比較厲害這樣 但我自己是覺得 真的是參考一下啦 明明有更好的Evaluation 就是這個Chapar Arena 為什麼你不看呢 像臉書這次也知道Benchmark 是不太好的 所以他們除了Benchmark以外他們也另外 做了這些真人的評測 因為當然他們在 釋出之前呢 他們並不會有Chapar Arena 的成績 所以可以自己抓一批人來做評測 他們有做這個評測 但大部分媒體還是不報 還是去報這個Benchmark 我自己是覺得蠻可笑的
(19:17~20:17) 反正不管啦我們還是講一下 這個Benchmark的成績 但我們待會是比較Focus在 Chapar Arena的表現 Lama 3在Benchmark這邊的成績 當然都是比它同等級的這些 模型是更厲害的 所謂的同等級就是差不多 大小的模型 而這個Lama 3呢 它就有跟Google的Gemma 7B比 還有Mystral 7B比 你現在如果在 在想為什麼Lama是8B 為什麼不給它出個7B 明明這個Lama 2也是只有7B 那這個其實是因為 他們模型架構上有了一些 調整啊 這邊我待會會再介紹 在所有這些常見的Benchmark 也就是這個MMLU GPQA Human Eval GSMA-K這些Benchmark之中 這個Lama 3的表現 都是比較好的 然後在比較大的這個 模型尺寸就是Lama 3 70B的這個尺寸 他們是拿Gemma 9 1.5 Pro
(20:17~21:17) 跟Club 3 Sonnet來比 然後這個是在大部分的Benchmark 它也是比較強 雖然說在其中兩個Benchmark呢 它是有輸Gemma 9 1.5 Pro一點 然後老實說 這個比較我覺得也不算是很公平 因為Gemma 9 1.5 Pro 絕對大於70B很多 我自己覺得啦 我也無從驗證啦 但我覺得應該是這樣 Benchmark這邊的表現呢 比其他同等級的模型是厲害很多 很棒這個拍拍手 接下來呢我們來講重點 也就是在這個Chapar Arena中呢 Lama 3的表現如何 那你要知道這個是經過了 上萬人實際盲測 也就是他們並不知道他們在 跟模型講話 但是他們會選擇一個回答比較好的 上萬人實際盲測了 Lama 3之後 給他的一個排名這樣 在這樣子的一個排名之中呢 Lama 3 70B排到了第六名 而且在他前面的 這個五個模型之中呢
(21:17~22:17) 有三個是GPT-4 就是這個GPT-4在不同的時間 有釋出不同的版本嘛 那你把這些重複的去掉 他前面只有三個模型耶 GPT-4 TURBO 還有這個 GOOGLE GEMINI 1.5 PRO 然後還有 ANTHROPIC CLAW 3 OPUS 你要知道這三個模型就是我們所謂的 Tier 1的模型 或是你說God Level Model 他們這些就是一個自成一格的存在 他們就是非常強 然後在他們下面的其他模型 就立刻輸他們一大截 這樣子的存在 那這個Lama 3 70B沒錯現在還是輸他們一截 但是他已經成為了 Tier 2所有模型的頭了 他已經打贏了其他所有的模型 他眼前就只有這三個最強的 而已了那具體來說他打贏了誰 他打贏了GEMINI PRO 1.0他打贏了 CLAW 3 SONNET也就是這個OPUS 下面的一個版本他打贏了 Cohere這間公司出的Command R Plus然後他也打贏了
(22:17~23:17) 第一版的GPT-4 也就是去年3月14號 出的GPT-4 我知道你如果沒有時時刻刻在關注 這些不同的大學員模型 你可能聽剛剛一大堆名字一大堆專有 名詞你聽了都亂了你也不知道 哪邊是重點嘛 所以我這邊就先幫大家白話講一下 這邊的重點是什麼這邊的重點有兩個 第一個臉書再次 成為了開源社群的領導者 他因為他打贏了 Command R Plus 在臉書這個Lama 3出來之前 Command R Plus就是開源社群 最厲害最強的模型 他是一個104 billion 參數的模型 然後這個Lama 3用70 billion個 參數就贏過他了 所以說臉書這次不但是贏回了 開源社群的寶座他還贏得 很漂亮另外一個重點 就是你可以從Lama 3的成績看到 說這個大學員模型 這個領域究竟進步的 有多快因為這個Lama 3 他是打贏了GPT-4 031-4 就是這個3月
(23:17~24:17) 去年3月14號出的GPT-4 版本還記得當時這個GPT-4 剛出來他真的是贏過 所有其他的模型一大截 沒有人趕得上他 然後有傳聞說GPT-4是一個 超爆大的模型 有1.8 trillion個參數 1.8兆然後他是一個 MOE的模型所以說雖然說他有 1.8兆個參數但他真正在 使用的過程中呢並不是 同時會用到1.8兆他只會用到 四分之一的參數這樣 但也四分之一也是 有四百多billion的 也是很大的一個模型了 那這件事情OpenAI當然沒有自己公佈 但是一開始是被 George Hart給爆料 另外在前一陣子的 NVIDIA GTC 老黃感覺也有點說溜嘴 老黃都一直說 假設我們今天要train OpenAI 1.8 trillion的MOE 模型靠北他就是在講 GPT-4啊反正我要說的是 去年OpenAI這個看似 無人能超越的這個GPT-4
(24:17~25:17) 模型現在我們用一個 70 billion也就是比 1.8 trillion小25倍 的一個LAMA3的模型 就可以超越他了我真的覺得 現在AI的發展真的是快得很 誇張啊這種一年25 倍的進步你在其他哪一種 科技上看過就是你看 最快的在這之前人類 歷史上進步最快的一種科技 一種技術就半導體嘛 這個MOE定律每兩年 增加一倍一倍的電晶體 或者說電晶體縮小一倍 這樣你每兩年增加一倍 這個進步的速率就真的已經是 非常嚇人了你有沒有看 三四十年前的那種處理器 長什麼樣然後我們現在這個 可能iPhone裡面的處理器長什麼樣 你就知道了現在都已經 幾十billion個 電晶體幾百billion個電晶體 在一張小chip裡面了 那現在這個AI發展的速度比MOE定律 快這麼多一年25 倍的成長你覺得這個 接下來的五年會發生什麼事呢 那雖然說這樣子比較
(25:17~26:17) 是實在不公平啊就 AI是軟體嘛然後這個半導體 是硬體那軟體 的開發週期這個疊代 速率一定會比較快一點 那這個是很自然的然後 我覺得軟體可能也 比較容易遇到一些比較多的瓶頸 吧就可能硬體上啊 資料上whatever那這個 這個半導體呢他的瓶頸 就是物理的限制 就是你沒辦法突破物理極限 就是可能再過個幾年這些 電晶體小到一定程度呢 他們就開始有一些這個量子特性 出現那那個時候呢這個 這個晶片就不work了 你就不能再更小了但無論 如何啊我只是想說 哇what a time to be alive 我們可以見證到這個 人類歷史偉大的一刻然後 在這個刺激 又緊張的這個 時代中活著是很酷的 一件事情好啦回到這個LAMA 3 的表現那除了70B以外 LAMA 3 8B在這個 Chapel Arena的表現也是非常的
(26:17~27:17) 好那他打贏了一些像是 MIXTROL 800022B 跟GPT 3.5 TURBO的模型那一樣 這兩個模型都是比他大非常 非常多的模型MIXTROL 800022B 你自己乘一下就大概 176雖然說他實際的參數會 比這個再小一點因為他是MOE 但也是100多個模型100多Billion個 參數的模型 然後這個GPT 3.5呢 就是175Billion參數嘛 所以這個8B也真的是非常的 厲害好那你聽完了我剛剛 講的這些你可能會覺得哇 這些LAMA 3的模型真的是太強了 靠著他們這麼小的尺寸這樣 可以打贏比他們大這麼多的模型 甚至已經開始有點接近這些 GOD LEVEL tier 1的模型了 但我接下來要講的是 更驚人的就是 我剛剛講的這個排名是在 各種的問題綜合之下的 表現就是各種 就是大家盲測的時候可能會問各種 問題嘛所有問題混雜 在一起的綜合表現這樣 但是假設你今天只單獨看
(27:17~28:17) 英文的問題部分的排名 就是你挑 挑掉其他的語言像是中文 或者是這個法文 然後也挑掉像是Coding的問題 你單純只看這個純英文 問答的這些問題你會發現 LAMA 3的表現又進步了非常多 他們在這個 純英文問答的這個排名之中 他們是全部模型的 第二名 他們打贏了GEMINI 1.5 Pro 也打贏了Claw 3 Opus 他們只輸給GPT-4 Turbo 也就是說你如果單純 只看這個純英文的問答 這個LAMA 3 70B 他可以被當作GOD LEVEL MODEL了他是tier 1 MODEL的一員了而且還是 比較強的tier 1 model 比1.5 Pro跟Claw 3 Opus強 所以這邊又帶出另外一個重點 我想討論的就是我覺得開源社群 在此時此刻已經非常 接近幣源社群了 因為這個LAMA 3的出現呢 真的是帶開源社群往前走了一大步 要知道在LAMA 3出來之前
(28:17~29:17) 排行榜前幾名全部都是幣源模型 這些tier 1 model當然全部都 幣源模型嘛在他們之下的幾個 模型其實也都是這些 模型的小型版 Germany 1 Pro還有Claw 3 Sonnet 在他們之下 通常還有好幾個 才有一個開源模型 可能是Mixrol的 在前一陣子 Cohere出了Command R Plus之後 Command R Plus確實追到了 Sonnet之下 但也上不去了 但LAMA今天出來是直接開始逼進 tier 1 model了 甚至在英文純英文這邊的表現 他已經是一個tier 1 model了 他只有70 billion個參數而已 你別忘了臉書還在訓練一個 405 billion參數的一個模型 當這個模型出來之後 他可能自己成為一個tier 1 然後現在的這些tier 1 都變成tier 2 所以現在因為LAMA 3開源社群 已經非常接近幣源社群的 最強模型了 而且有可能在這幾個月後
(29:17~30:17) 開源社群甚至會 超過幣源社群 像當初這個GPD 4剛出來的時候 那時候LAMA也出來沒多久 那時候是LAMA 1 那時候的LAMA 1跟GPD 4 根本是天跟地的差別 完全不能比啊 但現在這個LAMA 3已經很接近GPD 4了 開源社群的發展也真的很快 但接下來 我覺得有一件事情可能會讓這個差距 瞬間又被拉開 這件事情就是GPD 5 OpenAI在今年 應該是會使出GPD 5 然後甚至我覺得有可能在 接下來這幾個月就會使出 有可能在這個臉書的405B 出來之前就使出 那這個GPD 5呢現在有各種 謠言啦但我自己真的是 覺得有可能是跟 那時候GPD 4出來一樣 又跟其他所有模型拉開一大段 距離畢竟這個OpenAI 就是去年3月就有GPD 4了嘛 然後他靠著這個GPD 4的模型 然後靠著之後慢慢微調他
(30:17~31:17) 他已經可以撐到現在都還當 都還在撐網了 那在這段期間這一年多的時間 他當然是有持續的在開發 他的GPD 5 所以說這個GPD 5呢 我相信真的會比其他模型 強非常多 好那講完了這個LAMA 3他究竟有多強之後 我們接下來聊聊他究竟為什麼 可以這麼強 那這邊呢我覺得可以分成三個部分來看 分別是預訓練也就是這個 Pre-training然後還有Post-training 也就是這個 後訓練就是預訓練之後的那個訓練 然後還有第三個部分是 模型的架構 先簡單介紹一下這個Pretraining跟Post-training 好了這個是訓練一個 大訓練模型最主要的兩個訓練 階段那在這個 Pretraining的這部分呢你會拿 極大量的資料通常是 幾兆個Token這麼多的資料去讓你的 模型建立起這個世界 的基本認識這個 基本知識他會學會各種語言 所有名詞各種領域的知識 等等的
(31:17~32:17) 如果你從一個比較Machine Learning的角度來看 他這個階段他訓練的目標 就是讓模型獲得最好的 文字接龍能力 他訓練資料裡面常常出現 生龍活虎生龍活虎生龍活虎 然後他就是要訓練到 一個程度是他看到 生龍活之後他知道要接 虎就是這樣那Post-training呢 就不會用這麼大量的資料去訓練 而是用這個精挑細選 比較少量的資料去訓練 這個模型 這邊的訓練目標就是讓模型 更熟悉某些特定領域的知識 然後同時也更能夠 像一個聊天機器人 一樣跟人類對話 然後也更能夠 符合人類的價值 也就是說這邊模型要去學習 我要怎麼樣回答人類才會比較喜歡 然後我有哪些事情是可以講的 哪些事情是不太可以講的 比如說人家跟我說 要怎麼製作一個炸藥 然後跟他說不好意思我不能幫忙 那首先在Pretraining這邊呢 LAMA 3這次有兩個進步
(32:17~33:17) 第一個進步就是他們用了 更多的資料 比上一次LAMA 2 訓練資料多了7倍 他們上一次LAMA 2只用了 2 trillion個token 他們這次用15 trillion個token 另外一個進步就是他們調高了 訓練資料裡面Coding資料的比例 就是這些程式碼的 資料的比例 我記得是調高了4倍這樣 那他們這次LAMA 3的變強呢 100%跟預訓練資料量的變多 跟Coding資料的增加 是有關的 但這個關聯性究竟有多強 就是他們這一部分的進步 究竟是貢獻了他們最後 成績進步的幾% 我自己是覺得應該是絕大多數 雖然說他們還有其他部分的進步 這待會我可以再講 就是Post Training跟模型架構 但我自己是覺得這個預訓練資料的 增加是最大的關鍵 因為其實像竹克柏上人家的Podcast 聊到這個LAMA 3的進步 他也都是在講這個訓練資料的增加 就是他有說
(33:17~34:17) 他們有發現他們這次 首先他們這次 特別實驗這個 使用極大量的預訓練資料去訓練模型 然後過程中他們發現 隨著這個訓練資料不斷的增加 這個模型的表現 也是持續的在變強 絲毫沒有飽和的現象 就是說這個變強的幅度是Log Linear的 也就是這個 你的訓練資料 它變多的倍數 是取Log之後 才得到你模型變強的程度 這什麼意思 白話解釋好了 白話的解釋就是說你的資料以指數形式在成長的時候 你的模型成效也會跟著成長 但它是以線性的在成長 也就是說 你的資料可能從 2倍變到4倍 8倍再變到16倍 以這種2的倍數指數成長 但是你的模型成效 只是會從 3分變成 6分變成9分變成12分 變成15分
(34:17~35:17) 它是這個比較線性的成長 但是儘管如此這個模型確實是 有時時在在的變強的 然後就算他們訓練到了15TL的Token 這個模型還絲毫沒有飽和 所以它可以繼續 再訓練到20TL然後30TL OK我不知道30TL會如何 但至少 接下來的這幾個TL 這模型仍然會持續的變強 他們當初會停在15TL 完全就是因為他們覺得 再訓練下去就太浪費這個運算資源了 我們要把運算資源 留到訓練下一個模型 或者是訓練405B的 模型 這件事情對於整個AI領域 整個AI社群來說也算是一件新的事 也算是一個Insight 為什麼呢 因為在這之前 大多數人在訓練大型元模型的時候 他們都會選擇一個Scaling Law來Follow 這個Scaling Law在講的就是 你在訓練一個更大的 大型元模型的時候 也就是你要Scale的時候 你要先Scale模型的參數
(35:17~36:17) 還是先Scale模型的訓練資料 哪一種做法或者是哪一種配方 才可以讓你訓練出最好的 大型元模型呢 很多人會Follow的一個Scaling Law Google DeepMind在2022年 出的一個Scaling Law 叫做Chinchilla的Scaling Law 這個Chinchilla的Scaling Law就是說 你在訓練一個大型元模型的時候 你要根據你這個模型的參數 有多少來決定你應該用 多少的訓練資料 每一個模型的參數數量 都有一個對應的 最適合的訓練資料的數量 很多人在訓練 大型元模型都是Follow Chinchilla的Scaling Law Lama2也是Follow Chinchilla的Scaling Law 他們之所以會選擇使用2 Trillion Token 就是因為2 Trillion Token 就是對於Lama2 70B 一個70 Billion的大型元模型 是最適合的訓練資料量 由Chinchilla的Scaling Law 算出來的量 但是其實大家都Follow這個Chinchilla的Scaling Law
(36:17~37:17) 是有一點點侷限了 因為為什麼呢 Chinchilla的Scaling Law所建議的 訓練資料量是最適合的訓練資料量 並不是最好的 並不是讓你模型可以表現最好的訓練資料量 並不是讓你模型可以表現最好的訓練資料量 那最適合的意思是什麼呢 其實是最Computed Optimal 也就是說 你用這樣子的訓練資料量 你可以最有效的運用你的運算資源 你不會浪費太多 但是你繼續 再增加你的訓練資料量 並不是代表模型會變弱 或者是走歪還是怎樣 它其實是會變強的 只是你會開始浪費越來越多的運算資源 那臉書這次就有點像是在測試說 他媽的不管了 不管Chinchilla建議2 Trillion 還是1.4 Trillion的Token 不管了我們直接用 15 Trillion的Token下去訓練 那過程中我們浪費一點運算資源 沒差 就儘管是這個 模型的變好是以Log Linear的方式 再增加
(37:17~38:17) 也沒有關係啊 運算資源沒差 我們就實驗看看 看他訓練到第幾Trillion Token的時候 他會Plateau 就是他的模型成效會開始停滯不變 但他們沒有發現停滯 這個模型就一直持續的變好 那我個人覺得臉書做的這個決定 是我自己覺得很明智 然後很棒的一個決定 為什麼呢?因為他雖然是 看起來好像是在這個訓練的時候 多用了很多的運算資源 但他其實是在審你 他其實是在審你 每個使用者使用這個模型的時候 的運算資源 為什麼?因為你現在有了一個 比較強的70B模型 不是嗎?比較強的70B跟比較強的8B 你以前要達到 你根據Chinchilla的Scalling Law 你要做出一個 跟現在Lama370B差不多 強的模型 你可能要好幾百Billion參數的模型 那確實你在訓練他的時候 你是最Compute Optimal的 你是不會像訓練Lama3一樣
(38:17~39:17) 花這麼多的運算資源 但是使用者 在用這個模型的時候 是大很多的 當你這個模型是大規模的 讓很多人使用的時候 這個Inference的Cost Inference的時候的運算Cost 絕對會遠超過你預訓練的時候的Cost 所以臉書這麼做真的是 很聰明喔 然後我也覺得Lama3他們大部分的智商 應該是來自這一點 但我剛剛也有說到 就是他們Pre-training的訓練資料裡面 Coding資料比例的 Coding資料的比例是調高了 那你聽很多的這些 主流媒體還是自媒體的評論 他們講到這點都是講說 臉書這次為了提升Lama3 的Coding的能力 他們還特別提升了這個Coding資料的比例 但其實不是這樣 他們提升Coding資料的比例 並不單純是為了提升 Coding能力而已 我們有發現說 這些大型語言模型在訓練了很多Coding資料之後 他們不只是Coding能力
(39:17~40:17) 變強了 他們各種邏輯推理能力也變強了 那這個就跟人類學習Coding是一樣 人類學習Coding不只是 為了會寫Code而已 還可以增加自己的問題解決能力 邏輯推理能力 因為這個程式語言 本身就是一個邏輯非常 嚴密的語言 那這些大型語言模型 他們通過這些Coding資料 學到了很高層次 這些抽象的邏輯推理能力 所以這次臉書就有刻意 調高Coding訓練資料的比例 以上這個是 Pre-training的部分 再來我們講講Post-training的部分 Post-training的部分老實說 其實沒有太多公開的資訊可以講 我們只知道一些大方向 他們有特別提到 這邊他們的資料有非常 仔細的挑過 當然具體上是用什麼樣的filter 怎麼挑這個我們都不知道 我們也不確定之後他們 論文釋出了會不會講 但目前我們就
(40:17~41:17) 只是知道他們用很高品質的資料 除此之外臉書也講了一下 他們Post-training 比較高層次的策略 這個其實也沒有什麼特別的 跟很多大家用的策略 是一樣 就是Supervised Fine Tuning 然後還有一個RLHF Learning from Human Feedback 然後RLHF又分PPO跟DPO 這邊就是 有興趣的可以自己再去查 這些關鍵字啦 Post-training我們就簡單講到這裡 除了這個Post-training跟Pre-training的 資料上的優化以外 他們模型架構上 也有一個蠻顯著的優化 就是他們有選擇一個 不同的tokenizer 這個tokenizer顧名思義 他的工作就是把文字 換成token 因為我們知道這些大型元模型 除了他們以外,這些大型多模態模型 這些所有Transformer的model 他們都是吃token 他們都是認token,他們不會真的去認 這些英文單字
(41:17~42:17) 所以你要先把這些英文單字變成一個token 這個就是tokenizer的工作 這每個tokenizer 他都有他自己的詞彙量 也就是說 他只能把所有的英文單字 Map到一定 數量的token當中 他們選擇的token就是那幾種而已 這次他們LAMA3 他們用了一個 有更高詞彙量的tokenizer 他們原本LAMA2的tokenizer 只有3萬2千個詞彙量 他們現在新版的 tokenizer有12萬8千個詞彙量 他們這邊 tokenizer的優化 主要是為了 使用上的效率 再進行優化,而不是為了 表現而優化 他會優化使用上的效率 是因為當你今天用 更多詞彙量的tokenizer 再tokenize一串文字 你可以把這串文字變成 更少的token 因為今天他增加了很多詞彙量 就代表說很多常用的字
(42:17~43:17) 他並不需要再把它break down成 多個token 他可以直接用一個token來代替 比如說原本icecream 他是把他變成兩個token 他有一個ice的token跟一個cream的token 但他沒有icecream這個token的 字彙量 但今天你有128k 你新增了icecream 的token進去 你icecream這個字你直接用一個token 來代替就好了 所以同一串文字今天你只要用比較少的token 來代替,那你在運算的時候 你就省了一些運算 除了tokenizer的改變以外 他們這次也為小模型 新增了一個新的attention 叫做grouped query attention 這邊可能比較技術一點 我就先不要講太多 但反正他是讓這個 在運算attention 也就是這個注意力機制的時候 可以更有效 靠著這兩個優化grouped query attention 跟這個新的比較大的tokenizer 他可以讓這個LAMA3 8B
(43:17~44:17) 在跑的時候 使用的運算量是跟LAMA2 7B 是差不多的 LAMA3他硬生生是多了1Billion的參數 但他實際使用上的運算量 其實是跟7B是差不多的 很厲害喔 但其實這個tokenizer改成128K 他雖然說 主要是為了這個模型運算上面 的一些效率提升 但他其實對於成效的提升 也是有一點幫助的 因為他現在有更多的詞彙可以用 然後這個模型 就可能可以學得更好之類的 但是總括來說啦 那這個LAMA3這次變強呢 我覺得主要還是要歸功於他們 預訓練資料的變多 好那我們現在講完了這個LAMA3 模型他究竟有多強 以及他為什麼這麼強 我們接下來就來講一些 臉書他們這次還有發表的一些 其他內容以及他們的下一部 究竟是什麼 臉書這次他們其實有發表一個叫做 meta.ai的一個網站 在這個網站上面你就可以
(44:17~45:17) 使用這個LAMA3的模型 就是這個70B的模型應該啦 就是我是直接問他說 which model are you which model are you running on 他是說這個70B 所以我猜他應該就是跑這個70B的 LAMA3 但除了這個文字的模型以外呢 你還可以產生圖片 他們產生圖片這邊的功能是挺酷的 就是他產生圖片是非常 快的多快呢 快到你在打prompt的時候 你每打一個字他圖片就 就根據你目前的prompt去做變化 他產生圖片的速度 已經跟你打字的速度是 差不多了 那這部分呢我有在我的IG上面發一則 現實動態demo給大家看嘛 就是我打說 一個瑪利歐 然後我打到這邊就瑪利歐的照片就出來 然後我繼續打賽車 然後就變成瑪利歐賽車 在一個街上 然後這個街景就出現 反正就我每打一個字他的圖片 直接呈現那樣子
(45:17~46:17) 這當然是蠻酷的一個技術 但他其實不是新的啦 去年年底Stable Diffusion 就有做出這個技術了 他們是做了一個叫做 SDXL Turbo的模型 他就是可以即時的 產生圖片 大部分人呢我覺得他們的認知 都還停留在Diffusion Model 產圖就是很慢這件事情上 因為確實Diffusion Model 一開始是被歸類為 非常慢的一個模型 因為他產生圖片的過程中 他是要經過 很多個步驟的 可能十幾二十甚至五十 步以上的步驟 這所謂的步驟呢 一個步驟就是他從 一張圖片中移除一些 雜訊的過程 重複這個過程幾十次 你才可以產生出一張圖片 但今天這個Stable Diffusion 用SDXL Turbo的技術 你只要一步就可以產生圖片了 他直接一步到位 不用很多步來移除這個雜訊
(46:17~47:17) 他一步就把所有該移除的雜訊 全部移除 具體來說這個技術 是怎麼做到的我們今天就不細講了 我們也不可能有這個時間 說真的我還有很多我原本要講的東西 但我看現在時間已經四十幾分鐘了 所以說我們就不細講 但反正這個技術不是新的啦 但就是挺酷的這樣 另外他們還有發布一點就是說 他們要把Meta AI 現在整合進很多他們的 社群媒體的產品中 就是包括WhatsApp 然後IG跟FB 我不確定有沒有 但我確定我有聽到的是WhatsApp 就是你之後在使用WhatsApp的時候 你可以直接開一個 聊天實裝跟Meta AI聊天這樣 我有去更新了我的WhatsApp 然後是沒有看到這個feature 所以我想應該是這個 只有北美的用戶可以先使用吧 或者是甚至是一個一小部分的 這個測試觀眾在使用這樣 但我覺得 我有想像了一下這個 使用場景我自己是覺得
(47:17~48:17) 這感覺是挺實用的一個使用場景 就是你在WhatsApp上跟其他人聊天的時候 確實有時候你會 需要一個LM來幫助你 思考一些回答 這樣就假設你 今天是在跟一個客戶講一些 可能你不是非常懂的領域 你可能會需要一個LM 即時幫你驗證一些東西 那這個時候你就不用跳出去 然後打開ChatGPD的app 然後問了然後再複製就過來這樣 你直接在WhatsApp裡面 就可以使用的話是蠻方便的 然後甚至會不會有一種使用場景是 你今天要把妹的時候 你可以直接使用 這個Meta AI幫你塑造一個 人格形象 就是你今天 聊到一個新的妹子你就 搞不好這個Meta AI有個對話串的功能 你就開一個新的對話串 然後做一些prompt engineering 跟這個Meta AI說 接下來我會假裝我是一個妹子 然後你要想盡辦法 把這個妹子給拔到 然後你要裝作一個個性
(48:17~49:17) 高冷但是 很聰明的一個男人 這樣 之後這個妹子傳訊息給你 你就直接把這個訊息FORWARD到這個對話串裡面 然後這個Meta AI 回復完之後你再把Meta AI的回復 直接FORWARD回去 那這樣子你就 輕鬆 輕鬆穩聊妹子好不好 而且還可以一次跟十個 再來我們講講這個Meta的下一步規劃是什麼 就是大家要知道 其實他們這一次 釋出LAMA 3是一個 Premature release 就是他們其實並沒有內部並沒有準備好 他們內部並不覺得 LAMA 3已經準備好要釋出了 這次主要是 主客伯他想要趕快 把這個東西丟出來 他們內部才趕快趕了一個 Block post趕快把這個 這個Eval的東西 趕出來這樣 他們內部會認為這個LAMA 3是不完整的 主要就是因為他們這個 405 billion的這個模型
(49:17~50:17) 還沒有出來嘛他才訓練到 一半這個因為主客伯 一聲下令他們還為這個 模型趕快趕出了一些 benchmark的成績這樣給大家 看但他其實真的只訓練到一半 而已然後除此之外呢 就是一些現在已經變成 這個業界標準的一些 大型元模型的feature 就包括這個長 context對不對他可以一次 吃非常多的文字的這個 能力以及這個multi modality對不對這個理解 多模態包括這個圖片 影片聲音的這些能力 這些都已經慢慢變成 業界標準了那這次 LAMA 3的釋出都沒有釋出這些東西 所以說 這個內部很明顯的是 都還沒準備好 但主客伯也有說 接下來他們要釋出的就是 這一系列的功能那這邊 大家就拭目以待啦 最後啊我覺得還是想 回到這個我們一開始講到 的這個meta的新聞就是
(50:17~51:17) 有兩個嘛一個是meta 他開源他們的horizon os也就是他們這個 這個metaverse頭盔的這個作業系統 然後另外一個新聞呢 就是meta他股價 暴跌那主要也是 因為投資人覺得 主客伯他花很多的錢在 投資AI跟metaverse的計畫 但這兩個計畫都沒有 在賺錢因為大家 要知道這個meta他 差不多98%的 revenue都是來自他的 這個社群媒體 的廣告營收那這些 AI跟metaverse 是根本就沒在賺錢的甚至在 虧很多錢尤其我相信很多人 不解的就是這個臉書 花這麼多錢去開發這個 LAMA3的模型但他把它開源 開源的意思就是他免費讓 大家使用耶除非 你的這個產品真的有超過 7億的月回月用戶 不然你都不用付錢就可以用 這個產品就可以用這個模型了 然後在這個metaverse這邊也
(51:17~52:17) 一樣他直接把這個 的作業系統直接開源給 所有人使用以後這個 ASUS的ROG 可以使用metaquest horizonOS這個 Lenovo也可以使用horizonOS Samsung我不知道Samsung會不會 使用他們可能是使用Google whatever 但竹科爾博到底為什麼砸了 這麼多RND砸了這麼多運算 砸了這麼多開發成本 下去然後開發 出來的產品就免費拱手送人 這樣那 我最後就想聊聊這個話題 就其實啊我覺得 這個臉書開源這些技術 是有價值的然後 也是有很好的理由的首先 一個最好的理由就是當臉書 今天不是靠技術A在賺錢 而是靠產品B在賺錢 而技術A可以 幫助到產品B 那臉書就應該開源技術A 因為開源之後這個技術A 一定會進步的越來越快 因為這個整個社群都可以 幫助技術A持續的迭代
(52:17~53:17) 持續進步嘛最好的一個例子 就是臉書在2010年 初那個時候他們開源了 他們自己資料中心的一些 Server的設計 Rack的設計Networking的設計 他們把這個稱作Open Computer Project OCP 這樣那這個 OCP在整個資料中心的供應鏈 產生了非常大的影響力 很多這些 廠商都開始遵循 他們這一套OCP的標準 這個OCP也建立起了 一整個社群這樣 迎來非常多公司的加入 那大家就一起集思廣益 想辦法讓整個資料中心的 硬體以及運行的成本 都大幅的下降這樣 那臉書確實是免費 拱手送出了這個 他換多R&D費用去開發出的 這些這個伺服器的design 但是 他其實省了更多錢 為什麼 因為這當然就是剛剛說的嘛 大家集思廣益讓整個資料中心的
(53:17~54:17) 這個Server 這些Cooling這些Networking的設計 讓他變得更有效 所以說臉書靠著這個 新版的這些Server呢 他是運行成本是降低的 然後另外一個更顯著的就是 這些Server本身這些硬體的 這個成本也降低了 因為很多的這些製造商都開始 採用這個OCP的這些標準 那越來越多製造商用同樣的標準 就越來越多的競爭 那越來越多的競爭就越來越低的價格 對吧這個就是 經濟學的原理嘛 透過這個Open Compute Project 臉書只是Open Source的一些design 但他最後得到了 省下了超級多資料中心的成本 然後今天竹克博 覺得AI是一樣的道理 他們不覺得他們會像是 OpenAI或者是Google一樣 釋出一個AI的subscription service 像是這個ChadGBT或者是Gemini 這樣子的service 那他們這個就 這個MetaAI呢是完全免費的 竹克博認為AI這個技術
(54:17~55:17) 對於臉書現在的價值 是在於他會怎麼樣去 更豐富現在他 這個目前的商業模式 或者是 更加強他目前的產品這樣 那說真的 就是這個 這個是大家還沒看出來的部分 就我覺得大部分投資者 還不確定這個AI 究竟會如何幫助到 臉書的top line 臉書的這個revenue 但竹克博有給出一些線索 竹克博覺得首先他可以 讓所有的創作者跟中小企業 都有一個自己的AI 因為這些社群媒體上有很多的 創作者跟中小企業 他們都 就是缺乏跟粉絲 互動的時間 那他們覺得 竹克博覺得他可以透過這些AI 製造出很多這些分身 然後讓這些分身跟這個 粉絲互動 雖然說老實說就目前看到這個 臉書的初步嘗試
(55:17~56:17) 我是覺得好像沒有太大的用處 就臉書不是 在去年Meta Connect 他有發表一個 我一定忘記叫什麼了 但反正是你在IG上 你會看到一些明星的假IG 像是 有這個美國的超模 Kendall Jenner 還有MrBeast世界最大的YouTuber 他們有這些 假的IG profile 然後他們 他們有取一個假的名字 像是Kendall Jenner就變成Billy 然後你可以 密這些profile跟他們聊天 根據臉書的說法 這些profile 他們聊天起來 是真的很像明星本人的 然後可以幫助你 回答一些你人生的 疑難雜症 說真的我覺得 大部分人的反應是 一部分人是覺得這到底三小 你放這個到底有什麼用 另外一部分人是覺得
(56:17~57:17) 這個太詭異了吧 有點讓人毛骨悚然 你長得就跟Kendall Jenner 一模一樣 為什麼你叫做Billy 這個問題真的是OK的嗎 所以老實說啦我自己也是沒有看得很明確 說就是這個 臉書未來AI的策略 究竟是什麼 AI賺錢策略究竟是什麼 不賺錢的這邊我們看得很明顯 就是LAMA3會持續做大 持續變強嘛 但你究竟怎麼賺錢這我不確定啦 但這個祖克柏有自己的想法 然後我相信他 我一開始有說了嘛我很仰慕他 他很厲害 加油吧西域人 除了AI以外臉書這次在這個 元宇宙Metaverse的 作業系統的開源 他這邊就是另外一個考量了 我覺得他這邊就是要掌握 這個元宇宙作業系統 元宇宙生態系的主控權 那這個就 這個就是他未來主要想要 發展的business啦
(57:17~58:17) 就是這個祖克柏很明顯的他想要在 未來假設這個元宇宙真的變成主流了 他希望這個 MetaQuest是吃到絕大多數的 市佔率 他希望蘋果不會在這場 Metaverse的戰役之中 跟他在 手機市場取得一樣的成功 然後同時聽說Google也在 開發他們自己的 Android based的作業系統 老實說我記得這個 Horizon OS也是這個Android based 但他們是兩個不同的 紙牌啦對 反正這個Google也在開發他們的 Metaverse的OS 但這個臉書就想要搶先他們一步 先成為這個 Metaverse的最 主導的開源 那這邊我個人是覺得 先不管未來元宇宙會不會 變成主流 或是你說VR,AR,MR 我不管你要什麼名字去 稱呼他就目前來看 Meta絕對是這邊 最最領先的公司
(58:17~59:17) 而且是直接 開其他人一條街 我覺得他們在某些部分也是領先Applevision Pro 很多 包括MetaQuest Store 以及 我覺得一些搖桿上的操縱吧 所以這部分呢 我也是蠻看好臉書的 那我們今天就來聊聊 潮南西藝人 的公司Meta 最近的一些新的發表 最後也跟大家說一下 其實我這一集我是蠻想來念 一些Apple Podcast Spotify跟YouTube的一些留言 因為我覺得蠻多留言 我其實蠻想回應的但是 我常常錄音的時候 都顧著講自己的東西 我都忘記跟大家互動 所以說這部分是 我一直有在檢討我覺得我應該做個更好的部分 所以我原本打算這集 來念一些留言啦 但今天這集真的有點太長 然後我錄到現在說真的我有點累了 所以讓我先去休息 下一集我再來
(59:17~60:03) 可能念一些大家的留言 所以說大家可以趁這一集 就趕快先留一些 之類的 如果你喜歡今天的這集Podcast 也別忘了給我一個五星評分 把科技量Podcast分享給你的朋友們 因為這個 我已經講一百萬遍了 但我還是要一直講 Podcast頻道要成長真的很困難 Podcast平台 大家平時也不會划 然後演算法也沒有用 然後這個排名也很難 擠進去 所以真的是希望大家可以多多分享 讓更多人知道科技啦 然後同時也感謝一下今天的 贊助商Speak 想學習這個英文口語的 可以去參考一下 最後一樣就祝大家有個愉快的一周
