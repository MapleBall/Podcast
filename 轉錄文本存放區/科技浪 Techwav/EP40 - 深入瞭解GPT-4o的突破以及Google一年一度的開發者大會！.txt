(00:00~01:01) 【音樂】 哈囉大家好,歡迎收聽科技浪,我是主持人哈利 科技浪是一個白話跟你聊科技的Podcast 希望可以用簡單易懂,但是又深入的方式 帶你了解時下最火的科技話題 本集節目由天下雜誌贊助播出 台灣2000大企業,今年誰才是最大贏家呢? 天下雜誌今年首度推出了AI版營運績效50強喔 天下雜誌每年都會做2000大企業調查 今年他們獨家解析了台灣在全球供應鏈的關鍵角色 要幫大家回答一個問題 就是到底哪一些AI濃度高的企業是值得你投資的呢? 除了台灣AI產業的獨家報導以外 天下連續彙整了39年的2000大企業資料庫也會同步更新 到底誰才是全年營收獲利最好的企業呢? 誰又是成長最快的潛力股呢?
(01:01~02:04) 你都可以從天下的2000大資料庫之中 透過交叉比對與查詢輕鬆獲得企業的歷年資料喔 天下雜誌當然也是有給科技浪的聽眾一個獨家優惠 你現在只要花一塊錢 就可以解鎖天下2000大資料庫 無限查詢一個月 同時你也可以解鎖剛剛我說的天下雜誌獨家的 台灣AI50強深度解析 除此之外你還可以無限閱讀天下雜誌全佔文章一個月 全部的這些東西只要一塊錢真的是太賺了 原價要290現在只要一塊錢 這是給科技浪粉絲的專屬優惠 如果你是想要多多了解這2000大企業 以及台灣AI50強的話 你不要錯過這個好機會 現在在本集資訊欄裡面你就可以看到這個連結了 點進去就可以立刻購買了 本集業配就到這邊結束 謝謝天下雜誌的贊助 過去這個禮拜我有五天是待在美國的舊金山灣區這邊
(02:04~03:06) 會去主要就是要去參加Google一年一度的開發者大會 Google I.O. 這次是Google那邊邀請我去的 也是真的很榮幸 這次全台灣有被Google邀請過去的創作者 就只有兩個 一個是3C Tim哥 他是在YouTube上面講3C的 另外一個就是我們科技浪了 所以說真的是全台為二 然後這個Tim哥過去那邊主要是 因為手機3C這邊被邀請過去 就是Pixel 我們科技浪主要就是因為在AI這邊的專業被邀請過去 真的是非常的榮幸 感覺有被認可的感覺 這次真的是也是非常感謝 這個機票啊酒店啊 然後還有一些交通雜誌啊 都是這邊Google這邊幫忙負擔 真的是讓我的負擔變輕很多這樣 但是我也必須說就是
(03:06~04:08) 今天啊我還是要這個非常誠實的跟大家講一些我對於Google I.O.的真實想法 那這邊就是在後面再跟大家分享 那我這次去美國呢 我真的覺得是太精彩了 有超級多的故事可以跟大家分享 我是解鎖了蠻多的新的人生體驗啊 可以這麼說 就是包括我去做他們的這個Wemo的無人計程車 就是全世界只有在舊金山的一小個區域 有這種無人計程車的測試 我不確定大陸有沒有 但反正Wemo是只有全世界 只有在舊金山那邊有在試營運這個無人計程車的服務 那我去做了真的是太酷了 那除了這個以外呢 我當然也體驗到了這個AppleVision Pro 那這個因為這個就是在美國才會上市嘛 那這個Wemo跟這個AppleVision Pro呢 我其實都有蠻多心得可以跟大家分享的 但我覺得今天都先不講 我們今天就講一個故事就好了 因為我覺得我們今天這個正題的內容真的非常多 所以我們可能沒有那麼多時間可以去閒聊這樣
(04:08~05:09) 但我覺得有一個故事必須先跟大家說 因為實在是太酷 那這個故事呢就是在這個Google I.O.的當天 我上午聽完了他們的發表會之後 我下午就是到這個媒體的Press Lounge 也就是這個媒體的休息室 在那邊吃午餐然後坐著休息這樣 那這個Press Lounge除了這個一些桌椅然後餐點以外 他們還有一些這種不透明的小房間 是在Demo他們這次I.O.發表的一些AI產品這樣 當然其中就是包括他們這次發表的 算是一個他們覺得是這個重頭戲的Project Astra 但我自己這邊對於Astra是有點失望 那這邊我待會會再講 那現在不是重點 那我在桌子那邊吃他們的餐點然後休息到一半呢 我突然聽到有一些人開始在講說 好像有看到Google的共同創辦人Sergei Brin出現 我聽到的時候我整個我嚇一大跳 Sergei Brin是什麼樣子的人物 他竟然會出現在Press Lounge
(05:09~06:11) 然後我就立刻站起來到處去問就是 真的有人看到Sergei Brin嗎 他真的在這邊嗎 然後就有人跟我說他好像是在這個 在DemoProject Astra的這個密閉小房間裡面 那這個房間你要進去看這個Project Astra的Demo 你還要在外面排隊 所以我就立刻排進隊伍裡面 然後就想說哇靠 裡面如果真的有Sergei Brin 這趟Trip就太值得了 因為你知道這個Sergei Brin 我剛剛有去查 他是現在全世界第七名的富豪 全世界第七有錢的人 他的身價有1390億美金 非常誇張 而且他還創立了Google這間改變世界的這個科技巨頭 所以他真的是一個傳奇人物 我覺得只要看到他一眼就值得了 甚至連跟他講話都不用 我真的是很想看到他到底長什麼樣這樣 所以說我就立刻排這個Project Astra的隊伍
(06:11~07:11) 但是排了大概5到10分鐘 這個隊伍完全都沒有動 然後我就想說這個東西到底還要多久 所以我就問了排在我前面的那個人說 聽說Sergei Brin有在裡面 你是因為這件事情去排嗎 然後這個東西到底要排多久 前面的那個人就跟我說 沒有沒有Sergei Brin沒有在裡面 他剛剛有在裡面 但他現在已經走了 那我聽到當下我其實並沒有很失望 我反而是還有一點點驚喜 因為他等於是證實了這個Sergei Brin好像真的有 就是來到會場這樣 就是我並不是在做夢 所以我就想說好那他不在Sandbox裡面 他現在還他可能才剛剛出去沒多久 我在會場裡面晃一晃 我可能還是可以遇到他 所以我就立刻衝出Press Lounge 在會場那邊走來走去 嘗試要找到這個Sergei Brin 那我在外面走了大概15分鐘吧 完全沒有看到Sergei Brin 那走到後面我就我基本上就已經放棄了 然後就偷了外面的一些這個餅乾之類的
(07:11~08:11) 就回到Press Lounge這樣 因為Press Lounge其實有Press Lounge的食物 但是外面也有外面的食物 然後外面的食物我覺得比較好吃這樣 Anyway這不是重點 反正我就回到Press Lounge繼續休息之後 休息了一陣子我想說 我離接下來一個活動還有大概可能半小時吧 那這個時間可能還夠去看一下Project Astra的Demo 就是儘管看不到Sergei 我也是蠻想看這個Project Astra的 所以我就還是回到了這個Project Astra的隊伍這樣 那現在人已經比較少了 所以說我排一下就排進去了 那我進入這個小房間 他們裡面的人就開始進行這個Project Astra的Demo 然後Demo到一半 我那個房間的門就突然打開 然後Sergei Brin就走進來了 哇我看到他的時候 我真的我不敢相信我的眼睛 就我先看到他的臉就 幹這個人怎麼那麼像Sergei Brin 然後我就往下瞄 我就發現他帶著一個名牌
(08:11~09:11) 上面寫著Sergei Brin 然後這個在Demo的人呢 他看到Sergei進來 他也沒有特別去打招呼什麼的 他就是他好像有簡單說一句 嗨Sergei之類的 但反正他就繼續講他的Demo 然後其他人也很淡定 就持續繼續看Demo 好像Sergei也不只能這樣 然後所以我就只能轉過身去 繼續看Demo 但是我心裡面其實已經非常的激動 就是哇靠Sergei站在我的後面 然後這個Sergei就是看了一陣子 大概看了大概可能一兩分鐘吧 然後他覺得這個都OK 然後他就轉身就走出去了 然後他走出去這個Demo還沒結束喔 其實其他人就待在原地繼續看Demo嘛 那我也待了大概5到10秒 但我心裡就想說不行 我一定要追上去給他拍照 所以我就立刻衝出這個Demo房間 然後就跟Sergei說 Can I take a picture? 然後他就說sure 他也叫我的名字
(09:11~10:11) 他說sure Harry還是什麼的 然後我有簡單跟他介紹一下 就是我是一個Taiwanese podcaster這樣 然後雖然說我也沒有特別講科技啦 他應該沒有聽過 反正我們就 我就跟他拿手機拍一張自拍這樣 然後拍完之後就說 我剛握了手 我剛握到手了 這個1300億的這個身價的手被我握到了 事後我真的是非常激動 然後我在這個IG上把這張照片給PO出來 然後也是我覺得是近期得到最多讚的一個IG限動這樣 不過這個限動24小時內就自動刪掉了嘛 所以說我在想就是 我應該把它做成一個貼紋把它PO出來 讓它永遠的留在我的主頁面上面 然後把它置頂這樣 然後呢就是我應該還是 我應該還要把這張照片這個表框 然後貼在我的房間裡面 因為真的是太酷太酷了
(10:11~11:12) 我覺得他本人真的很酷 就是那種蠻隨性的那種Tech Billionaire的感覺 就是他就是鬍子也沒有什麼刮 然後頭髮就是捲捲的 就自然捲捲的也是很帥 然後就穿一件那種有點像Uni Kulo的那種排汗的T恤 然後戴一個墨鏡 就這樣 就你如果真的沒有看過他的臉的話 你是看不太出他是什麼特別的人物的 但他其實是全世界第七有錢的人 然後就出現在我眼前 反正很酷啦 那美國型其他的這些我解鎖的人生體驗 就是包括這個Wemo還有這個Apple Vision Pro 這些東西我們都等到下一次再講 因為我們這禮拜呢 真的有太多大事要講了 應該說主要就是兩件 那就是這個OpenAI的發表會 他們發表了GPT-4O這個模型 然後還有Google的IO 那這兩件事情都算是一個超級大新聞 那兩件事情本身都可以做成一集獨立的科技檔
(11:12~12:14) 但是呢他們就是在同一個禮拜發生了 然後我相信也不是什麼巧合啦 就是這個應該是OpenAI刻意的作為 就是你怎麼可能每次發表東西 都正好是在Google發表什麼東西之前 這太快了吧 就是在上次這個Google要發表他們的這個Gemini 1.5 Pro之前 OpenAI也發布了他們的Sora模型 我不確定是之前還是之後 反正就是在那一兩天內 就是發布了Sora模型 然後把Google的光彩全部都搶走了 全部人都在講OpenAI的模型 沒有人鳥Google 但那時候Google的1.5 Pro其實也是很大的發表 那這次是更可惡 就是GoogleIO是一個他們一年一度的超級大發表會 然後這個OpenAI還來搶他們的光彩 然後還搶贏了 就是這個不得不說 這次GPT-4O的發布 真的是贏過了這個GoogleIO的發表 那明明GoogleIO是可能長五六倍的一個發表會 然後還發表了這麼多東西
(12:14~13:16) 但光彩就是被OpenAI搶走了 所以今天呢 我就來跟大家深入的解析一下這兩個發表會 那沒有看的人也不用擔心 就是我也會在一開始先跟大家講一下這兩個發表會的內容 然後也會回答一下大家所有想問的問題 就是包括 為什麼這麼厲害 怎麼做到的 那這件事代表什麼 這些問題都會跟大家討論到 那我們就先從這個OpenAI的發表會 開始講起好了 他們的發表會是在GoogleIO的前一天 真的是非常心機的一個時間 那整個發表會全部都在YouTube上面可以看到 然後非常短 只有大概半小時 然後是非常精簡的一個發表會 整個發表會他們只有發布兩個東西 一個就是GPT-4O這個模型 然後另外一個就是MacOS的ChaiGPT Desktop App 那很明顯的重點是放在前者 後的那個App就是一個他們建出來的新的介面而已
(13:16~14:18) 重點是前者這個GPT-4O的模型 那這個GPT-4O的O是Omni的意思 Omni就是你去台北信心區會看到一間夜店叫Omni 沒有啦開玩笑 這個Omni的意思呢就是全部的意思 就像是這個Omnivores 就是那種雜食動物嘛 就全部都吃 那這個Omnichannel就是這個全渠道 對不對全部的這個渠道 那GPT-4O他想表達的就是他們全部的模態都有 那這邊的模態指的就是這個聲音影像文字 這種東西各是一個模態 那他們會取名叫GPT-4O呢 就是因為他單一個模型可以同時處理很多種不同模態的資料 那這邊主要有包括三種啊 就是這個文字、音訊跟影像 影像就是圖片啦 他還沒有到這個影片 但影片可能下一步這樣 那他現在就是同時輸入跟輸出都可以同時處理這三種模態 白話來講呢就是
(14:18~15:20) 不管你是打字傳給這個GPT-4O的模型 還是你用講的給他來吩咐他 還是你給他看圖片 他都可以理解 然後同時他能產出的也是這三種 他可以產出文字也可以產出聲音直接跟你對話 然後也可以直接產出這個圖片這樣 那這個GPT-4O呢一出來的時候立刻就爆紅了 因為他們Demo他們有實際在這個發表會Demo這個GPT-4O 然後他們也有拍很多這種GPT-4O的Demo影片 他真的是太強了 太太驚人了 那驚人的點呢有很多 首先第一個就是他聊天非常的自然 那這邊我會把一些我挑過的這種Demo影片 放在本集的資訊欄裡面 本集資訊欄最下面 我非常非常推薦大家先去把這些東西看完再來聽 你會更知道我在講什麼 那他講話非常的自然就是 他我可以很確定的跟大家說
(15:20~16:30) 他就是我從出生到現在看過講話最自然的AI 就是他不但語句中有非常正確的這個上下起伏的語調 他還有很豐富的感情在裡面 以免你懶得看下面的Demo 我這邊很簡短的放兩句他講的話給大家聽聽看 首先是第一句 那個女生在講話的聲音就是GPT-4O 然後他們講話內容呢 基本上就是這個GPT-4O在問工程師他們要發表什麼東西 然後工程師就跟他說 欸我們要發表的東西是你喔 其實就是你喔 然後那個GPT-4O就感到很驚訝這樣 好那大家聽聽看
(16:30~17:30) 然後另外一個呢就是這個工程師是叫ChadGPT數數 然後他之後又叫他數的更快一點 我們來聽聽看 好那現在我相信大家應該都認同了 就是哇他講話真的是非常像人類喔 雖然說有些人會覺得他有一點點太煽情了 好那剛剛聽完這些Demo 我相信大家應該有發現另外一個很驚人的點 就是你可以隨時打斷他 而且他回覆是非常即時的 Open來說這個GPT-4O回覆人類語音的速度呢 大概是200到300多毫秒的時間之內 那這個時間呢其實基本上跟人類在溝通時候的這個 人類講話的Latency是差不多的 然後他不只講話跟人類一樣自然 然後這個接話速度跟人類一樣快
(17:30~18:31) 他還有視覺 他看得到東西 就是在這些Demo裡面呢 剛剛那兩段沒有 但是你可以打開你的這個手機的相機 然後這個讓這個ChadGPT-4O有視覺 你可以問他說你看到了什麼 或者是這個東西的細紋怎麼講 或者是這一題數學怎麼解 那確實在這個Open來的Demo影片呢 他有Demo到這些應用 他有一支就是這個 他這個ChadGPT-4O在教一個小孩子數學 然後他不是就是跟他講答案而已喔 他是一步一步引導他把這個數學題目解出來 就是這個GPT-4O就跟那個小孩子說 欸我們先來找出這個三角形他的斜邊在哪裡啊 那你覺得斜邊是哪一條呢 然後那個小孩子畫了一下 他直接拿筆在那個平板上面 把一條他覺得是斜邊的邊給畫出來之後 然後GPT-4O看到他畫了之後就說 欸這個是一個Good guess 但是這是錯的喔 這個是零邊不是斜邊 斜邊是另外一條的
(18:31~19:31) 就一步一步引導他解題 然後除了這個以外呢 也有即時翻譯的Demo 也有這個語言學習的Demo 那這些所有的Demo你我建議大家去看一下 真的是非常的驚人 而且啊你知道嗎 這個GPT-4O強的地方還不只這些 他除了這個講話很自然 然後這個還有視覺以外呢 他本身的智力也很高 他現在基本上是全世界最聰明最厲害的模型 那這個就是依照這個 我最常用的這個Chatbot Arena的這個排名來看 他現在是第一名的模型 比原本的第一名這個GPT-4T還更強 而且不是強一點點喔 強一點點喔是強非常的多 直接強一個檔次出來 那這個GPT-4O他的成績呢是 他的E-Low Rating是1287分 然後這個GPT-4T他的成績是1252分 直接差了三十幾分 然後有些人可能會問說就是
(19:31~20:31) 這個GPT-4T才剛出來幾天 會不會是這個樣本數還不夠 然後這個分數他的起伏還很大 其實樣本數已經夠了喔 就是他已經有這個一萬五千多票了 那這是因為這個GPT-4T 講錯GPT-4O他在出來之前呢 他就已經有在Chatbot Arena上面 在讓大家做評測了 只是那時候大家都還不知道 這個模型是GPT-4O 他們那個時候是把它叫做 I am a good GPT-2 他的名字就這麼長 I am a good GPT-2 我是一個好GPT-2 我也不知道為什麼是GPT-2 反正大家也不知道這個模型是啥 但大家前一陣子就是有注意到說 有一個叫做I am a good GPT-2的模型呢 大家都不知道他什麼模型 也不知道他什麼來頭 但他的成績就是超級好 然後他用起來就是比GPT-4T還更強 那很多人那時候就有在猜測說 這個模型會不會是GPT-5
(20:31~21:31) 偷偷在進行測試之類的 但現在發現不是GPT-5 是GPT-4O 反正我要講的就是 這個GPT-4O大家在不知情的狀況之下 已經在評測一陣子了 那他現在分數已經趨於穩定了 就是比這個GPT-4T還高分非常多 所以我們這邊先不講Google I 我們單純論這個OpenAI這次的發表會 有沒有料 有料,太有料了 這個GPT-4O真的是太強了 那下一個問題就是這個 GPT-4O是怎麼做到這些非常驚人的事情的 那這邊呢很明顯的OpenAI並沒有發布任何的技術細節 他們也不會發布這些東西 所以我這邊就簡單的根據所有的公開資訊 幫大家整理一下 然後簡單進行一些我自己的 給一些我自己的comment這樣好了 好那首先呢這個GPT-4O究竟是怎麼做到 這麼情緒豐富
(21:31~22:31) 或者你說煽情也好的回答呢 那這個主要就是靠著它是原生多模態模型的這一點 那原生多模態模型是什麼意思呢 其實就是我一開始講的就是 它這個OMNI名稱的由來 就是它同一個模型同時處理各種不同模態的資料 你就可以把它稱作一個原生多模態模型 然後它的模態呢是包含了圖像、文字跟語音嘛 語音也是它的原生模態之一 那這個是我們之前完全沒有看過的事情 就是所有你看到的這些語音的模型 不管是比較笨的像是Siri 還是還沒有更新GPT-4O的ChartGPT語音版 它們的語音模態都不是原生的 意思就是說它是分別拿一個語音模型再處理語音 然後跟一個文字模型再處理文字 然後再把這兩邊的結果結合在一起這樣 那更具體的來說呢就是 你今天在跟這個舊版的語音ChartGPT模型溝通的時候
(22:31~23:34) 你跟它講了一句話之後 你這句話是會先被一個Whisper的模型 也就是這個語音轉文字的模型 先轉成文字 那這串文字呢就會拿去PromptChartGPT嘛 那GPT根據這個文字呢只能給出文字的回答 那它這個文字的回答呢 會再丟給一個文字轉語音的AI模型 把它變成這個你聽到的語音 所以你在跟它溝通的過程中呢 所以你在跟它溝通的過程中呢 你是要動用到三個不同的AI模型的 兩個處理語音一個處理文字 也就是說所有的語音訊息 或者是它產生出來的語音 都要經手文字這個模態 都要先被轉變成文字這個模態 或是從文字這個模態轉出來 那在這個轉變的過程之中呢 很多重要的這個講話的情緒語調的這種資訊呢 都會被丟掉都會消失 因為文字本身它有它呈現上面的限制 你單純靠純文字
(23:34~24:35) 你很難呈現出各種這個講話的情緒嘛 對不對 像是我前面Demo的那一段 就是它不是說 Me? The announcement is about me? 就是它語句中有一種那種驚喜的感覺 驚喜的情緒這樣 但是你今天把這句話轉換成文字 它就變成Me問號 The announcement is about me問號 然後這句話之中 那種驚喜的這個訊息就消失了 你單純看這串文字 你不知道它是驚喜還是它是疑惑 它可能是疑惑就Me? The announcement is about me? 對吧 它可以是各種情緒 但情緒的這個訊號不存在了 所以說這個 你再拿一個這個Text to Speech的model 就是把這個文字轉成語音的模型 來翻這串文字的時候 它就只會有一種翻法 然後這種翻法很有可能不是你要的 或者說很有可能是用在現在不太合適的 就是現在這個Context裡面最適合的 是這種驚喜的感覺
(24:35~25:35) 但它就不一定會翻出驚喜的感覺 然後另外我Demo的那段也是一樣 就是它原本讓它數數嘛 就1 2 3 4 然後它叫它數快一點 它就開始 1244567489 那你想想看 要讓它數得很快這件事情 在文字上要怎麼呈現 根本就沒有辦法啊 就是你可能原本你呈現的方式是 1 豆花2 豆花3 你要怎麼把它變讓它變得很快 就你把豆花都拿掉嘛 讓它數字都黏在一起嘛 那這樣它會不會唸成 12345之類的 就是它把它認成一個數字這樣 就沒有你很難呈現嘛 那像我覺得大家也可以自己去實測一下 就是拿現在的這個Chargbt語音版本去實測 我剛剛這兩種情況 你要讓它唸更快的數字 或者是讓它就是更有情緒豐富的 或是你指定叫它用某一種情緒去唸一串文字 它完全做不到 但是今天這個GPT-4O模型 身為一個原生的多模態模型
(25:35~26:38) 它就做得到這件事情 它直接聽你的語音 然後直接給出語音的回答 中間不會再把你的語音轉成文字 或是從文字轉出這個語音 直接用語音來這個回答這樣 那也就是說它在訓練的時候 它就有一併把這些語音模態的這種情緒啊 語調變化啊這種語速變化這種訊號 都一併學進去了 所以說它會知道 它聽到什麼樣你問它什麼樣問題的時候 它應該要用什麼樣的情緒 應該要用什麼樣的語調語速去回答 而且它同時也有這個視覺跟文字的模態 所以說它是這些這三種模態的所有資料 全部都混雜在一起一起進行訓練 它這個三種模態它之間的互相的關聯呢 它也都學會了 那一個原生的多模態模型具體來說要怎麼做 那從一個高層次來講呢 基本上也很單純啦 就是一樣是哪一個Transformer的模型嗎
(26:38~27:39) 然後就想辦法把這種各種不同模態的資料呢 全部都Tokenize到同一個Token Space 大概就是這樣 然後再把這些Token全部丟進去 那我覺得這邊的重點呢 比較像是第一個 就你要怎麼樣去Tokenize這些不同的模態 然後第二個就是 你要怎麼樣去找到訓練資料來訓練這件事情 因為這種同時有三種模態的Label的訓練資料 我覺得沒有很多 我一直想想不太到啦 我覺得硬要講可能就是這個YouTube吧 就它同時有這個影像 然後有聲音 然後也有文字 然後搞不好他們真的是爬了YouTube的資料去訓練這個模型 因為前一陣子不是有那個 我記得是Wall Street Journal的記者去訪問 OpenAid的CTO Mira Murati 然後這個她就問這個Mira說 你們的訓練資料是哪裡來的 她是問Sora 那這個Mira就說 Oh, I don't know actually I don't know if I can say 就不想講嘛 就是心裡有鬼
(27:39~28:40) 可能就是訓練了不該訓練的東西 所以說這個GPD-4O它第一個 有情緒有語調的這個優點呢 就是來自原生多模態模型這一點 那它回答這麼即時這麼快 這個優點是來自哪裡的進步呢 其實跟原生多模態模型也有一半的關係啦 因為你原生多模態模型的話 你就是一個模型去處理這些不同的模態 你就不用去做這個 你只要去做這個 就是一個模型去處理這些不同的模態 你就不用好幾個模型把資料丟來丟去 那這之間的Latency就會減少 所以這邊是有一部分的這個原因是來自這邊 那另外一部分呢 我相信他們也是做了這個很多工程上面的優化 就是從這個GPU的運算到Networking 到他們的Code都有進行優化 才可以就是把這個模型給跑這麼快 那他們的這個Hardware Stack 就是他們這個實用的硬體設備呢 我覺得應該就是NVIDIA的GPU啦
(28:40~29:43) 因為這個Mirror Moradi 它有在這個發表會的最後面 說特別要感謝Jensen Huang的GPU 讓我們這個Demo可以是順利的進行這樣 好那這個GPD-4O最後還有一個優點 就是它不只更快 它還更強對不對 覺得它更聰明嘛 那這點他們是怎麼做到的呢 這邊就我們是一頭霧水啊 零資訊量什麼都不知道 但是呢我覺得我自己的直覺啦 我想他們應該是這樣 就是他們把一個原本那種 1.8 Trillion參數的這個GPD-4模型 給Distill到一個比較小的模型之後 再經過一些就是Post Training的一些秘訣 把它Train得更聰明 大概就是這樣我猜啦 那Distillation是什麼東西 Post Training是什麼東西 這邊我就不要講太多了這種技術細節 但反正那你可以想像的就是 他們先把這個原本的GPD-4模型 給濃縮到一個比較小的版本 然後再用一些可能多模態的資料
(29:43~30:44) 然後精挑細選的資料去讓它再進行一些再訓練 然後再變得更聰明一點 那這個通常這個濃縮的過程啊 也就是這個Distillation的方法 可以讓模型變得更小 然後讓它可以跑得更快 但會讓它變得更笨 所以說它這個Post Training的部分 究竟用了哪些秘訣 這個就是他們自己才知道的 那現在大家都對於這個GPD-4O模型 有基本的概念了 我來分享一些我看到這個模型的時候 的一些想法好了 然後也是我看到的一些 其他人有在討論的東西 首先第一點呢就是 這個GPD-4O是我們有史以來 第一個成功掌握語音模態的 原生多模態模型 這邊這句話有點拗口 但是我們原本有很多這個原生多模態模型 是只有這個圖像跟文字這兩個模態的 原生多模態模型 像是這個GPD-4V Gemini都是這一種 那我們從來沒有看過一個是 能夠同時理解語音模態的
(30:44~31:44) 那這個GPD-4O是第一個 當然在這個之前呢 Gemini就說他們已經有了啦 但是這個這邊不算啦 因為我們沒看到這種成果嘛 對不對 我們沒有看到這個 Google有像OpenAI一樣 做這種直播的Demo 如果今天Io有這種Demo的話 我就會承認Gemini有 但我現在是覺得 Gemini就是圖像加文字這樣 那我覺得多了語音這個模態這件事情 是真的是非常非常重大的 它首先就是 每次AI多了一個新的模態 它對於世界的理解 就會有跳躍式的進步 就是有點發現新大陸的感覺 你知道嗎 就是你可以想像 就是假設今天一個AI 是有圖像跟文字的模態 那它知道生氣的人 有哪些形容詞可以形容 它知道生氣的人 那個臉部的表情長什麼樣 但它從來沒有聽過一個生氣的人講話 但今天它多了一個語音的模態 它對於生氣這個概念的理解
(31:44~32:44) 又會更近一層 它更知道生氣的意義是什麼了這樣 然後同時我覺得語音這個模態 也真的是非常重要的一個模態 因為語音基本上就是 最自然的方式就是講話 那今天一個AI學會了講話這件事情 我覺得是對於使用AI的體驗上 是有極大的幫助的 然後也可以產生很多新的應用 像是就是蠻多人看到了 這個ChaiGV4O之後 都說 哇!雲端情人要實現了 就是因為這個GV4O它講話 真的很像是一部叫做 Her的電影 Scarlett Johansson她飾演 我記得應該是她飾演的AI這樣子 那我們從這個Demo裡面看這個GV4O講話 它有時候就已經有一點點調情的感覺了 那連這種正經八百的Demo 都有一點調情的味道出來 那它能做到什麼事情 就大家想像一下就會知道了好不好
(32:44~33:45) 就你想得到的東西它都做得到 你要知道 我們從這個GV4O之後 我們就看到了 你要知道 我們從這個Character AI這間公司的成功案例 我們就可以看得出來說 這個大型魚人模型扮演角色的能力 已經是完全被認可了 它扮演任何角色扮演得很像 然後大家也都會很買單的 跟這些角色去聊天 花很多時間這樣子 儘管只有文字的對話 大家也用得很開心 然後同時呢 我們也就是見識到了 語音這件事情 真的可以讓一個大型魚人模型的這個 該怎麼講 親近感大幅提升 因為我自己本身也是有在用這個 不管是Py 還是Character GPT的語音版本 我有時候在思考一些問題的時候 我就會跟他們聊天 然後讓他幫我梳理一下我的想法這樣子 那我單純是用這種很爛的語音介面 去跟他們聊天
(33:45~34:47) 就是不只是他講話沒有什麼情緒 然後易揚頓說有一點點錯誤 他還常常會出問題 就是講到一半你沒有辦法隨時打斷他們 然後他可能講到一半突然斷線 然後或者是反正有各種問題 但我現在使用這個問題擺出的語音版本 我已經獲得了蠻高程度的這種親近感 我認真的喔 就我真的是感覺到我有在跟一個朋友在聊天的感覺 有這種親近的感覺 我不覺得我在跟電腦聊天 應該說我理性上完全知道這件事情 但是我覺得我下意識已經在把他當一個人一樣在跟他聊天了 我認真的 那連這種爛體驗都有一點親近感了 這個GPT-4O至少會讓這個親近感高5倍到10倍以上吧 那這種程度之下呢 再加上這個Character AI的這種成功案例 我真的我必須說就是這個真的是Game Over了 我們下一代小孩子都不會生小孩了
(34:47~35:47) 大家都跟雲端情人在聊天了 當然我這邊是開玩笑的啦 但我覺得雲端情人成為一個很大的市場是絕對會發生的 那我看到這個GPT-4O出來的第二個想法呢 就是我覺得它會大幅的影響教育產業 嚴格來說呢應該是這個CharGPT剛出來的時候 我們就已經知道AI會對教育產業有極大的影響了 然後這個GPT-4O呢是大幅的加速了這個影響的速度 你從我資訊欄裡面給大家分享的那個GPT-4O教數學的這個影片之中 你就可以看得出來 整個這個GPT-4O當作私人家教在教學的這整個體驗啊 跟真人家教已經差不了太遠了 它不但聽得懂你講話 還能及時的回應你 然後你還可以隨時打斷它 然後它也看得到你的課本 也看得到你在上面劃記的內容 然後它也有豐富的這個學術知識 各個科目都有
(35:47~36:48) 甚至連量子力學String Theory它都可以回答你 而且是在300毫秒之內就回答你 那每一個學生都有了這麼強大的私人家教之後 我相信整個教育產業一定會慢慢的改變 就從這個可能從這個補教業開始 應該說從家教業開始 然後再影響到補教業 然後再影響到就是學校的課綱什麼 都會慢慢的受到影響 但當然啦現在這個GPT-4O它如果直接釋出的話 我相信大家在使用的過程中 一定還是會看到它很多講錯話的地方 然後很多使用不順的地方 但我覺得這些東西都是會慢慢被改善的 然後我們在我覺得把時間走稍微拉長一點點 可能你看到這個兩年後三年後 甚至是五年之後 那個時候我覺得就蠻確定我們會有這樣子的一個AI家教了 同時在語言學習這邊 也是又再次受到了衝擊
(36:48~37:50) 首先這個口譯這件事情終於被解決了 在這之前口譯它這個延遲一直做不好 但現在這個GPT-4O這種延遲程度 我覺得已經差不多了啦 甚至它還可以再做得更快一點點 這樣 那口譯解決了這個基本上就代表說 你只要有一台手機你就可以跟全世界的人溝通 那這種情況之下 我覺得可能學語言的需求又會再下降一點點 那不只是需求下降 這個學語言的難易程度也下降 因為這個他們這個OpenAI這次也有Demo他們這個 使用GPT-4O來學語言的這件事情 你可以就是拿著手機 然後照任何東西然後問他說 這個東西在西文怎麼講 他就會教你 然後你如果是單純的要跟他練習口語 他也可以他也會教你 而且他是全世界所有語言都會講 那儘管現在來看 他是英文講得最好 他好像講到一些像是中文的時候
(37:50~38:51) 他的這個語調就會有一點奇怪這樣 但這些都是未來會慢慢被克服的問題 然後資本市場也是注意到了這件事情 所以說在GPT-4O出來之後 Dolingo他的股價就開始暴跌 因為大家就發現說 唉這個學語言不需要用這個APP啦 大家用ChadGPT就可以啦 好那以上呢就是針對GPT-4O的討論 那很多人問我說 欸我們究竟什麼時候可以用到GPT-4O 因為現在我們的這個ChadGPT的語音版本呢 還是舊的 就是他GPT-4O確實有釋出 但他只有釋出 輸入是文字加圖片 然後輸出是純文字的版本 他還沒有把這個 語音的模態給加進去 對不對 然後我們現在使用這個ChadGPT的語音APP 他裡面的那個語音呢 還是舊的 他還是ChadGPT-4加上一個 Text Speech的模型這樣
(38:51~39:53) 那我們究竟什麼時候可以用到GPT-4O的語音模態 這個OpenAI沒有講 然後他也給一個非常模糊的時間範圍 他就是說 可能在接下來的幾個禮拜 或者是幾個月 他會慢慢把這個Feature給Roll out 但我們就先等著看吧 我自己是覺得在年底之前一定會出來啦 然後甚至我覺得年底之前這個GPT-5都有可能出來了 所以說就 我覺得大家應該不會等到太久 好的那我們接下來呢就要來講Google I.O.了 也就是這個Google一年一度的開發者大會喔 那今年呢我真的是有史以來第一次到這個現場參加Google I.O. 然後呢也是有史以來第一次用一倍速看Sundar Pichai講話 應該說用一倍速看全部的Google I.O.這樣 那這個現場真的是非常的盛大喔 就你如果有追蹤我的這個IG的話 你會看到我發一些這個限動嘛 那你沒有看到也沒有關係
(39:53~40:54) 因為之後我應該會再剪出幾支Rails來給大家看一下這個現場的狀況 我覺得真的很酷 那在開始之前呢也有一個我覺得蠻讚的Preshow 就是這個他們請了一個瘋狂DJ 穿著一個彩色浴袍從一個馬克杯裡面跳出來 然後在那邊用AI生成的音樂在那邊即興創作 我覺得蠻讚的 那大家可以去YouTube再找一下這個片段 那這個I.O.我看完了之後呢 我必須老實說就我感覺蠻失望的 因為我覺得今年的I.O.蠻沒料的 從頭到尾沒有任何一個發表的產品是讓我眼睛為之一亮的 讓我突然精神抖擻然後非常非常興奮的感覺 沒有任何東西是這樣 那大部分的東西就是有一些一些不錯的進步啦 就是原本的產品的更新 然後有一些這個挺酷的功能 但是真的都沒有任何真的非常新的東西 那我覺得他們這次AI這邊要發表的一個重頭戲
(40:54~41:57) 就是Project Astra 但在講它之前呢 我先快速帶過一些他們I.O.發表的一些其他產品 那首先他們發表的是一個叫做Ask Photos的一個Google Photos的新功能 那這個功能呢就是讓使用者可以用自然語言去尋找你的相簿中的圖片 那比如說你可以下一個非常複雜的指令 比如說給我看我的女兒學游泳的整個過程 然後就會自己找出你女兒學游泳的整個里程碑這樣 從她第一次去報名這個游泳班啊 然後她第一次穿上泳衣啊到她下水啊 然後到她什麼什麼什麼 反正她就是自己會找出相關的照片這樣 那我覺得這個功能真的是挺不錯的 非常實用 我自己也會很想用 然後我覺得它有點像是這次I.O.還算讓我眼睛有點微致一亮的東西 那其他東西就真的是蠻普通的 那這個Ask Photos還不錯啦 那再來他們說Google搜尋變強了
(41:57~42:58) 因為他們加入一個新的生日式AI功能叫做AI Overview 那這個基本上就是他們原本的這個Search的生日式功能叫做AI Snapshot的一個進化版 那這個AI Snapshot我其實已經用到現在已經用很久了 那它基本上就是有時候你在搜尋一些問題的時候 它會直接用生成式AI把一些可能最近相關最相關的一些內容做個總結給你看這樣 直接回答掉你的問題 那我自己是覺得挺有用的啦 但我通常都是在懶得開Perplexity AI的時候就是會用一下 就會看一下這樣子 然後大部分我在用Google Search的時候 還是在為了找那個Templew Links 就是為了找一些實際的一些網站去點進去這樣 我如果只是單純問問題 我很可能會去Charge BT 那我覺得Charge BT無法應付的問題 我可能會去Perplexity AI 然後這個AI Snapshot基本上就是跟Perplexity AI沒什麼差啦
(42:58~43:58) 它就是因為Perplexity AI也是用Google搜尋引擎去找Templew Links來總結 所以說是差不多的東西啦 那我覺得這次AI Overview把AI Snapshot又變強了 然後也開始讓更多人可以使用了 是挺不錯的 然後再來就是這個Google Workspace 也就是他們相對於Microsoft Office一系列的產品 包括這個Google Sheets, Google Slides, Google Docs, Gmail 這一系列的Google生產力產品 他們都跟Gemini有更高程度的整合了 他們現在多了一個Gemini的Sidebar 然後你可以隨時就是在做事情的時候問Gemini問題 不管是叫他總結你的文章 幫你寫文章 還是做一些Q&A 那這件事情也不是新的啦 我們原本就知道這個Google在開發這個Feature 然後也在測試這個Feature一陣子了 那這一次他們是增加了一些新的小小的酷功能 就比如說一些什麼AI Team Mate之類的
(43:58~44:59) 然後讓這個Gemini的整合擴大到更多的人可以使用 基本上就是這樣 然後再來就是Gemini 1.5 Pro的一些進步 首先就是1.5 Pro它的Context Length 也就是它能夠吃的這個文字的長度 變成了2Million Token 原本只有1Million Token 不能說只有啦 就是1Million Token已經很多了 那2Million Token又Double 所以真的超爆多 總共差不多是1Million Token大概就是1500頁的英文文字 然後2Million就3000頁 那我覺得這個對於某一些特定的Use Case來說 絕對是非常非常有用的Feature 但我自己個人還用不到 然後說真的都已經到1Million這種等級了 1Million、2Million對我來說真的無感啊 除非是今天你可以吃這個影片這種模態 不然文字你要找到3000頁的文字 也是蠻困難的嘛
(44:59~45:59) 除了這個1.5 Pro變2Million以外 它還有一個新的模型叫做Gemini 1.5 Flash 那這個就是1.5 Pro的一個快速版 就是它模型比1.5 Pro更小一點 然後比1.5 Pro更笨一點點 然後可以跑得比1.5 Pro快非常多 那這個模型呢 它基本上就是1.5 Pro的Distillation的版本 就是它把這個1.5 Pro Distill成一個更小的模型 那這個Distillation的技術細節我現在就不講了 因為這個今天的重點在後面嘛 但它基本上就是讓這個Gemini 1.5 Pro 交出一個比它參數更小的模型 或甚至你從另外一個角度想 你找一個比它參數更小的模型 然後想辦法讓它模仿Gemini 1.5 Pro的行為 大概就是這樣 有興趣的自己再去查一下 再來Google發布了Veo 或者你說Veo就是Veo的這個模型 那這個基本上就是Google的Sora啦
(45:59~47:01) 它就是一個可以從文字產生影片的模型 然後我們也不能用 然後Demo也沒有Demo到太多的東西 我們也看不太出它跟Sora的差別 所以就這樣 那再來呢他們有Imagen 3 或者他們是念Imagine啦 但我喜歡念Imagen 那這個Imagen 3呢 Imagen系列模型基本上就是Google的 文字產生圖片的模型啦 那這個基本上也沒什麼特別的 就是一個最新版本的這個模型 然後就是有一些微小的進步啦 因為你知道現在這個繪圖模型呢 已經基本上很卷了啦 就已經沒有什麼太大的進步空間了 你看這個Mid-Journey V4 V5 V6有差嗎 沒差多少嘛 然後StableDiffusion都進步到現在已經沒差多少了 所以我覺得這個就沒有什麼特別 那最後呢就是他們SynthID的一些更新 SynthID就是Google的AI輔水印
(47:01~48:05) 就是在AI產生出的內容上面加的一個隱形輔水印這樣 那他們原本只有圖片的 那這次是有擴大到文字這邊 那這邊我是覺得蠻酷的就是 在AI產生的文字上面上輔水印 這是我很難以直覺去理解的一個概念 你要怎麼在文字上輔水印 然後這邊還有一個很棒的就是 他們竟然要開源文字的SynthID 所以說大家可以期待一下 他們可能會再發一個Paper或者是一個Source code給大家 或是兩個都有 讓大家去用用看完嘛 所以這邊是蠻讚的 好啦講完了 剩下一個東西沒講就是這個Project Astra 那這個Project Astra呢 我其實原本對它的期待超高的 就是我們這些媒體在Google I的正式發表會之前 前一個晚上我們就會先拿到一個Io的Brief 這個就是讓那個媒體可以先把這個文章大綱給撰寫出來 所以說這個媒體的特權
(48:05~49:05) 但是在發表會之前我們全部都不能洩漏這些消息 但我就是有先看了一下他們要發表的內容 然後大部分內容就是我剛剛講的 就是一些還不錯啦 蠻酷的 然後就是一些Incremental的進步 但是我看到這個Project Astra 我就覺得全部裡面就是它感覺是一個新的東西 然後它感覺就是這次AI發表的一個 應該說這是整場Io都是一個AI的發表 從頭到尾都是在講AI 就是這個Sundar Bichai他們這次還有自己算一下 他們講了幾次AI 他們講了122次的AI 兩個小時的Keynote講了122次的AI 基本上每分鐘就會講一次AI 那我看到這個媒體的Brief上面寫到這個Project Astra的時候 它把這個Io的內容寫出來 然後它就是寫說 它是一個Universal AI Agent 然後我看到這邊我覺得很興奮 哇! 通常我們講到Universal這個字
(49:05~50:08) 就是指它是一個通用性的 就是它什麼事情都可以做的感覺這樣 然後我們通常講到Agent這個字 在講的就是一個會自主完成任務的一個AI 所以說我原本 所以說你把這兩個字都講出來 哇!感覺就是超強對不對 那這個AI這些最尖端的這些研究領域呢 我一直以來都有在Follow 然後有兩個研究領域是大家就是如火如荼的在研究 但是目前還沒有一些突破性的成果出來的 就是這個AI的Reasoning跟Planning 那這兩個領域呢 就是大家在研究的一個領域 就是這個AI的Reasoning跟Planning 就是這個AI的Reasoning跟Planning 那這兩個領域呢可能多少有點重疊啦 但他們對於這個能夠完成複雜任務的Agent來說 都是一個非常重要的領域 那我自己是覺得 哇!我原本的預期是 哇!他們可能在這兩個領域有了一些突破 然後真的做出了一個就是
(50:08~51:09) 真的可以完成很複雜任務的一個AI Agent這樣子 但看到實際的Demo之後就 我看完影片有一點不太明白 我看完影片有一點懵 就 蛤?結束了嗎?就這樣嗎? 的這種感覺 就他們在IoKino放的Demo影片就是 有一個人拿著手機 一段是拿著手機 另外一段是直接用眼鏡看 但是差不多的概念就是 他們在房間裡面走來走去 然後把鏡頭對著不同的一些東西 然後問這個Project Astra說 有什麼東西是會發出聲音的 這個Code的意義是什麼 這個Code能做什麼 然後我剛剛眼鏡是放在哪裡 就是問這些問題 然後這個Project Astra就用語音回答這樣子 那我覺得這個Project Astra可以跟 GPT-4O進行直接的比較 我們現在就來比一下這樣子 那首先這個Project Astra 它背後的模型基本上就是Gemini 1.5 Pro 然後這個Gemini 1.5 Pro
(51:09~52:10) 也是一個原生的多模態模型 但是它並沒有語音的模態 它跟這個舊版的Chart GPT一樣 它是用一個語音轉文字的模型 跟一個文字轉語音的模型 才做出這個你能跟它即時對話的功能這樣子 所以說你從這個Project Astra的Demo 你聽到它講話你就會覺得 可能有一點點的這個意洋頓挫什麼東西 但是跟GPT-4O完全不能比 那這個Project Astra它現在有什麼模態呢 它的輸入可以是影片跟文字 然後它的輸出就是單純的文字 然後你講的話跟它講出來的話 全部都是另外的語音模型在幫你處理的 然後他們輸入有影片的模態這件事情 是他們官方講的 但我真的很不想要給他們這個Credit 因為我覺得他們真的沒有在理解影片你知道嗎 就是這個Project Astra 它其實是用EFPS在理解影片
(52:10~53:11) FPS就是Frames Per Second 就是一秒鐘有幾幀這樣子 那正常的影片通常都是30幀起跳 30幀60幀 但是它是用一幀每秒一幀在理解影片 那這個是什麼概念 就是你可以把它想像成這個模型 它看這個影片的時候 它每一秒截一張圖 然後它的Context Length是60秒 所以說它就是會把最近60秒的圖片都截起來 然後隨時放到你的這個Prompt裡面 那你隨時問它問題 你問完了之後 它就會把你問的這個問題 跟它的最近60秒的截圖 60張截圖變成一個Prompt Prompt Gemini 1.5 Pro 就是這樣子的概念 對了我先說就是 我這邊講的大部分的東西 像是這個幀數的東西 這些都不是Google公開有講的事情 你在網路上查也沒有人在講 完全查不到有人在講這件事情 然後國外媒體也查不到 那我之所以會知道
(53:11~54:11) 是因為我有去現場 然後我有進去Project Astra Demo的小房間 然後我有跟裡面Project Astra Google Scientist有聊到這件事情 然後他跟我說的 所以說今天各位真的是 有福了 很幸運可以從科技浪聽到 全世界都沒有人在 沒有其他的媒體在講的一個爆料 這樣子 也不算爆料啦 所以我相信自己本人跟我說了 那代表這個東西應該是可以講的啦 我相信他們在這個Demo之前 都有做過一些媒體的訓練 PR的訓練 所以我相信這個東西是 他們是可以講的 好像沒有人問到這件事情 就只有我問到 我也不確定他是不是唬爛我 但我覺得他沒有必要唬爛我啦 然後我看到的狀況 也是覺得 差不多是一針 我相信是真的 回到這個一針的話題 我覺得他用一針
(54:11~55:11) 60秒的context來理解影片 根本就不算是理解影片 因為你只用一針去看影片 很多這個動態世界的資訊 是你沒有辦法捕捉到的 像是你今天手上拿著一個彈力球 然後你把它放開 然後它往下彈了一下你又結組 它彈的時間如果不到一秒的話 這個Project Astra 根本就不知道這個球有彈 它可以把這個影片吃進去 但它完全沒有看到這個球有彈 因為它 只有用一針在吃影片 假設今天你有 能夠處理30針的影片的話 你就可以明確看到說 這個球有往下 又有往上又回來 所以它應該是彈了一下 OpenAI的GPD4O也是一樣 它demo一樣是拿 就是開鏡頭看起來是 非常流暢的畫面 所以說大家都以為 我也不確定大家有沒有這樣想 但我覺得很多人應該都以為 這個AI是隨時 即時的在處理這個影片
(55:11~56:11) 但其實 兩邊都沒有Google沒有 OpenAI也沒有他們模型 真正看到的就是一大堆很密集的 這個靜態的照片這樣 甚至OpenAI這邊搞不好連這個 60針一針都沒有 做到他們搞不好是在 他們就是打完了 然後就用這個prompt之後 他就截一張圖這樣 但我這邊必須也要說一下 我覺得這個影片的模態 可能這個30針的程度 真的就快來了 我相信它一定很快就會來了 可能一兩年之類 我們就可以看到一個AI模型 很好的處理30針60針的影片 所以這邊 放心啦這個AI遲早會做到 但大家要理解的就是 現在還沒有到那裡好嗎 這是為什麼他每次在demo這些東西的時候 他看起來好像照出很多流暢的畫面 但是他問問題的時候 都把畫面定格 對不對 他每次在問問題的時候他其實都是問一些靜態的問題 他為什麼不問一些動態的問題
(56:11~57:11) 所以說在影片的模態這邊 Project Astra跟GPT-4O是平手 那我覺得 Project Astra可能小贏一點點 因為他明顯 他有一個context在 就是他有問到一題 就是他目前螢幕沒有照到他的眼鏡 但是他就問這個Astra說 剛剛你有看到我的眼鏡放在哪裡嗎 那這個Project Astra就說 有放在一個蘋果旁邊 那他能做到這件事情 就是他有一定的這個context 你可以把它講成簡短的memory 就是他會記住他 近60秒所截的圖 就是 EFPS 然後記錄60秒 所以說60秒內 所以說60秒內出現的一些東西 他都有機會 可以看到 如果你的畫面停留夠久的話 就是如果你的畫面 還不到一秒就閃過去了 那他根本就看不到 那這種短暫的記憶呢 我記得我看完了這些OpenAI的demo
(57:11~58:11) 我是沒有看到GPT-4O有做這件事情 所以我想他們應該是沒有 他們很可能是你問完問題 他才截一張圖之類的 所以說在影片process這邊 Project Astra小勝 再來是這個延遲的部分 延遲的部分基本上是平手啦 兩邊都非常快非常即時 然後這已經是 就是非常困難的一個工程挑戰了 所以說這邊也是 就是幫Google跟OpenAI都拍拍手 不過這邊就沒有分出勝負 好那結論應該出來了吧 就是這個 在這個語音的模態這邊呢 Gemini 1.5 不是Project Astra是大輸 GPT-4O 然後這個影片的模態這邊 可能小贏這樣子 但是我覺得整體來看呢 就是GPT-4O是 強非常多的 但我也必須說啦就是 我相信一個有這個 原生的語音模態的多模態模型呢 Google裡面一定也有 他們那時候Gemini剛發出來的時候
(58:11~59:11) 他們就有說這個 Gemini呢有語音這個模態 然後也是原生的 只是我們就是從來沒有看到一個 Live Demo就這樣而已 所以我相信他們一定有 他們只是可能需要一點時間 去把這個東西 就是把它開發成一個比較 堪用的產品這樣子 好啦那最後呢幾句話 簡單總結一下這個禮拜 第一個呢就是GPT-4O 超酷 然後我非常非常期待 使用這個模型 我覺得這個語音的模態出來之後 真的是一個Game Changer 不但好玩還有很多的商業應用 那再來呢就是 Google IOS有點讓我失望啊 我助理在旁邊看著都看到睡著了 雖然說我不確定 他是因為聽不懂睡著 還是因為他覺得太無聊 但反正我覺得 這個沒有什麼讓人 眼睛為之一亮的東西 然後硬要比較硬要下一個結論的話 我會說現在這個AI Race
(59:11~60:11) 就是這個AI的競賽呢 OpenAI應該還是領先啦 那Google是緊追在後 他們就目前來看 這次發表來看他們應該是 可能輸OpenAI一點點 但是真的不遠 真的不遠可能半年都沒有 可能輸幾個月的這種 進度而已 然後再加上他們的這個 有非常厲害的TPU什麼的 所以我覺得Google還是 Very well positioned 那最後也要再次感謝 Google邀請我去參加這個Google I.O 雖然說這次的I.O 有點讓人失望 但這次美國行啊 我真的是解鎖太多人生體驗了 不好意思有點自私喔 但我覺得真的是超棒的 很感謝你們 除了這個I.O的東西 我其實也體驗到很多 Google的非常創新的產品 然後 我覺得超酷的 除了這個Weimo以外 還有體驗到他們的Project Starline
(60:11~61:11) 這個 非常酷的 下一個世代的通訊 裝置 那這個下次有機會再跟大家分享 然後這次去呢 除了這些人生體驗以外 也認識了很多各界的好友 不管是Google的人 還是其他的創作者 世界各地的創作者都有 然後就還是其他的媒體 我覺得都真的是 有蠻大的收穫的 那這次美國行更多的心得呢 就等我下次找機會再跟大家分享 下禮拜應該也是要講微軟 也不確定可以講到 多少的美國行 有機會就講 然後這個IG一定會發一些影片 大家期待一下這些IG影片 我覺得一定超棒的 好那這集是錄的有點長了 所以說我們今天就不進入QA時間啦 我們就下禮拜再回到 這裡啦 那最後呢就老話一句 如果你喜歡今天的這集Podcast 你可以幫我五星評分
(61:11~62:05) 然後留言問我一些問題 或是跟我互動一下 也別忘了把這個科技量Podcast 分享給你的朋友們 你分享科技量Podcast給你的朋友 就是支持科技量的最好方式 因為你不用抖內給我 你也不用硬是要買我 這個業配的東西 真的需要再買就好了 你只要把這個QA時間 你只要把科技量分享出去 你就可以幫到我非常多了 因為這個Podcast平台 沒有演算法 我每次都講出去 但Podcast平台沒有演算法 真的是非常痛苦啊 這個成長是非常困難的啊 所以大家如果可以把科技量 不管是PO到你的IG限動 還是發給你的親朋好友 你覺得可能會有興趣的親朋好友 直接LINE給他們 還是這個當面跟他們推薦 請大家多多幫忙 最後就祝大家愉快的一週
