(00:00~01:01) 【開場音樂】 哈囉大家好 歡迎收聽科技浪 我是主持人哈利 科技浪是一個白話 跟你聊科技的Podcast 希望可以用簡單易懂 但是又深入的方式 帶你了解 時下最火的科技話題 歡迎來到科技浪的第五集 這集如你所聽到的 我們終於有了開場音樂 哇我昨天真的是累到爆 我做這個開場音樂 做到凌晨兩點鐘 跟大家分享一下這個故事好了 最一開始呢 其實是我從EP1的時候 第一集的時候 我就有看到有聽眾留言跟我說 欸他覺得有個開場音樂會蠻不錯的 然後我就一直想著說 OK我要做這件事情 但是我一直忘記 然後又挺忙的 所以說我就一直拖到了這一集 應該說一直拖到了昨天晚上 我才想到要做這件事情 然後我一開始的想法就是 OK我們既然是一個講AI的頻道 那我們開場音樂當然是要由AI來做啊 所以我就去用了一下 臉書最近開源的那個模型 叫做MusicGen 那MusicGen這個模型呢 它的用法有兩種
(01:01~02:02) 第一種呢就是你輸入一段文字 就是敘述一下這個音樂是什麼樣子的 它就會產生那段音樂 第二種方法呢 是你先附給它一段參考的音樂 然後再跟它講要把這段音樂改成什麼樣子 它就會把那個音樂改成那個樣子 那這個AI模型是開源的嗎 意思就是說 每個人都可以去他們的GitHub下載這個模型 下載到自己的電腦裡面自己跑 但他們說是開源也不是完全開源啦 我去看一下他們的license 他們license是這樣 就是他們的code是MIT的license MIT的license基本上就是一個 你要幹嘛都可以的license 意思就是說 你要用他們的code去做商業用途都可以 但是他們模型的權重 是一個叫做CCBYNC4.0的license 那這個license基本上是non-commercial license 就是說你不能拿這個模型的權重去做商用 那我剛剛講的code 基本上就是你跑這個模型 或者是你要微調這個模型所使用的程式嘛 這一部分是完全開源的 那權重呢 我上一集就有講過嘛 一個AI模型的權重 就是它所有參數的實際數值 也就代表了這個模型
(02:02~03:04) 它所有的知識 它對世界的了解 所以使用這個模型 或是微調訓練這個模型的這個程式碼 是可工商用的 但是這個模型本身 它的知識是不工商用的 我不知道這樣子的license的狀況 實際翻譯成這個實際作為的時候 哪些行為是可以 哪些行為是不行 我覺得我不是法律專業 我真的不太知道 我只知道有幾件事情是底線啦 就是第一個 你不能把這個模型build到你的APP裡面 你的產品當中 再來就是你可能把這個微型 拿去微調訓練之後 訓練出了一套新的參數 新的權重 然後拿這個新的權重去做商用的話 應該也是會被告的 但是呢 我如果要拿這個模型去產生出一些音樂 然後用那些產生出來的音樂去做商用 這個我就真的不知道 到底會不會被告 那這個也是我現在想做的事情嘛 就是我想要拿這個 產生出來的音樂呢 來做我的podcast開場白 但是我想了一想 我覺得可以賭一把 因為說真的啦 首先 臉書到底要怎麼判斷 我這個音樂是music gem生出來的 第一步
(03:04~04:07) 他連判斷這個音樂是不是AI生成的 都有一點困難了嘛 對吧 那就算他可以知道這是AI生成的 那他又怎麼知道這是哪一種AI生成的 我可能是拿Google的music lm生成的 臉書並沒有說他們的音樂裡面會有浮水印 然後我覺得應該也是沒有啦 所以說他們根本無從判斷這件事情 而且就算退一萬步想 他們真的有能力能夠發現 就是我的音樂是music gem生出來的 他們為什麼會想來告我 他們的incentive在哪裡 他們根本不會想鳥我嘛 我就是一個台灣的小podcaster啊 所以說我就毅然決然的去用了 我的Google collab跑了這個模型 那我一開始輸入的prompt呢 是太空科技風格的這種朗朗上口的節奏 然後他丟給我的東西是這個 喔 很難聽耶 超難聽的 他背景一直有一個很刺耳的音樂 我也不知道那是什麼東西 而且重點是 我要調我也不知道要往哪個方向調嘛 我也不知道我的那個prompt 我要改成什麼樣啊 或是是他的參數要怎麼調 所以說我就覺得我就直接放棄了
(04:07~05:07) 因為我覺得要把一個這樣子的程度的音樂 調到一個可以用的音樂應該是不太可能 所以呢 我最後就去網路上隨便找了一個這個 5版稅的音樂 然後你開頭聽到那個 就是我在5版稅音樂網站找到的一個片段 就這樣 那我覺得 ai生成音樂這一塊 相比那個ai生成圖片 或是ai生成文字 是一個比較不成熟 然後大家通常也比較陌生的一個領域 就是比較少人在研究這一塊 然後同時這個領域 他最強的模型 就像是我剛剛給大家demo那個music gen 他就只能做到這樣而已 我覺得主要會這樣 還是因為就是大家對於文字跟音樂的關聯 沒有像是文字跟圖片 或是文字本身這麼強烈 我們能拿來形容音樂的這個詞彙 真的不夠多 然後也不夠精準 我們通常聽到一段旋律 我們就只能大概講說 哦這可能是哪個類別啊 然後有給人什麼樣的感覺啊 然後都不是很精準 不會像是你形容一張圖片的時候 這是一支袋鼠 它就是一支袋鼠 它是紅色 它就是紅色 不過雖然說這個音樂生成ai這個領域
(05:07~06:08) 是沒有辦法跟這個文字 或者是生成圖片這個領域相比 但他們確實還是有一批人 就是一直有在這個領域不停的做研究啦 那我覺得這個臉書 這個大概是一個月前開源的這個musicgen 應該對他們來說就是一大福音 那所以說這個臉書模型的權重是不工商用的 但那個license本身 它其實就是一個research license 所以說應該會有很多人拿這個模型下去做研究 然後同時他們也有公佈那個fine tune的code fine tune就是微調 這個我在上一集有講過 就是一個拿自己的資料 去把ai模型客製化的一個過程 所以說我們之後就可以等著看看 有沒有一些研究員tune出蠻好的結果 但在那之前呢 我就是持續用我這個網路上隨便找到的500歲音樂 好那我們就閒聊到這邊 不過我剛剛有講到就是這個音樂沒有浮水音這件事情 這件事情其實跟我們今天要講的其中一個topic有一點相關哦 我今天呢最主要是想跟大家聊聊 就是google在一兩個禮拜前呢 他舉辦了一場google cloud next的大會
(06:08~07:10) 這個google cloud next的大會呢 基本上就是google cloud 也就是谷歌的雲端平台它的一個發表大會 所有它雲端最新的產品最新的發展 都會在這個大會公佈 但他們同時也會公佈一些跟雲端沒有最直接關聯的一些產品 像是今年的這個google cloud next呢 我其實覺得他們應該改名成google ai next 因為他們幾乎整個keynote全部都在講ai 那大部分的發表呢 我覺得沒有說特別讓我驚艷 但是都還挺不錯蠻有料的 那我今天想特別聊聊的是其中兩個主題 第一個是他們要把google workspace生產力工具的ai助手 do it ai開放讓所有企業付費使用 那我講的這些生產力工具呢 當然就是大家平時常用的這個gmail啊 google sheet啊 google slides google把這些所有工具包起來同一稱作google workspace 那這個google workspace有一個所謂的do it ai 這是一個在每個生產力工具都會出現的ai助手 然後google現在開放讓所有的企業都可以付費 讓他們的員工在使用這些生產力工具的時候
(07:10~08:12) 可以使用這個do it ai ai助手 那第二個我特別想聊聊的就是 google它開發出了一個圖像的隱形浮水印叫做synth id 之後可以在所有ai生成的圖像中加入這個synth id 大家就知道這個東西是ai生成的 那當然除了這兩個topic以外 他們還有公佈很多其他東西嘛 就是有包括這個他們的企業的ai平台叫做vertex ai 他們加了一些新的模型啊 加了一些新的搜索功能啊 然後他們的infrastructure有跟這個NVIDIA談了一個partnership 會使用一些NVIDIA的gpu啊 或是NVIDIA的這個djx系統 這邊的話有興趣的大家可以自己去搜索看看 它keynote裡面還有邀請黃仁勳來一起講 那我主要挑了這個do it ai跟浮水印來聊聊 主要是因為首先do it ai 也就是他們google workspace的ai助手 它的影響是很大的 就是它會影響到所有在使用google workspace的企業 那相對於google workspace 那個vertex ai在使用vertex ai的企業就沒有很多嘛 那我會想聊浮水印的主要是因為
(08:12~09:14) 這算是很多人一直敲碗的一個話題 就是我常常收到很多dm是在跟我講說 就是問我說哈利要怎麼樣分辨這個 這個文字或是這個圖片是否是ai生成的 那這個話題呢 就是ai的分辨真真假假假假真真 這是大家在討論ai能造成的問題當中的時候 最常被提到的一個問題 所以我就想說趁今天跟大家好好聊聊這個話題 好那我們就先從do it ai開始聊聊吧 好那這個do it ai的英文拼法是duet ai 它是所有google產品當中ai助手的簡稱 就是不只是我剛剛講的這個google workspace的產品 就是那個什麼gmail啊slides sheets google cloud產品裡面的所有ai的integration 它也是把它稱作do it ai 但他們這次發布也就是這次開放讓所有企業可以購買的 是只有google workspace的do it ai google cloud的do it ai則是有少部分的企業在做測試 還沒有開放讓所有的企業購買
(09:14~10:17) 所以我們今天的討論主要就是在討論這個google workspace的do it ai 那這個workspace的do it ai他們的定價是每人每月30美金 換算台幣就是900多塊 意思就是900多塊一個月 你可以在你用這些所有的google workspace產品當中 得到許多ai助手的幫助 好那在我詳細評論它的每個功能它的價值 然後跟慶平做比較之前 我先直接跟大家講我的結論 我的結論就是現在這個do it ai我不會想買 不會想買的原因很簡單 就是它提供的價值不及我每個月要付出的成本 也就是30塊美元 好那我們首先現在聊聊這個workspace的do it ai 它究竟帶來什麼價值呢 這邊要看的當然就是這個do it ai 它現在有什麼features嘛 它現在有什麼功能 它在每一個生產力工具裡面它可以做到什麼事情 這邊有一個我覺得算是google還蠻賊的地方 就是它現在開放讓所有企業購買的 這一套google workspace的do it ai 它並不是所有的功能都可以使用 而且他們賊的地方是 他們其實最炫的那些功能全部都沒有開放
(10:17~11:20) 就是他們在google ILO在這個google cloud next 還有他們的部落格裡面 他們demo出來的那些最酷的那些功能 其實全部都是還沒開放的功能 然後他們在他們的presentation裡面 他們其實也沒有刻意去講 就是他們從來都沒有刻意講到說 現在的do it ai有哪些功能是available 哪些功能是還在develop 就是你仔細去聽它那個google cloud next的keynote 它就是並沒有明確講說 ok the following are the features that are under development 它是說let's see what the future holds for do it ai workspace 然後在那之後呢 它就開始demo了這些很新很炫的功能 但全部都是現在不能用 翻成中文的意思就是說 它並沒有明確的講說 接下來我要demo的這些功能都是我們還在開發的喔 它的講法是 我們來看看 do it ai的未來長什麼樣子 那我覺得這個就是很大眾很容易忽略的一點 就是很多人可能聽一聽就是在做筆記的當下 就是沒有聽到這些關鍵字 然後就以為它現在在demo的這些東西 全部都是現在do it ai有的功能 然後確實我有看到啊
(11:20~12:20) 就是那個國外的媒體一個很有名的科技媒體叫做diverge 它的報導裡面就有寫錯 它就有寫到那些google demo的功能 但是現在還不能用的功能 那do it ai workspace這些已經現在已經可以用的功能呢 跟那些還在開發的功能大致可以分成兩類 已經可以使用的這些功能呢 是我們所謂的prompt-based interaction 那還在開發的這些功能呢 是我們所謂的context-based interaction 那麼兩者的差別是這樣 prompt-based interaction在講的就是 你跟這個ai講你要的需求 然後這個ai就單純從你要的需求裡面 生成你要的內容給你 就比如說你今天假設你是一個鞋子公司 然後你就跟這個ai說 請幫我寫一個adidas跟nike的球鞋的比較 然後這個ai就單純從他對於這個adidas跟nike球鞋的認知 寫一篇部落格文給你 那今天如果是一個contextual-based interaction的話 這個ai不只是會從你給他的指令 還會從你自己所有的資料裡面 找到所有相關的資料來 來結合他自己的知識一併給出答案
(12:20~13:21) 比如說你可以直接跟ai說 幫我做一個PPT 這PPT呢 前十頁是在講說 我們自家球鞋跟adidas球鞋的比較 後十頁是在講說 我們自家球鞋現在的銷售狀況 然後這個ai就會去抓 所有我們公司裡面現在跟這個球鞋有相關的資料 然後他可能有找到一份這個 documents是有在講這個球鞋的specs 他的規格啊 他的材質啊 然後又有找到一份excel是這個球鞋的銷售狀況 這個財務報表 然後他會把這兩個東西結合他自己對於adidas球鞋的認知 全部包在一起變成一個你要的PPT給你 這兩個的差別很明顯嘛 contextual base真的是強太多了 有用太多了 但是好死不死的 現在開放的所有功能全部都是prompt based contextual base的都還沒有開放 然後prompt based的功能其實真的就 就沒什麼好demo的吧 就跟你隨便開一個chat gb的視窗 然後問他問題 最後再把結果貼回來是一樣的道理啊 所以說這也是為什麼 Google都沒有demo這些prompt based的功能 但其實現在你花30塊美金
(13:21~14:22) 你買的就是這些功能而已 這部分我如果有講錯 大家可以再更正我 但是我覺得應該不會錯 因為我這邊我是看Google 他官方的support document 他其中有一個頁面就是在講說 目前獨立AI for Workspace Enterprise 可以用的所有功能 他全部都列出來 然後全部都就是就是這個prompt based的 好那這些雖然說這些prompt based的功能呢 沒有到非常的exciting 但我們還是簡單的過一下 首先呢在Google Doc 也就是Google版本的word當中呢 獨立AI可以做到什麼事情呢 你要不要猜一下 對你猜對了就是生成文字嘛 你可以在Google Doc裡面打說 幫我寫XXX 比如說幫我寫一個求寫比較的部落格文章 然後獨立AI就會把這個文章寫出來 然後你自己再修改一下 就這樣 基本上就是跟你開一個chat gpt的視窗 然後複製貼上差不多 有差的當然就是背後的模型嘛 對吧 獨立AI背後使用的模型 我猜應該就是 一定幾乎一定是prompt 2沒錯啦 然後我猜可能就是一個bison這樣子的大小
(14:22~15:22) 就是那個prompt 2 他那時候公佈的時候 有公佈很多不同的大小嘛 我猜他們是bison 就是一個第二大的這個prompt 2模型 那你如果是用chat gpt的話 你就是用gpt 3.5或者是gpt 4嘛 我不知道你怎麼想啦 反正如果我要寫文章的話 我一定是叫gpt 4幫我寫啊 我才不信什麼prompt 2 然後再來那個在gmail當中一樣 就是幫我寫一個什麼信 然後他就幫我寫 他就幫你寫出那封信這樣 然後在Google slide呢 就是Google的PPT當中呢 你可以說幫我visualize 就是幫我視覺化什麼什麼東西 他這邊講的其實就是生成圖片啦 就是幫我產生一張某某某圖片 然後他就幫你產生這張圖片 然後在Google sheet當中呢 就是Google的那個excel 你可以跟他講說 幫我產生一個什麼什麼什麼的模板 然後他就會幫你產生一個模板出來 這邊講的模板啊 就是比如說你叫他產生一個 你的一週行事曆好了 然後他就會幫你做那個橫軸 就是週一週二週三週四 然後縱軸就是7點8點9點10點 好那在Google meet
(15:22~16:24) 就是Google的這個視訊軟體呢 也有一些新的功能 不過這邊我們等一下講 我們先評論一下 剛剛我們講的Google doc Gmail slides跟sheets 首先我就直接說了 我覺得這些功能對我的幫助極小 首先Google doc的部分 我剛已經講了嘛 我寧願用Chai gp幫我寫 然後Gmail的部分呢 我寧願自己寫 我本來寫email就是一個非常簡潔的人 所以說我覺得AI幫我寫 應該沒有辦法幫助我省什麼時間啦 他寫完我可能還要大幅的修改 再來Google slides 他可以產生圖片給你 首先我覺得這個簡報會用的圖片 可能有兩種需求 一種是那種像是i stock的那種圖片 就是那種單純要表達某種概念的圖片 就比如說你想要一個在穿著正裝 在打電腦的一個人之類的 那如果是這種需求呢 我覺得真的要產生圖片的話 我也不一定會使用Google的這個 Google slides圖片產生器 因為說真的 他在Google slides裡面立刻產生 跟我在用一個其他的網站 比如說Mid Journey才生一張圖片 然後貼過來兩個Effort Level
(16:24~17:25) 根本兩個程序是沒有什麼時間的差別的 所以說比較的重點就是背後的模型 那Google可以用的模型 我覺得不可能會比Mid Journey好 或者是現在最強的StableDiffusion XL 那個Mid Journey還要付錢 StableDiffusion XL是完全免費的 那這是第一種圖片的要求 那第二種圖片的需求 其實是客製化的圖片 就是你的產品的圖片 那這件事Google slides的獨立AI 就不是做不好的問題 他是根本就做不到嘛 要做這件事 我一定會使用這個StableDiffusion XL 拿你的產品 或者是你們家的模特兒的圖片下去做微調 然後用它來產生 好那我這邊簡單解釋一下StableDiffusion 我怕有人不知道 StableDiffusion是一件叫做Stability AI 製作出來的一個完全開源的生成圖像模型 你可以輸入文字 然後這個AI就會根據你輸入的文字產生一張圖片 然後我剛剛講到的StableDiffusion XL 是這個StableDiffusion現在的最新版模型 簡稱是SDXL 那這個SDXL我自己有微調過
(17:25~18:26) 我是去蒐集了那個BlackPink Jennie的照片 我拿大概20張吧 然後下去微調這個模型 調出來的SDXL 它產生 Jennie的圖片跟真正的 Jennie一模一樣 你完全認不出來 而且整個微調過程 我是用Google Collab GPU好像是用A100吧 花大概一個半小時就調完了 然後之後產生一張圖片 只要花大概 應該不到一分鐘 然後我是用TESLA T4的GPU 你如果對這些GPU的名詞有些陌生 你可以去聽我上一集 我講了很詳細的GPU的介紹 Anyways 我想講的就是 這個Slice的圖片產生功能 真的是有跟沒有一樣 對我來說 最後這個Sheet的功能 我覺得也是有跟沒有一樣 但這可能是我自己本身比較會用Excel 就是我操作上我是蠻熟悉蠻快的 然後同時我如果是在做分析的話 我也會想要很好的掌握每一個分析的細節 所以說比起他直接給我這個Headers 我會自己想要寫自己的Headers出來 好那以上是這個 Google Doc Gmail Slice Sheets的部分 再來我說他在 他在Google Meet也有一些新的功能
(18:26~19:27) Google Meet的新功能主要有三個 第一個是產生背景圖片 第二個是調高影像品質 第三個是即時翻譯字幕 那我覺得第三個蠻讚的 前兩個蠻還好的 首先產生背景圖片 這個我的想法就跟前面那幾個產生圖片的功能差不多了 再來調高影像品質 我覺得這個還挺不錯的 但說真的我覺得可能沒有到那麼實用 第一個就是你在用Google Meet開會的時候 你真的會在意你的Client多高清嗎 你真的會在意你的同事的臉多高清嗎 我覺得還好 再來就是現在大部分的Webcam也都有 大概最少也有720 然後通常都是有1080P的畫質 所以說我覺得也不會想要調高到哪個程度 1080P已經非常非常高清很夠看 不過第三個即時的翻譯字幕 我覺得倒是不錯 這邊有一些什麼你在跟國外的Client開會的時候 它有一些很重的那個腔調你聽不懂 你可以有一個即時翻譯的字幕 就是你甚至不用看英文 它直接翻譯成中文給你看 這樣就還蠻不錯的
(19:27~20:27) 所以Google Meet這邊的 我覺得算是挺讚 但是真的有那麼大價值嗎 我覺得也還好啦 那以上呢我講的這些所有東西 就是現在所有開放的Feature囉 以上這些所有是30塊美金一個月喔各位 所以我才會說現在這個讀A.I我不會想買嘛 但是它如果加了這些它現在還在Develop的功能 我覺得30塊應該就可以買 好那我們現在來大概講一些這些功能好了 首先在Gmail這邊呢有個功能叫做彙整整個Email串 就你可能跟一個Client在討論一個Project 來來回回寄了二三十封信 這時候你可以叫讀A.I直接Summary整個Email串 或者是直接問他你想知道的問題 比如說我們第一次Agree的金額是多少之類的 我覺得這個Feature簡直是天使啊 太讚了 這真的是立刻就會有Impact 而且可以省下很多時間 而且市面上沒有什麼競品可以做到類似的功能的 但我覺得他們現在不釋出這個功能 我覺得也是情有可原啦 因為這個功能它好用是好用沒錯 同時它的風險也很大
(20:27~21:29) 就是你跟你的客戶在討論Project 這個Deal可能幾百萬幾千萬 然後你這個你萬一問這個讀A.I 然後他搞錯那個數字不就糗大了嗎 他們應該我猜啦就是因為這個原因 所以說還在還不開放這個功能 再來很讚的一個功能呢 就是我一開始在講那個Context-based Interaction的時候給的例子嘛 那個他們也有Demo就是 你直接跟你講 你直接跟獨立A.I講你最後要產生的PPT是什麼東西 他就直接幫你把所有相關資料收集起來 然後幫你產生那個PPT 然後最後呢是在Google Meet的一些新的功能 它有兩個是有Demo的 一個是智慧讀稿 就是你在報告的時候看那個稿子的時候 它並不是死的文字 而是像那種KTV的那種會一直一直一直移動焦點的 同時它也會一直去聽你現在講到哪裡 然後隨時做它的調整 比如說你如果跳了幾行 它就它就會發現這個 然後它那個字幕就會直接跳跳到那邊這樣 還有另外一個Meet的功能呢 是即時的Meeting Summary 就是假設你比較晚加入這個Meeting 你加入了之後
(21:29~22:30) 你可以直接在旁邊的獨立A.I問他說 欸我們現在的Meeting Summary是什麼 他們講了哪些東西 或者是你直接問他Specific的問題 比如說針對我們現在的這個Marketing Project 前面的那幾位同事有給出哪些comment這樣 我覺得這個也算是一個有impact的一個功能啦 所以希望獨立A.I可以及早開放這些功能 那我覺得它如果這些功能一開放了 然後金額仍然是維持在30塊美金一個人的話 應該會變得蠻多公司加入的 那說到這個30塊美金啊 我不知道大家會不會好奇 就是Google的利潤到底有多少 就是這30塊美金Google可以賺到多少錢 它的成本究竟多高 我也挺好奇這一點的 因為畢竟它設30美金這個價格真的是挺貴的嘛 所以我這邊簡單估算了一下這個 他們每一個人使用獨立A.I一個月 他們的營運成本大概是多少 那當然這個是極度極度簡化的一個估算 然後我們也主要只估這個變動成本 不估這個固定成本 所以說各位這個非常厲害的分析師呢 請不要嘴我好不好
(22:30~23:31) 這個估算絕對不準啦 但是就是給大家一個概念這樣 好那我們知道他每個月每個人 就是收30塊美金嘛 那每個月每個人他的成本是多少呢 那我的算法呢 就是每次這個人使用獨立A.I Google的成本 乘上這個人 每個月他會使用多少次獨立A.I 那每次他使用獨立A.I這個Google會產生的成本呢 我覺得我們可以用這個Google賣這個Palm 2 API的價格來估算 因為我猜獨立A.I背後的模型就是Palm 2嘛 應該應該就是Bison 那Google呢 他現在自己在那個Vertex AI賣那個Bison API的價格呢 是2000個字0.001美元 這邊的2000個字 包括你問他問題的字數 以及他回覆你問題的字數 那2000是我自己抓的數字啊 就是我覺得可能大概就是 平均每次問這個獨立A.I問題呢 可能加起來就差不多2000個字吧 你如果是問簡單的問題 絕對低於2000個字 但你如果是問一個比較 就是叫他產生一個文章 應該高於2000個字 所以說平均可能就 我猜是2000個字啊
(23:31~24:32) 那每一個人每個月會使用幾次呢 我的估算方法是 每個小時使用三次 這樣我覺得已經算是蠻頻繁了哦 就是可能20分鐘你就用一次 20分鐘就用一次這樣 所以說呢 3乘以8 因為一個一天工作8小時嘛 再乘以20 因為一個月工作20天 等於480 所以說一個人一個月會使用 480次的獨立AI 那這個480再乘以0.001 就是平均每一次他用這個2000字的context 等於0.48美元 0.48美元的成本 然後他賣你一個月30美元 而且你要知道這個0.48一定是高估 就是因為我們在算那個 獨立AI的使用價格的時候 我們是用API的價格在算 但他的成本一定不到他的價格啊 那我雖然不知道他的margin是多少 但就通常這種SAS的margin 都是可能大概70%左右吧 所以說可能2000字的context 他的成本並不是0.001美元 而是0.0003美元哦 那當然啦 就是我說就是有很多成本 我沒有算進去嘛
(24:32~25:32) 就是可能包括他 因為要提供這個服務 而要多買的GPU 要多請的人啊 那些成本 或者是就是他在 我剛剛講的是 比較偏是這個生成式AI的部分啦 那我剛剛講那個Google Meet 他有那種提高畫質的服務嘛 那那個就是另外一種AI 那種我也沒有算到 不過但我覺得這零零總總 這個cost全部加起來 這個一定還是賺到翻啦 所以說我覺得Google訂這個 30美金的價格 並不是因為他有30美金的價值 而是他可以訂30美金 為什麼可以呢 因為人家隔壁那個Microsoft Copilot 就訂30美金啊 你若不知道的話 微軟的那個Office 365啊 就是包含那個PVT啊 Excel啊 Word啊之類的 他們也有提供一個 類似獨立AI的服務 叫做Microsoft 365 Copilot 那這個Copilot提供的服務呢 基本上就是跟 Google Workspace的獨立AI是差不多的 然後他的定價他是先開的 他是開30美金一個月 然後Google的這套產品 也差不多的功能啊
(25:32~26:33) 那他們當然也可以開30美金一個月啊 只是我覺得這樣其實不太公平啊 因為微軟要做這個365 Copilot 比起Google要做這個獨立AI 更難一點 最主要原因就是 他們沒有自己的LM 也沒有自己的硬體設備啊 他們都要蹭別人的啊 但是Google兩個都有 沒有自己的LM 那個LM 我在講的就是Large Language Model 大型語言模型 像是什麼GPD 3啊 Google的POM 2 微軟沒有自己的大型語言模型啊 他們一開始呢 那個做了那個Bing 就是他們Bing的那個瀏覽器 有一個類似Chat GPT 或是Google Bard的那個聊天機器人 他們那時候是蹭OpenAI的GPD 3嘛 對吧 那這個365 Copilot呢 我覺得他們應該也是蹭OpenAI的GPD系列 然後那個時候臉書在發布LAMA 2的時候 LAMA 2就是一個 現在最新的開源世界 最強的大型語言模型 那時候在發布的時候 微軟也去蹭了一波啊 就是LAMA 2就是臉書的模型 但他們在發布的時候 有跟微軟簽一個partnership 就是他們可以在微軟的客戶呢 可以在他們的雲端平台
(26:33~27:34) 叫做Azure上面 使用LAMA 2 那這個 這個就 我自己覺得這個deal 基本上有跟沒有一樣 這邊可能有些法律 或是他們Partnership有些細節 我可能不太了解啊 但就我現在所知 我覺得這個deal根本就是在蹭而已啊 我覺得它是沒有什麼實質意義的一個deal啊 就是Google這次Cloud Next也有講 他們也有把LAMA 2 放到他們的Vertex AI平台上面 他們也沒有跟臉書簽什麼特別的deal啊 還是有我不知道 反而我就覺得 就是 就是在蹭 然後再來微軟也沒有自己的硬體設備 意思就是說他們有自己的AI晶片 Google的話 它有它的TPU嘛 Tensor Processing Unit 這部分 你如果想要多了解的話 你可以去聽我的EP4 我有詳細的解說 雖然是有消息流出說 微軟他們有在自己做一個叫做 Athena的AI晶片 但目前還沒有看到什麼 很確切的這個成果出來 好那最近在 企業端AI生產力工具這個市場呢 其實還有一些其他的新聞 比如說OpenAI推出了Chat GPT的企業版 這個就是讓一間公司一個企業
(27:34~28:34) 可以直接幫他自己所有的員工 都買這個Chat GPT Plus 而且不能說是Chat GPT Plus 應該說是Chat GPT Plus Plus 因為他們是有很快的GPT-4 很長context的GPT-4 就是你可以輸入很多數字 有高達三萬兩千個token的GPT-4 然後當然還有 我覺得最讚最有價值的 Code interpreter可以用 我覺得雖然說不能這樣比較 但我覺得Code interpreter這個功能 直接屌打獨立AI現在的所有功能 而且是你個人單買Chat GPT Plus 可以使用Code interpreter Chat GPT Plus一個月是20美金 光是一個人的就已經比 獨立AI的一個人便宜了 我猜Chat GPT Enterprise 可能一個人的價格 可能會在20美金之下 所以說哇 那真的是 如果我是一間公司 我絕對先買Chat GPT Enterprise 不會先買獨立AI 然後除了這幾間巨頭之外 這個產業還有一些大家可以注意的 就是有一些其他的獨立的生產力工具 他們也在瘋狂的追逐AI這一波趨勢
(28:34~29:36) 像是Zoom 它正好也在上禮拜發布了 他們的一個新的AI功能 叫做AI Companion 然後這個像其他的 像什麼Salesforce、Box 他們也有發布自己的AI solution 反正這邊的take away就是 遲早你工作上用到的所有工具 不管是這些生產力工具 還是你coding的東西 全部全部都會有AI助手 好 那我們讀AI的話題 我們就先講到這邊 再來 我說我們第二個要講的話題就是 Google的Image Watermark 對吧 Google的圖像浮水印 首先這個東西是什麼呢 Google DeepMind 就是Google的AI部門 說他們開發出了一個叫做 Synth ID的圖像浮水印 浮水印是什麼東西呢 就是你有時候用一些剪片軟體 然後你沒有付錢的 你可能產出來的影片 它的右下角就有一個 Made by什麼什麼軟體之類的 那個東西就是浮水印 但是Google的Synth ID 圖像浮水印可沒有那麼智障 它不是說它加 它就是把在每一張AI產生的圖片的 右下角加一個 Made by AI 沒有那麼智障 它是一個完全隱形的浮水印
(29:36~30:38) 而且不會改變圖片的任何特徵 就是你加了這個浮水印之後 你這張圖片完全不會有任何的改變 這是因為它是用一個 非常聰明的演算法 把這個浮水印加在你的像素當中 然後你這個圖片不管怎麼變 你不管怎麼切 不管怎麼轉 不管怎麼改造 這個浮水印都還是會存在 而且是可以被偵測出來的 那詳細來說 他們究竟是怎麼做到這件事情 他們全部都沒有講 但有些人可能聽到這裡 還是沒有很有概念 所以我簡單解釋一下 首先你一張圖片 是有很多很多的像素組成的 你常常看到一張圖片 它的解析度 它是說什麼 幾百乘以幾百嘛對不對 比如說200乘以400好了 它那個在講的就是那個像素 所以說200乘以400 就是等於8萬嘛 所以說那張圖片有8萬個pixel 8萬個像素 意思就是說 那張圖片是有8萬個像素組成的 那像素是什麼呢 像素基本上就是一個色塊 就是一個有顏色的一個方格 就這樣 每一個像素都只能有一種顏色 那我們知道世界上每一種顏色 都是由三個原色可以挑出來的嘛 就是RGB嘛
(30:38~31:39) 那個紅綠藍 然後這三個原色呢 紅綠藍 各有256種深淺可以選擇 所以說每一個像素 可以用三個數字來代表 每一個數字就是代表 三原色的其中一個顏色 它的深淺程度 比如說一個顏色 可能是234 108 72之類的 然後說真的 你一張圖片幾萬幾十萬個像素 你其中一個像素的 其中的一個顏色 它的數值增加了1 從72變73 你整張圖片看得出來嗎 當然看不出來啊 我覺得郭德迪麥在做的事情 應該就是用一個很聰明的方法 在這個這張圖片的所有像素上 埋一個數學的特徵進去 可能就是做一些很細微的調整 讓你完全看不出來 但是機器可以看得出來 那目前這個SynthID的功能呢 是必然的 就是它Google沒有公佈它怎麼做的細節 然後也沒有開放這個功能 給大家自由使用 你只有使用這個Google的 官方的服務才可以 而且只有一批人可以進行測試這樣 好那首先呢
(31:39~32:40) 我必須先給Google鼓掌一下 就是他們做的這個SynthID 已經比之前所有大家的嘗試好非常多了 我不知道是我沒看到還是怎樣 但我之前有看到的大部分 都是在做MetaData的embed 意思就是說 每一張照片其實都有一個所謂的MetaData 這個MetaData就是關於這張照片的資料 比如說這張照片是從 哪一支手機什麼型號 什麼時間拍出來的 你iPhone裡面你拍的每一張照片 全部都背後都有這個MetaData 然後這張MetaData是 附在這個照片的檔案當中的 所以說你這張照片 不管傳到哪裡傳給誰 它這個MetaData都會存留在那邊 那之前的做法呢 就比較像是我們在這個MetaData裡面寫說 這張圖是AI生成的唷 但是方法超級無敵爛 因為MetaData隨便就可以被洗掉 然後隨便就可以被篡改 那這Google的SynthID已經好非常多了 因為你沒有辦法篡改它 據他們所說 就是你很難去把它去除掉 你圖片怎麼修它也是會存在 但我覺得身為一個很機車的自媒體 我還是可以提出兩點批評
(32:40~33:41) 第一點就是SynthID 我如果真的要破解它的話 其實超級簡單 我會怎麼做 首先我去收集100萬張或是1000萬張的圖片 隨便圖片任何圖片都可以 然後我把這些圖片全部丟給Google的服務 叫它幫我上浮水印 然後上完之後 我是不是有2000萬張圖片 1000萬是原本的圖片 另外1000萬是有上了浮水印的圖片 然後我把這2000萬張圖片 全部丟到一個神經網路裡面 直接硬串一波 意思就是硬是叫它找出這個 浮水印的規律到底是什麼 讓它學會就是 你看到一張有浮水印的圖片 你就是要產生一張看起來一模一樣 但是沒有浮水印的圖片 我們都知道神經網路什麼東西都學得起來 你資料量夠大就沒問題了 那我剛講1000萬可能有點太誇張了 應該是不需要這麼多 就可以找出那個浮水印 除非那個浮水印的規律真的是千變萬化 再來第二個硬是可以批評的就是 這點我其實不確定 但是我覺得這個浮水印 可能會影響這張圖片 在未來要用AI進行修改的時候 修改出來的成果 這部分比較技術一點
(33:41~34:41) 但是在我們Machine Learning的圈子當中 尤其是在電腦視覺這一塊 有一個詞語叫做Adversarial Attack 就是你拿一張熊貓的照片 給你這個影像辨識的AI看 然後那個AI成功的辨識出 這是一張熊貓 那你在拿同樣這張照片 但你加一個完全沒有意義的Noise進去 對於一般人來說 我們肉眼根本看不到那個Noise 就是在一些我剛剛講的那個 像素那個層次的一些數字上的微調這樣而已 但這時候電腦在看 它可能就會把它認成一個一支手機之類的 就是變成完全完全不相干的東西 那這個Sense ID呢 會不會影響這件事情 我覺得是需要討論的 好那這個辨識AI產物這個話題呢 其實還有超級無敵多可以討論的 就是不只在圖片 文字這邊這是一個很大很棘手的一個問題 但今天我覺得我第一個話題有點講得太久了 所以說我們就留到下一次 再給大家一個完整的Overview好了 畢竟我們最後還是要留一點時間給Q&A嘛 那我們現在就進入Q&A吧
(34:41~35:43) 首先不得不休說感謝哈利 以為自己對ML沒興趣修學 離開實驗室後因為IG推了哈利給我 才重新開始接收相關資訊 很感謝哈利讓我不用自己讀論文還能接收薪資 好不客氣也謝謝你追蹤我的IG 再來MM1000MM說 非常棒的科技Podcast 但是我有疑問想請教 我想請問要學習AI的使用是要學習什麼 針對上班族來說 工作經歷十幾年 有試用過ChadGPD跟Bing 但是還不知道要怎麼跟現在的工作結合 建議怎麼跟上時代的腳步呢 首先我覺得你可以跟你的老闆建議說 要買Duet AI Workspace 哈哈 好了沒有我是開玩笑的 首先我不知道你的工作是做什麼 所以說我也很難給你很具體的建議 但我覺得要慢慢把這個AI 融入你的工作流當中 不外乎就是從兩件事情開始 第一件就是了解AI到底能做到什麼 第二件就是思考哪裡可以使用AI 首先先去了解說 現在到底有哪些AI工具 每個AI工具它最多可以做到哪些事情 哪些事情它做得好 哪些事情它做得不好 在它做得不好的事情
(35:43~36:44) 有哪些外加的工具可以幫它做得更好一點 你知道了這些所有東西之後 你之後在工作的時候 你也要不停的問自己 我現在做的這件事情 可不可以讓AI做 我覺得你這樣不停的去學習新知 然後不停的思考學習新知思考 時間久了你一定會慢慢的發現 你的工作流當中多了越來越多AI的環節 因為任何人的工作中 百分之百都有可以用到AI的地方 那你想要多學習 就是現在有哪些AI然後它們可以做到什麼 最好的當然就是追蹤科技浪Podcast啦 再來A濤久說 感恩哈利 讚嘆哈利 機器學習苦手救星 這不是一個讚可以解決的 從內容的紮實程度就感覺哈利花了很多心思 製作每一集的節目 聽Podcast也幾年了 哈利是我唯一同一集聽兩遍的 肝惑滿滿 也希望哈利能持續創作 通勤時看到哈利有單集 已經變成小確幸了 應該很多重度Podcast使用者 跟我有一樣的想法 不說了 我要再去聽一次第四集 哇 謝謝你 說真的 第四集我真的是花滿多時間在準備的 因為儘管那些東西我可能原本大部分都知道了
(36:44~37:44) 但我還是要花很多時間去再次做Fact Check 因為我怕從我的印象講會講錯話 然後也花很多時間思考要怎麼樣編排這個內容 聽起來才比較順 然後大家也比較好吸收這樣 每一集Podcast其實都是我血跟汗堆出來的 講這些東西真的很累啦 但是看到有人喜歡 有人覺得有幫助 有產生價值 我覺得就一切都值得了 再來Barblur說 讚 我覺得你需要一個防噴麥 我那時候有看到這則留言 然後我這次有買一個防噴麥 應該說我買一個防噴的一個罩子 對 所以希望這集的結果有比較好一點點 但我最後必須還是要說 儘管我們科技讓起步蠻快的 就是我們很快的就上榜了 但是我們還是一個非常早期的節目 就是這集才第五集 所以我覺得一定有非常非常多地方 是我可以再進步 可以再做得更好的 然後每一集都會嘗試調整 那要知道要調什麼東西 最主要就是看大家的回覆 所以說大家有任何建議 都可以在底下留言 再來MeNu728說
(37:44~38:45) 專有名詞請求補充解釋 EP1跟EP2超讚的 真的是大家都能聽懂的敘述方式 已經很完整的有專有名詞的補充解說 重複聽了好幾次 EP3專有名詞比較多一些 雖然能大略猜一下是什麼意思 累積起來後面就比較難理解了 真的好想好想聽懂 謝謝哈利分享這些科技內容給大家 了解了 先跟你說聲抱歉 就是確實我覺得我EP3 可能專有名詞沒有解釋得非常好啦 我其實一直以來都有非常刻意的 在解釋我講的所有專有名詞這件事情上面 但我有時候還是會不小心漏掉幾個 就是我在看那個EP3的後台分析數據的時候 我發現蠻多人在我講第一個故事 第一個新聞的時候跳掉了 然後回去看的時候就發現 我可能API這個概念沒有講得非常清楚 我覺得這個就是我可能會比較忽略的部分 就是你如果是一個軟體工程師你就知道 API這個字根本就是用到爛了嘛 你平時每天都在用API 然後每天你甚至自己在建API 就跟對於一些行銷人來說 轉換率這個詞是一樣的概念
(38:45~39:47) 就是大家每天每天在用 你就不會想到說 欸其實有人不知道這個字是什麼意思 那我已經盡量的嘲弄就是 所有人都聽得懂的方式去解釋這些所有的名詞了 但有時候第一個我還是會忘記 第二個我還是必須得抓一個平衡嘛 我不可能每一個詞都要講到解釋到底啊 因為每個詞背後還有很多很多的概念 然後每個概念我如果都要解釋的話 這一整集根本就聽不下去了 畢竟我的聽眾裡面還是有很多非常高知識分子 研究AI研究得很深入的人 比如說PHD研究生啊 或者是一些非常資深的這個機器學習工程師 這樣子他們就不會想聽嘛 因為我就沒有辦法深入到很深入的地方 所以我還是必須得抓個平衡啦 那所以我覺得這邊就是 我會盡量的去做到最好 把這個平衡點抓到最棒 那你如果有聽到一個詞是你聽不懂的 我建議你是立刻暫停 然後去ChatGPD問一下這個詞是什麼意思 了解了你再繼續回來聽 因為我跟你保證你不會需要查很多次啦 我大部分的詞都有很好的介紹 所以說你應該不會查很多次
(39:47~40:48) 然後你可能每一集就多自己多查幾個詞這樣 那我覺得每一集你就可以聽懂了 而且這樣你應該也可以學到比較多的東西啊 最後呢 Rock and Fall他說 謝謝帶領耳朵進入艱深的專業科技海 聲音很溫柔有高辨識度口條很好 科技跟英文根底一聽就是超強 第一次留言就為了支持你的好節目 聽一集就知道要追蹤到底 加油感謝你有潛入深剖析AI 哇謝謝你 第一次聽到有人稱讚我的英文耶 好那我在做這幾次前呢 其實我思考了很多 因為說真的這幾個禮拜有一些AI新聞 是我覺得也算是蠻酷蠻重要的 但是我都沒有講到 那我在思考的就是 我究竟是要講很多則新聞 但是每一則都淺淺的講一下 還是我應該講少少的幾則 但是每一則都非常深入剖析 那我最後的選擇呢是後者 像我這集目前就是這樣嘛 我就是講了這個Google Cloud Next的獨立AI 跟它的浮水印這樣 那我覺得主要兩個原因 第一個原因就是我覺得 這樣子比較符合我幫大家提供最好的造型筆 這樣子的一個初衷
(40:48~41:48) 就你如果去看我的IG 我IG的那個自我介紹那邊 我就打了一行就是 帶給你AI時代最好的造型筆 造型筆的英文是Signal to Noise Ratio 每一則新聞都可以是一個信號 也可以是一個噪音 那你如果每一週都一定要cover好幾個新聞 你一定會有一些週次是cover了一大堆噪音 我不是很想這樣 尤其在現在AI時代噪音真的多 雖然說沒有那個Crypto那個時候那麼多啦 應該說Crypto那時候應該幾乎9x9都是噪音 再來就是我覺得 如果可以深入剖析一兩個故事 大家比較能夠真正的吸收這些知識 你可能每個禮拜就砰砰砰一大堆五六個故事 可能聽一聽就沒有一個可以記得住的 但如果每個禮拜我們都花半個小時 40分鐘去聊一兩件事情 我覺得這一兩件事情應該每個人都可以一直記住 好啦不過大家有什麼想法 也都可以在留言區跟我說 然後我現在還有規劃我未來要開始做一些訪談的節目 所以說你如果覺得你想要我邀請誰的話 你也可以在留言區說哦
(41:48~41:49) 那這集就先講到這邊啦
