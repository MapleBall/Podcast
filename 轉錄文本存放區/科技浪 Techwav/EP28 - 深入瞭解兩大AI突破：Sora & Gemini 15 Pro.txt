(00:00~01:00) 【音樂】 哈囉大家好,歡迎收聽科技浪,我是主持人哈利 科技浪是一個白話跟你聊科技的Podcast 希望可以用簡單易懂,但是又深入的方式 帶你了解時下最火的科技話題 本集節目由NordVPN贊助播出 那今天一樣,我們在介紹NordVPN之前呢 我先稍微介紹一下VPN究竟是什麼 以免你上一集業配沒有聽到 VPN呢就是虛擬私人網路的意思 它最主要的功能呢我覺得可以分成兩項 一項是資安的保護,一項是突破地理限制 那在資安這一塊呢 你平時在一些公共場合使用他們的Wi-Fi的時候 其實是有一定程度的風險的 你透過這個網路傳出去的所有訊息呢 其實是沒有加密的 尤其是這個網站沒有使用HTTPS協議的時候 所以說呢,駭客可以連到跟你同一個網路 然後用一個叫做Packet Sniffing的方式 就是截取你傳出去的資訊
(01:00~02:03) 像是你在登入一個帳號的時候 它就可以截取到你輸入的密碼是什麼 那假設你今天是有使用VPN的話 然後你再連這個公共Wi-Fi 你的電腦要傳任何東西出去的時候 你電腦裡面安裝的這個VPN軟體 會先把這些東西全部都加密 然後這個加密後的資訊啊 是會先被傳到VPN的Server 你選擇的Server 不管是你選擇美國的還是台灣的之類的 你傳到這個Server之後 你的資訊才會被解密 然後被傳去應該要傳去的地方 所以說呢,儘管駭客跟你使用同一個公共Wi-Fi 儘管它還是有可能攔截到你的訊息 但它攔截到的都是一堆加密過後的亂碼 這個就是VPN對於資安的保護 那剛剛還有講到另外一個優點 就是VPN可以突破地理限制嘛 那這個就是比較直接一點 因為這些VPN提供商啊 就是在世界各地都會有他們自己的伺服器 所以你可以選擇你想要的那個伺服器 然後變成從那邊上網 那在眾多的這些VPN提供商之中呢
(02:03~03:04) 我覺得今天的贊助商NordVPN確實是最好的VPN 因為對我來說呢 我看VPN就是三大重點 信任、速度、跟可連線的地區 那對於NordVPN來說 它當然是我可以信得過的 因為它是一個全世界最大的品牌之一嘛 那在速度這邊呢 NordVPN確實也是全世界最快的VPN 有一個獨立的IT安全機構叫做AVTest 它進行了一個測試 然後它發現NordVPN是全世界最快的 而且還比第二名的VPN快了將近一倍 然後最後NordVPN可以選擇的連線地區呢 也是非常的多 在全世界60個國家有超過5000個伺服器 所以你如果也想要有更好的資安 或是你想要突破地理的限制 不管是要看別國的劇 還是要打別國遊戲的這個伺服器 你可以考慮一下今天的贊助商NordVPN 那現在NordVPN有週年優惠 你使用我本集資訊欄裡面的專屬連結 或者是在結帳的時候輸入我的優惠碼TechWave 你就可以得到週年優惠
(03:04~04:06) 就是購買兩年方案呢 可以加贈四個月 然後三十天內呢 可以無條件的退費 不滿意隨時取消 那這個優惠連結跟優惠碼 在資訊欄都找得到 但我這邊也再念一下 優惠連結是 nordvpn.com slash techwave TechWave是T-E-C-H-W-A-V-E 全部都是小寫 那優惠碼呢就是直接TechWave T-E-C-H-W-A-V-E 本集業配就到這邊 謝謝NordVPN的贊助 好那首先呢先跟大家說一下就是 下個禮拜不一定會有科技狼Podcast 原則上是沒有啦 但是假設有很多 真的是我很想講的話題的話 我還是會上片 我還是會上Podcast這樣 那主要是因為就是 我們科技狼呢 其實從半年前開始做 一直到現在每一個禮拜都有上 然後這樣做節目呢 其實雖然說我做得很開心 然後我也學到很多東西 大家也學到很多東西 但是呢就是 其實還是很累啦 就是有一些話題呢 我可能一個禮拜要花
(04:06~05:07) 可能20個小時以上在準備 所以說我有在想說 就是正好下禮拜還沒有接到業配 還是我們就關掉 休息一週這樣 但是還不確定啊 就是假設真的有很多 我想講的話題 我還是會再錄一期節目 但是原則上就是取消這樣 那假設下禮拜真的沒有科技狼的話 大家也可以用這個機會 去聽一聽之前的集數 我知道很多人是可能中途加入的嘛 然後之前很多集數都還沒有聽完 你可以去趁這個機會去補聽一下 好那這禮拜呢 我要講什麼話題 我相信你過去這幾天 如果有用手機 如果有上社群媒體的話 你應該都猜得到 我要講什麼話題 就是OpenAI的Sora嘛 但除此之外呢 還有另外一個非常大的AI突破是 Google的Gemini 1.5 這兩個很大的AI突破 都是在台灣時間上禮拜四的晚上出來的 那晚上我通常不太會看新聞啊 都是我自己去學習啊 或者是寫code的時間 所以我是禮拜五的早上起來 才看到這兩個新聞
(05:07~06:07) 然後我一開始看到Sora的影片的時候 我真的是我直接叫出來 我直接叫出來 What the B 這樣子 因為我追蹤這個 我們說text video的AI領域 就是從文字轉成影片 自動生成影片的這種AI模型的這個領域 我已經追蹤很久了 然後我一直以來有在跟大家 我沒有很常講這個領域啦 但我偶爾都會提到嘛 就是說這個領域它的發展很快啊什麼的 像是你看一些AI模型 像是Runway 或者是Picard Labs 或是臉書的E-Move Video 他們產出的那些影片呢 他們的進步真的是非常的快了 然後也產生出越來越好的影片了 但是我看到這個Sora的時候 這個Sora跟這些其他的這些text video的模型 完全是不一樣等級的 就是從各個面向來看 都差太多了 都強太多了這個Sora 然後這個Google Gemini 1.5這個這個新聞呢 比較沒有像Sora一樣
(06:07~07:08) 一眼就可以看得出這個東西不一般啦 但是我也是在讀了他們的這個technical paper 跟他們的blog文章之後 越讀我也是覺得這個東西越厲害 因為我一開始看到這個它的標題呢 我是抱持一點懷疑的 然後這個我的懷疑之心呢 在讀的時候慢慢的被解除 然後我就覺得哇這個東西也是真的是太厲害了 大突破 那我覺得AI這個領域呢 過去一兩年發展真的是超級快 每個人都有感受到嘛 但是像這樣子的非常重量級的大突破 兩個同時在同一天出來 我覺得也是不常見啦 所以說這禮拜呢真的是非常有趣的一個禮拜 然後我從禮拜五到現在禮拜日 這幾天呢也全部都在看這兩件事情 所以今天呢我就來跟大家好好的聊聊這兩件事情 那我會先講這個Gemini 1.5 然後再講Sora 你如果想先聽Sora的話 你也可以在本集資訊欄裡面找到那個時間軸 直接快轉過去就好了
(07:08~08:10) 那兩件事情呢我都希望可以幫大家深入的分析 這個東西到底是什麼 它怎麼運作的 然後它可能可以帶來什麼樣的後果這樣 好那我們先從這個Gemini的新聞開始好了 Gemini的新聞是這樣 就是Google他們宣布他們train出了一個新的模型 叫做Gemini 1.5 Pro 如果你還不知道的話 Gemini系列模型呢是Google現在最強的大型多模態模型 就是它可以吃文字、影像、影片、音訊作為它的輸入 然後它可以輸出文字跟圖片 那這個Gemini 1.5 Pro呢 它跟上一代的模型比起來 它有兩個進步特別特別多的地方 第一個是它有比上一代長非常多的context length context length在講的就是這種大型圓模型 或是大型多模態模型 它一次能夠處理的這個輸入資料的長度 你的context length有多長 你一次能夠處理的這個資料就有多多 雖然說嚴格來說不能這樣說
(08:10~09:10) 因為你的output也算在你的context length裡面 意思就是說 假設你的context length是3萬2千個token好了 那這個就代表說你的輸入資料 跟你輸出的這個結果總共加起來不能超過3萬2千 那我們通常都會講 就是把它context length跟輸入的長度劃上等號 是因為通常輸出的東西是相對短非常多的 因為輸出的東西是人類要看的 人類能夠消化的 那這當然不會很長 尤其是你可能問一個非常非黑即白的問題 它的回答可能就是一句話而已之類的 那這些context length的計算 全部都是以token作為單位 那個這個模型本身也是以token作為單位做處理這樣 那token是什麼我們等一下再講 我們先講這個Gemini 1.5 Pro它究竟進步了多少 好那跟上一代的模型比呢
(09:10~10:11) 上一代的模型最高的context length只有到3萬2千 就是Gemini 1.5 講錯Gemini 1.0的Pro 1.0的Ultra 他們最多只能吃到3萬2千的token 那Gemini 1.5 Pro它可以吃到最高1000萬個token 從3萬2千直接進步到1000萬 這個就算你不懂機器學習好了 你聽到你也大概會知道這是一個非常大的躍進嘛 對吧這真的是很大很大的進步 然後這件事情在整個領域對於整個這個LM的領域來說 也是一個大突破 因為在這之前呢 我們最長context length的模型就是Claw 2 就是Anthropic這間公司的模型 然後它可以吃20萬個context length 20萬個token的context length 那是原本最強的 但是20萬跟1000萬也是完全不能比的 所以這也是為什麼我一開始看到他們的這個發表的標題 我並沒有感到像Sora的那種的興奮感
(10:11~11:14) 而是我感到非常的懷疑 因為我覺得這種進步真的是太大 大到我沒辦法一時間理解這樣 然後我也很懷疑 因為很多人他們說他們的context length很長 其實都是拿他們的這個quality在做trade off 意思就是說沒錯你可以塞很大一坨的資料進去這個模型裡面沒錯 但是呢你這一大坨資料 它其實很大一部分 它這個模型其實沒有讀得很熟 意思就是說你通常我們發現是中間的部分 就是你可能問他這個資料中間部分的一些細節問題 他可能就答不太出來 他可能只有頭尾的問題會比較熟而已 然後也有些人會用一些我覺得很賊的方式去假裝這個模型有很長的context length 就是比如說你有一坨非常長的文章嘛 應該說這個這個等級已經不是文章了 可能是好幾本書一整套哈利波特這樣 你先把一整套哈利波特先拆成很多個小塊
(11:14~12:16) 然後把這些小塊先個別放進一個大型音樂模型裡面 叫他summarize 先把他總結成就是更精簡的一些小塊 然後把這些更精簡的小塊組起來丟進你的模型裡面 所以我那時候一看到這個Gemini的標題 我心裡就在想OK你們是在玩什麼trick 但是我在讀他們的technical report的時候呢 越讀我越覺得哇這真的是一個大突破這樣 為什麼呢 為什麼呢 因為我剛剛說前面那兩種我覺得有點小作弊的方式 一個是你可以確實可以輸入這麼多的資料進去 但他其實沒有辦法好好的吸收這個資料 然後另外一種呢是你根本用一支莫名其妙的方式去把這個資料簡化 這些方式啊其實都禁不起一個叫做needle in haystack的測試 這個測試我之前應該有跟大家介紹過 他在講的就是一個haystack就是一個草堆嘛 那種刺刺的草堆 然後一個needle就是一根針嘛 你在這個草堆裡面藏一根針是不是很難找到
(12:16~13:16) 那同樣的概念用在這些大型圓模型上面就是 你輸入的文章超級無敵長嘛 這就是你的草堆你的haystack 然後你在這個超級長的文章可能五六就是五六本書合併起來這麼長的文章裡面 藏在中間藏一句話 然後可能是一個很random的話 就是比如說the magic number is 42之類的 莫名其妙出現了這句話這樣 然後你接下來你再問這個LM說what is the magic number 如果LM這個時候回答42 那就代表他有找到這句話嘛 他有理解到這一段 也就是說他通過了這次的考驗這樣 然後呢一個完整的needle in haystack的測試呢 你是會在每一種不同的文章長度 然後每一個文章他不同的段落全部都測試過這個needle in haystack 都埋過一次看看 你才可以做出一個最完整的結果 就是比如說我們從10萬個token的文章開始 我們在最前面插這句話
(13:16~14:16) 然後看會不會找到 然後再往裡面再插一點 就是插深一點的地方 然後再深一點 然後再深一點一直到最後面 然後好接下來我們再來試試看120萬個token 一樣插插插插 然後每一種長度每一種深度全部都試過之後 你會畫出一個成功與失敗的這個分佈圖 然後就會看出說 這個模型啊 他handle到什麼樣子的context的時候 他在哪一個區塊比較容易找不到這個needle這樣 那對我來說呢 一個最嚴謹的測試就是 你直接把這個圖給畫出來 我如果看到這個圖的結果 全部都是一片綠的 就是他不管在哪一個長度 哪一個地方都找得到這個needle的話 我才承認這個模型是真的有非常長的context length 那很多這種聲稱很長context length的這些模型 在把這個圖畫出來之後呢 他就原型畢露了嘛 像是這個Claw2的模型
(14:16~15:16) 他畫這個圖出來就發現 哇到了這個context length開始變長的時候 就直接開始一片紅了 都找不到那個needle了 但是這個Gemini 1.5 Pro很誇張 他在100萬個token以內 他找到這個needle的準確率是99.7% 然後就算你一直增加那些token 增加到1000萬個token 就是他們說的這個他們最高的上限 他的這個準確率也幾乎是維持完美的狀態 那他能夠通過這個測試就代表說 他是認真有1000萬個token的context length 那這1000萬個token究竟有多長 從文字來看大概是700萬個字 然後從聲音來看 大概是100多小時的這個聲音音檔 那是影片的話大概是接近10小時的影片 所以說今天這個Gemini 1.5 Pro呢 他可以吃700萬個字 或者是100多小時或是十幾個小時的影片進去
(15:16~16:19) 然後你輸入資料的模態是可以混雜著使用的 意思就是說你輸入的資料 你不一定是要全部都是文字 或全部都是音訊或全部都是影片 你可以影片夾雜文字這樣 就比如說他們這邊有demo就是說 他們把一個黑白電影時長大概40幾分鐘 丟到這個Gemini 1.5 Pro裡面 然後同時也問他一個問題 就問他說某個人不是從另外一個人的口袋裡面拿出一個紙條嗎 這個紙條上面寫的是什麼 然後這件事情是在哪一分哪一秒發生的 然後Gemini的回答就是 他把這個紙條上面所有的訊息全部都寫出來 然後跟你說這件事情發生在12分01秒的時候 然後他們有去check是完全正確的 這真的很誇張喔 一個40幾分鐘的電影Gemini每一針都了解的清清楚楚 然後你可以用文字來問他每一針就是發生了什麼事情這樣 然後你也可以搭配圖像問問題 就是你的問題可以是
(16:19~17:23) 你的問題可以是你先畫一個非常簡單的畫面的塗鴉 就有點像是你小畫家用滑鼠畫出來的那種感覺 然後問他說這個事情這個畫面是出現在哪一分哪一秒 然後他就會回答你說是出現在15分27秒的時候 然後他們去check確實是這樣 那Google究竟是怎麼樣把這個Gemini的context length拉得這麼長的呢 這邊我們當然是完全不知道 你們大家去看他們的technical report就知道了 真的是講來講去全部都在講一些高層的觀念 完全沒有深入任何的細節 所有看的人基本上都是像偵探一樣 在這邊找一點細節那邊找一點細節 然後慢慢推敲去猜說他們大概做了什麼這樣 但大家都還是很摸不著頭緒 但是我們確定的事情有幾個 第一個就是Gemini 1.5 Pro它還是使用Transformer的架構 這個Transformer的架構最為人詬病的一點就是它context length不好scale 為什麼不好scale呢
(17:23~18:23) 因為它有一個非常重要的機制叫做注意力機制 這個注意力機制有點算是Transformer架構它的精華 就像是2017年推出Transformer架構的這個論文 它的標題就是Attention is all you need 就是你只需要這個注意力就好了 這個注意力機制在做的事情就是 它的最終目標是要讓模型可以了解 你的文章中的每一個字跟其他所有字之間的關聯是什麼 它的做法就是文章中的每一個字 它都會跟文章中的所有字 包括它自己算一個Attention score 意思就是說假設你今天的文章是十個字好了 然後假設這十個字就是十個token這樣 當然也不可能這麼小啦 我們簡單假設一下 十個字的context你要算一百個Attention score 為什麼因為十乘以十嘛 每個字都要跟其他的字全部算一個Attention score 假設你今天的文章變成了一百個字呢
(18:23~19:23) 也就是說你的文章長度你的context length變成了十倍 這個時候你要算一萬個Attention score 你有發現了嗎 就是你的context length變成了十倍 但是你要算的Attention score的數量變成了一百倍 所以很明顯的隨著你的context length變得越來越長 你的運算複雜度上升的速度是比context length上升的速度快非常多的 我們在軟體工程領域你就是會說運算複雜度是大歐的n平方這樣 所以說這就是一直以來大家詬病Transformer架構的一個地方 就是它真的是表現非常好 然後它最大的優點是它可以平行運算 它可以做大量的平行運算 就是我剛剛講的Attention score的計算 其實在實務上是完全平行進行的 所有的Attention score都是在同一時間被算出來的 他們就是用一些矩陣乘法的方式 這邊就比較數學 但想研究的可以去看一下
(19:23~20:24) 其實也不難 就是QKV的matrix在乘來乘去 但是你如果要持續的增加你的context length 你的運算量真的是增加的太快了 所以說有些人就是在研究很多Transformer alternative 就是我們可能用一些什麼State Space Model什麼的 這邊我跟計劃性也有聊到 大家可以去聽一下 但重點呢重點是這個 就是Gemini 1.5 Pro它還是一個Transformer的架構 所以說這很有可能是他們發現了一些注意力機制很大的突破 他們有辦法可以去降低這種複雜度 我也不知道他們是怎麼做的 雖然說目前已經有一些方式是在慢慢的改善這個問題了 但他們可能是把這個問題改善了非常非常的多 這邊他們究竟怎麼做到 我們也無從得知 但我們就希望這個開人社群呢 哪一天會有類似的論文出現
(20:24~21:25) 我是有聽到有些人說有一篇叫做Ring Attention的東西 我沒有去細看那篇論文啦 我這兩天也沒有時間 但是可能就是一些key value cache的一些trick啦 那這邊就我還沒看 有看的可以跟我分享這樣 好那我看網路上很多人在講的就是 大家都在想就是Google有了這個1000萬的context length之後 世界會變成怎麼樣 很多人在講的呢就是rag要死了 那你如果是科技量的聽眾 長期的聽眾你應該很熟悉這個rag的技術啊 那rag這個技術他在做的基本上就是 當你要輸入的這個文本實在太長的時候 塞不進你的context length的時候 你就會先根據使用者的問的問題 先去你的文本裡面找出最相關的這個片段 然後直接用這個最相關的片段進行回答這樣 那今天我們有了這個1000萬的context length
(21:25~22:29) 應該是沒有任何的文章是塞不進這個1000萬的context length裡面 應該說大部分你平時會用得到的這種前後文 應該都是在這個1000萬以內嘛 那這邊很多人就在說哇那rag is dead 好多公司在研究在開發的這個rag的技術呢 在一瞬間呢全部都變成沒有用了 因為這你再也不會需要rag了嘛 如果一個llm可以處理你所有的資料你幹嘛要rag 但我自己是覺得啦距離我們完全不需要rag還要非常久 短期內我們會看到的是rag跟這種非常長context length的模型的一個混合運用啊 這是我覺得 因為在很多使用場景下這種超長context length的llm 還是沒有辦法直接取代rag加llm的 那這邊最重大最重要的這個關鍵呢就是速度 我們從jabinite的那個demo你就會看到 雖然說他可以一次吃670萬個token 也就是整部這種黑白電影這樣
(22:29~23:32) 但是他產生他回答的時候花了一分鐘以上或是接近一分鐘這樣 但是rag幾乎是瞬間的 他就是算一個很簡單的一些數學這種cosine similarity之類的 很快的抓出相關的這個文案文本這樣 然後就直接把這個非常短的文本丟進你的llm裡面 讓他立刻去回答這樣 所以假設你今天在處理一些公司的事情 然後會需要很快的去搜尋你公司的資料庫然後問一些問題 你會想要等一分鐘然後才得到你的回答嗎 應該不會吧 然後我覺得大部分的這些問題啊其實都是這種就是 你會想要快速的得到你的回答 然後你有可能可以願意去犧牲這個回答一丁點的這種準確率的 那當然一定也有一些應用式 你同時需要非常非常長的context length 然後你同時也需要非常非常精準的回答 這個時候呢你可能就是不會用rag這樣 但是我覺得這樣子的場景啊我目前直接想我其實還是想不太到 所以我是覺得rag遲早有一天會死掉
(23:32~24:32) 就連這個llm index創辦人他自己也有說rag is a hack 就是他只是一個暫時性的一個小技巧這樣 但是我覺得我們內課還沒來好不好 我們還要再等一下目前這個rag還是沒有辦法被長context的llm取代的 好那我一開始有說嘛就是這個Gemini 1.5 Pro 它其實有兩個進步非常多的地方 第一個就是這個長context length嘛 好像不小心講的有點太久了 但這確實也是他們最大的突破 因為它第二個優點第二個進步的地方 是它的運算效率的提升 那這邊我是覺得沒有那麼大的突破啦 所以這邊我就快快帶過了 反正他們這邊就是說他們在整個training跟infrared stack 就是從它的基礎設施到它的資料到它的最優化到它的系統 都有非常多的這些微小的改變這樣 然後這些所有的改變呢就造就了這個Gemini 1.5 Pro呢
(24:32~25:34) 它的運算需求是比Gemini Ultra 1.0 就是上一代的最強的模型是小非常多的 但是它的表現竟然是跟Gemini Ultra 1.0是差不多的 所以說就是有了很多這個運算資源運算效率的進步這樣 那其中最明顯的呢就是他們採用了一個叫做MOE的architecture 這個MOE的architecture它的全名是Mixer of Experts的architecture 那這個基本上就是最近machine learning界非常紅的一種機器學習的做法 那基本上就是相較於你用一個模型然後很大參數很多 你是用很多很多的小模型組成一個就是一個Mixer of Experts 就是一些專家的組合這樣 那你在使用這個模型的時候呢 這個Mixer of Experts它會自己決定說目前這個token 我要用哪兩個experts來處理這樣 那假設你今天有八個experts好了
(25:34~26:37) 那今天每一個token平均你只用到兩個experts來處理 那這兩個experts它加起來的參數量絕對會比你一個很大的模型少非常多嘛 那這個時候呢你就省下很多的算力這樣 那這個真的是最近machine learning非常紅的一個做法啦 因為有前一陣子這個Mix Troll作為這個開路先鋒嘛 那這部分我可能下次再詳細的跟大家介紹 因為我迫不及待的要開始講這個Sora了 那最後做個很快的結論啦 就是我覺得Google今天這個Gemini 1.5 Pro是很大的一個突破 因為他們在Transformer這種大歐的N平方的架構上面 實現了1000萬的context length 然後這個想當然絕對是業界最高標準 然後他們的1000萬的context length呢也是實實在在的context length 因為他們有經過最嚴謹的測試然後都是有通過的 那這個突破應該還是不會立刻幹掉rag
(26:37~27:40) 然後我也一時之間也想不到什麼樣子的需求是會需要這麼長的context length 但是我相信大家未來應該會慢慢的發掘出越來越多的應用場景 這絕對是一個蠻好的突破這樣 好那我們現在把這個Google的新聞講完了呢 接下來我們終於可以來講我期待已久的Sora了 那這個Sora呢我覺得它真的是紅到圈外了 真的是像那時候ChatGPT一樣 每一個人都在講講machine learning的人當然都會講這個Sora 然後講一般的tech一般3C的人像是什麼Jomans之類的也來講Sora 然後一些其他什麼講社群媒體啊講生產力啊講自我提升啊什麼的 全部的人都在講Sora 所以說真的是大家都想蹭這波Sora的流量 那也害得我就是我已經算是台灣時間很早上片了 但是這個影片流量也沒有很好 因為就是被大家分走了嘛 那不是重點啦 重點是這個Sora模型呢它真的是太驚人了 驚人到它可以紅成這樣
(27:40~28:40) 我相信絕大多數的人現在一定都有看過這個Sora的影片了 但是如果你還沒看過的話你到本集資訊欄的最下面我有放這個OpenAI Sora的連結 你一定要點進去看一下你才會知道這個東西究竟有多扯 好那這個Sora呢它是OpenAI最新的Text-to-Video模型 Text-to-Video就是以text也就是文字作為輸入 然後輸出呢是Video也就是一個影片的模型 那原本在這個領域稱霸的一些AI模型呢 我一開始有講嘛就是那幾間 Picard Labs RunwayML StableDiffusion的那個StableVideo 然後還有臉書的EmoVideo然後還有一些Google的研究之類的 反正很多這種Text-to-Video的AI模型 但是這些所有AI模型他們的表現跟Sora都差了很大一截 這個Sora產生的影片真的是太過真實了 然後原本的這些Text-to-VideoAI模型他們的長度也有限制 就是他們通常都只能產生可能4秒左右的影片
(28:40~29:42) 但是Sora可以一次產生1分鐘的影片 然後解析度也是差非常多 Sora產生的影片都是非常高清的 然後這個Sora除了它可以透過文字來產生一支影片以外 它還可以有其他的用法 比如說你可以給它一張圖片 然後它會把這張圖片變成影片 讓圖片裡面的東西給動起來這樣 或者是你可以把影片變長 不論是往前變長還是往後變長都可以 所以說你還可以產生一種那些完美回圈的影片 這個Sora就跟OpenAI最近釋出的所有模型一樣 它是一個幣源模型 意思就是說它不會釋出這個模型本身 然後也不會公佈這個模型的技術細節 所有要用的人都只能透過OpenAI的產品去使用這個模型 沒有辦法下載到自己的電腦或者是自己的伺服器上面 通常看到這種很厲害的幣源模型的Demo 我都會抱持一點懷疑之心 我們沒有辦法去驗證它
(29:42~30:44) 我們沒有辦法自己去跑然後去看這個結果 這些結果都是他們跟我們說的 然後他們有沒有可能做Cherry Pick 意思就是說它可能產生幾千幾萬個影片 然後從裡面挑出幾個最好的影片這樣 然後我們看到的都是他們精挑細選過的最好的結果 那我相信OpenAI絕對有幫Sora的這個Demo做一些Cherry Pick 絕對是挑他們產生出來的這個裡面他們覺得最酷的結果給大家看 但是我覺得他們Cherry Pick的程度一定沒有很大 頂多可能產生個五個十個例子 然後從裡面挑一個他覺得最好看的 因為在X上面呢 OpenAI的CEOSam Almond有在X上面跟大家說 你們想讓Sora產生什麼樣的圖片 跟我說我直接產生給你看 然後很多人就在下面回覆 包括MrBeast也在下面回覆 然後這個Sam Almond就會回覆一個影片這樣 那你有去看這些產生的影片你就會知道
(30:44~31:45) 這些透過網友的Prompt產生出來的這些影片 也是非常的驚人很驚艷 所以說我們就可以確定說這個Sora的模型呢 它是真的非常強 它並不是一個Cherry Pick出來的很漂亮的Demo而已 那Sora產生出來的影片最驚人的一個部分呢 是OpenAI把它稱作Emerging Simulation Capabilities的一些能力 中文呢就是翻成慢慢浮現出來的這些模擬世界的能力 就是這個Sora模型 它對於這個世界是怎麼運作的 是有非常高程度的理解的 當然是跟人不能比啦 但是跟其他的AI相比呢 它真的是非常厲害 它很了解這個世界 就是比如說它有3D的概念 就你看很多這些影片 它其實有一個運鏡嘛 然後這個運鏡這個鏡頭往前走的時候 每個物體呢 有些轉向啊 有些變大什麼東西 看起來都非常的Consistent 非常的連貫 然後同時這個超驚人喔 就是Sora對於物件的存在是有基本的概念的
(31:45~32:45) 就是它知道說 假設我們今天畫面裡面有一隻狗 它趴在窗外 然後今天有一群人從這個鏡頭前面走過 把這個狗給遮住了完全遮住喔 然後這些人走過之後 這個狗還會在那邊 而且是完全找不出破綻的那種 看起來真的非常自然 那這一點跟剛剛講的3D的持續性 這個都是非常驚人的 因為Sora在產生這些影片的時候 它並沒有做物理模擬 也沒有做任何的3D的Modeling 它就是直接一個一個像素直接產生出來這樣 這個是什麼意思呢 就是我們在一般的這種電腦視覺的世界裡面 你如果要產生出這樣 讓電腦產生出這樣子的畫面的話 你第一步你是要先建一個3D的環境出來 你可能就是會使用這種Unreal Engine這種 拿來做遊戲的這種軟體 去把這個3D的世界給勾勒出來 然後在這個世界裡面 做出很多這些3D的物件 你趴在窗外的這隻狗 然後走過去的這些人 全部都是這些3D的物件這樣 然後這個Unreal Engine軟體
(32:45~33:45) 它有把所有的這些物理概念 全部都寫進去 所以說這些物件會怎麼樣 怎麼碰到會怎麼樣動啊 然後這些光影啊要怎麼樣呈現啊 這些全部都是有這個 數學的公式去Model出來的 然後在這個3D的世界中 你在跟電腦說 哪些東西要怎麼動 這些人要往右邊走什麼的之類的 然後這個電腦呢 再根據這個Unreal Engine 它原本具備的這些 原本寫進去的這些物理的公式 去算出說 光影啊會怎麼變化 然後其他東西會怎麼樣跟著動這樣 然後呢你再把最後這整個片段 變成一個2D的像素的呈現這樣 如此以來呢你才能做出一個 這樣子的影片 所以這邊計算量是很大的 所以你才會看到說 這些3D的動畫永遠都是一個動畫 它沒有辦法做得非常非常逼真 它沒有辦法像Sora這樣 產生其實幾乎分辨不出來是 假的的影片 因為一個現實世界中的畫面
(33:45~34:45) 有太多東西在動了 每一片櫻花 掉下來都是一個物件 每個東西你全部都要 就是放進這個3D的世界中 然後去同時進行模擬 是不可能的 應該也不是說完全不可能啦 就是你去網上查一下 就是Unreal Engine也是可以做出一些 真的是非常高度逼真的場景 但是當你東西一多 你就真的是很難去 真的是算出這個畫面這樣 然後這個Sora呢靠著 一大堆2D的這種像素的 影片進行training training完了之後 它竟然對於3D 跟物件的存在 是有概念的 完全沒有做任何的3D模擬 也沒有任何的物理概念 它就是直接根據你 給它的prompt 然後直接產出這些像素出來 變成一個影片 然後是3D consistent 而且是物件存在 也有一致性的這種影片
(34:45~35:45) 然後除此之外呢 世界中的這些物件 怎麼樣互動的這些一般常識 這個Sora也有一點 這就是為什麼你會看到 它在產生一個人在吃漢堡的影片的時候 Sora知道 這個人咬完了漢堡之後 他嘴巴離開漢堡之後 這個漢堡要缺一口 然後他也知道一個人在畫水彩畫的時候 他的筆跟這個紙 接觸完了之後 那邊紙上應該要出現 就是這個顏色的痕跡 然後針對這些emerging的 simulation capabilities OpenAI下的結語是 他們覺得這些能力 代表著這種video model 有潛力帶我們走向一個 能夠真正了解物理世界的AI模型 那我覺得基本上 當一個AI能夠真正了解 這個物理世界怎麼運作的 它基本上應該就是一個AGI了 它就是那種 已經可以取代 所有人類工作的那種AI了 不過當然啦
(35:45~36:45) Sora的這些simulation capabilities 就是它3D的一致性 物件怎麼運作的理解 這些東西我們都把它稱作 emerging capabilities 意思就是說它只出現一點 苗頭而已,它只是慢慢的浮現 你仔細看這個Sora產生出來的影片 裡面還是有非常多的 破綻的 最明顯的就是走路 我覺得不管是人類走路還是動物走路 你仔細看,看個10秒 你就會發現,其實好像有點怪怪的 人類走路應該不是這樣走 然後那個杯子打翻的時候 那個杯子移動的方式也是非常的詭異 打籃球的時候 那個籃球彈在那個 籃框上面的樣子也是很詭異 怎麼可能往前彈,然後又突然往後飛這樣 然後物件也會憑空出現 就像是這個小狗狗 原本三隻玩一玩 突然變成五隻了 所以說這個Sora模型啊,雖然說它對於世界的理解 跟其他的AI比 已經真的是非常強了 但是你如果拿來跟一個 真實世界中的生物比
(36:45~37:45) 比如說隨便路邊的一隻貓這樣 那隻貓對於世界的理解 絕對比Sora高非常非常多 好,那在講這個Sora有什麼應用 或者是有些什麼衍生的討論之前呢 我們先來講一下 比較技術的方面 就是這個Sora它究竟 是怎麼樣運作的 它是用什麼樣子的機器學習的技巧 那這部分可能比較難一點 你聽不懂也沒有關係 就當作一個補充這樣 那首先呢,就是OpenAI他們 有釋出一個他們所謂的 Technical Report 但這個Technical Report你仔細讀完 之後你會發現,啊,它原來 什麼東西都沒講 就是跟Google的Technical Report 是一樣的概念喔 它就講一些非常高層次的 一些大概念 然後講一些大家都已經知道的東西 不過這個Technical Report有給我們講一個 最重要的重點 就是說Sora背後的 模型呢,是一個叫做DIT的 模型,Diffusion Transformer 這兩個字,我相信你如果是
(37:45~38:45) 一直以來都有在關注AI 然後你也是,也會看得比較深的 那種,就不是像 媒體隨便報一報,你隨便看的那種 你會自己去研究的那種 你應該對於這兩個字都還蠻 熟悉的,但是我不確定你有沒有 聽過這兩個字合在一起 首先是這個最有名的Transformer架構 這個Transformer架構 基本上就是現在你看到的 所有大型原模型 它背後的架構 它是神經網路的一種 然後有用一些蠻特別的技巧 在進行資料的處理 包括這個,剛剛有講到的 注意力機制,Attention Mechanism 然後還有這個Position Embedding 就是,我不知道中文叫什麼 但就是會記錄 每個Token的位置這樣 這是一個Sequence to Sequence Model 意思就是說,一個Sequence就是 很多Token組成的一個序列 它是吃一整個序列 裡面包含很多個這種Token的序列 然後吐出一個序列 嚴格來說是一次吐出一個Token 然後這些Token會組成一個
(38:45~39:45) Output的序列這樣 然後這個Transformer Model,雖然說大家聽到它 可能都是聽到它 就是被用在大型原模型上面 但它的應用其實很廣 任何模態的資料它都可以處理 它也可以被拿來作為這個 圖片產生的模型 也可以拿來作為這個 語音辨識的模型 就是你看我們剛剛講的Gemini 它可以處理這麼多不同模態的資料 它本身也是一個Transformer Transformer之所以可以處理這麼多不一樣的資料 是因為它其實在做的就是 Next Token Prediction 它是在預測下一個Token 但是這些Token 它背後可以代表任何東西 換句話說啦,就是只要你可以把你的資料 變成一個Token的Representation 用Token來表示你的資料 這個Transformer就可以了解你的資料了 那我們剛剛講到的另外一個模型 叫做Diffusion Model Diffusion Model 我相信很多人有聽過這個Stable Diffusion 很有名的這個 能夠產生圖片的AI模型 那這個Stable Diffusion
(39:45~40:45) 它背後的架構 很明顯的就是一個Diffusion Model 其實你現在看到的 大部分的這些AI產生 AI生成圖片的模型 幾乎都是使用Diffusion的架構 像是Mid Journey也是 Diffusion這個詞 中文意思叫做擴散 你可以想像就是 一個一團 可能味道很奇怪的氣體 慢慢擴散到整個 就是房間之中的感覺 那這些Diffusion的AI模型 就是用類似這種擴散的概念 去產生他們的圖片 只不過他們產生圖片的這種Diffusion的過程 跟我們一般認知中的Diffusion是相反的 所以我們把它稱作Reverse Diffusion 那Reverse Diffusion是怎麼運作的呢 首先這個Diffusion Model 它會先產生一張全部都是雜訊的影片 這個雜訊 你就可以把它想像成 每一個像素的值 都是隨機的 那它其實也是有Follow一個分佈 它是Follow這個常態分佈 或是你說高斯分佈這樣
(40:45~41:45) 就是每一個像素的值 都是在這個Diffusion的分佈 就是每一個像素的值呢 都是在這個高斯分佈隨機進行抽樣 那你看出來當然就是一個 就是完全都是雜訊 什麼東西都沒有的一張圖片 那接下來呢這個Diffusion模型 會根據你給它的Text Prompt 你的指示 去移除掉圖片中的一部分雜訊 這個移除掉的這部分雜訊 是對於你的Text Prompt 來說是雜訊的雜訊 OK這樣不知道大家聽不聽得懂 但反正它一直重複這個過程 一直移除掉對於你的Prompt來說是雜訊的雜訊 你的這個圖片呢 就會越來越像是你的Prompt 對吧 然後一直移到了一個指定的步驟之後 就比如說20或30步 Diffusion Model就會停下來 然後你就有了這張漂亮的圖片 這張圖片呢 應該是跟你輸入的Text Prompt是一致的 因為 跟這裡的Text Prompt 不一致的這些雜訊都已經被移除掉了
(41:45~42:45) 那這整個移除雜訊的過程呢 我們就把它稱作Reverse Diffusion 因為正常的Diffusion 應該是這個雜訊慢慢的擴散到圖片 然後把圖片變成一個 完全Random的圖片 但是Reverse Diffusion 就是把這個雜訊慢慢的移出來 那你這樣聽起來你應該會知道 這個Diffusion Model 它裡面最關鍵的一個步驟 是什麼 就是知道什麼雜訊要移出去 對吧 假設我今天的Prompt是一隻貓 究竟什麼樣的雜訊對於一隻貓 來說才是雜訊 那一般來說呢 一個Diffusion Model它裡面是透過一個 叫做UNET的神經網路 在做這件事情 這個叫做UNET的神經網路 它叫做UNET是因為它 你把它畫出來它那個圖像 長得有點像一個U它從大到小到大 但重點就是 這個UNET它就會判斷說 這張圖片哪一邊的雜訊應該要移掉 那今天呢假設你把這個UNET 移掉換成一個Transformer
(42:45~43:45) 你今天就有了一個 Diffusion Transformer Model DIT也就是Sora 背後的這個模型了 也就是說Sora的這個DIT啊 它本身也就是一個Diffusion Model 只是它在判斷哪一個雜訊 要移掉的時候它是使用 一個Transformer Model在做這件事情 然後剛剛也有講了嗎 就是一個Transformer Model它是吃Token 應該說它是吃一個Sequence 然後這個Sequence是由很多Token組成的 所以你要想辦法把一個影片 拆解成很多很多個Token 這個你要怎麼做呢 OpenAI是有講說 他們是把影片呢 變成很多很多個 Space Time Patches 那首先他們有一段是在解釋什麼是Patches 這個他們講的好像是他們發明 這個Patches的方式 但絕對不是 這個已經超級久了就是 一直以來大家都是這麼做的 那影片就是 首先一部影片是由很多張圖片組成的 每一幀就是一張圖片 那這張圖片呢
(43:45~44:45) 它就把它切成很多很多的細塊 很多很多小正方形這樣 然後每個小正方形就是一個Patch 然後你把這些 這部影片呢 你就可以把它切成 一大堆的小正方形嘛 然後就把這些小正方形 每一個當作一個Token組起來 直接從第一個排到最後排出來 就是一個Sequence了 好那反正技術的東西不要講太多啦 我這邊下個總結 反正這個SORR的模型呢 它就是一個DIT Diffusion Transformer 那如果你要再去了解這個Diffusion Transformer的話 你可以直接找到他們的Paper 他們的Paper是叫做 Scalable Diffusion Models with Transformers 那是去年的論文 還是前年啦我有點忘記 反正是去年才被Publish出來的 然後這邊Paper背後就主要是兩個作者而已 然後其中一個叫做William Peebles 這個人呢現在就是在 OpenAI工作然後就是在 OpenAI做出SORR的這個 人之一所以呢 總而言之我們知道說你只要
(44:45~45:45) 拿一個Diffusion Transformer然後 拿超多超好的資料 以及超大量的運算資源 你就可以做出一個SORR 這樣子的模型了當然這個是非常 簡化的講法啦就是裡面 細節還是非常多嘛就包括 Diffusion Transformer它裡面很多的 有些什麼VAE這些的 Encoder啊這些東西可能都是 OpenAI他們自己去Train的啊 有特別調過啊什麼東西的 但反正大概念上 是這樣子然後在訓練資料這邊啊 其實有很多的謠言 很多人在講說OpenAI 是不是有使用Unreal Engine 5 來製造人工資料 來Train這個SORR 因為大家都覺得這個SORR 它對於物理概念的理解 真的是太強了怎麼可能運進 運進這麼順3D的概念這麼強 所以說一定是用 這個Unreal Engine 5這個 我們剛剛講過了這個 能夠製造一個3D世界的 這個遊戲引擎 去做一大堆人工資料 來刻意訓練SORR的這個
(45:45~46:45) 空間概念跟3D概念 那我自己猜是沒有啦 就是你仔細去看SORR的三個作者 我們會知道這三個作者是因為 這個Sam Omen他有Tweet嘛 他在他的推文裡面他在他的X文 裡面就是TAG這三個人 然後跟他們就是說 感謝你的貢獻SORR才會這麼強 所以我們知道他們三個是 SORR的主要作者 一個就是我剛剛講的William Peebles 寫DIT的那個人 另外兩個人呢也都是在搞 Diffusion Model 或者是一些其他的 Text to Image Model這樣 然後都沒有什麼3D的背景 所以我自己猜他們 應該是沒有 但我也很不確定 因為我看這個SORR的結果 真的是有點被嚇到 好那最後我們來聊聊一些SORR的 衍生討論跟這個SORR的 可能應用 我們先從一些衍生的討論開始 第一個就是很多人在講的就是 這個SORR呢是不是 代表AI開始有World Model了
(46:45~47:45) 我們這邊說World Model 就是AI的世界觀 AI對於世界的一個全面性的 了解 在這之前呢大部分我們應該有共識的 就是很多我們現在 有的AI他對於世界的了解 都是很片面的 像是大型圓模型 他有沒有對於世界的了解 有但是他是非常片面的 因為他所有知識都來自於文字 沒錯文字裡面有很多 對於世界的知識 就是他透過文字他已經學會了 基本上各行各業的所有基本知識 但是他對於世界的了解 還是非常片面的 他還是不會開車 其他的模型 都各自是在他的 他的領域 其實是有一部分的世界觀 建立起來了 但是他還是沒有一個完整的世界觀 最常講這種話的人就是一個叫 楊立坤的科學家 他是META的Chief AI Guy Chief AI Scientist 我不知道他的Title是什麼
(47:45~48:45) 反正就是META最屌的 AI科學家 他常常就跟大家說 現在這些模型啊 我們常常看到這些甚至是AI模型 他是永遠不可能建立一個 真的很完整的World Model的 他是不可能對於世界的運作原理 有很深入的理解的 因為他們的架構上 有很多根本性的限制 他每次講這個 就很多人喜歡嗆他 我覺得應該是他講話的語氣 他很喜歡在X上面開戰 然後就是講一大堆 就是他看到人家講錯 或是他不同意 就會非常震驚 非常嚴厲的去糾正人家 所以說他 我覺得很多人可能也是很喜歡跟他這樣戰 反正這個Sora一出來 很多人就開始在說 楊立坤被打臉了 因為你看Sora 他靠著Diffusion Transformer 他竟然可以學會一些物理的概念 一些3D幾何的概念 那我自己是怎麼想呢
(48:45~49:45) 其實我比較接近楊立坤的想法 就是我覺得 這些Sora這個模型 他沒錯 他感覺好像已經學會了一些物理的概念 然後隨著這個訓練資料越來越大 品質越來越好 他絕對會學得越來越好 就是你會越來越少看到 人走路的時候多一隻腳 或是他腳莫名其妙的交叉 然後你會越來越少看到 球在很奇怪的位置被彈出來 他會越來越接近 我們世界運作的模式 但是我覺得你要透過 繼續scale Sora這種模型 做到一個類似 接近一隻貓的World Model 的這種程度 我覺得還是不太可能的 因為根本上來看Sora這個模型 跟我們其他所有有的這些AI模型一樣 他都是一個統計模型 他都是一個Probabilistic 的Model 也就是說他對於世界的了解是透過一個統計模型 然後持續的去逼近這個世界 但是我們生存的這個世界 是一個確定的世界
(49:45~50:45) 牛頓力學是確定的 你不管走到哪裡地心引力 都是一模一樣 都是一樣的公式算出來的 F等於MA永遠都會成立 你可以透過這些物理公式算出非常非常精確的 所有物件怎麼移動 它的加速度是多少 你可以算得非常精確 然後你要靠著一個統計模型 去逼近一個非常精確的世界 當然你可以逼近到蠻類似的 但是要逼近到接近一個真正 生物理解世界的 這種程度我覺得是做不到的 而且就算做得到我們現在 也沒有那種資源可以讓我們做到 這件事情就是你會需要的資料量 跟你的運算量真的是 大到一個就是你完全 無法想像所以我們需要的是一個 懂什麼是F等於MA的 機器人他可以他可以透過 F等於MA去了解說 這兩顆球撞在一起 的時候另外一顆球他會用什麼樣 的速度往哪邊滾 而不是用一個就是用幾千幾 億張的兩顆球相撞 的圖片去慢慢學出說
(50:45~51:45) 這顆球應該要往哪裡滾 所以我剛po出那個Sora影片的時候 下面就有人在那邊留言說 是不是梁立坤要被打臉了呢 那我就有在下面回他說 我覺得啦 就是還不夠Sora還不夠 好那最後呢我們來講講 一些Sora的應用 或者你說會被Sora顛覆 產業好那 首先第一個我覺得最明顯 最直接的啦就是B-roll 產業我們所謂的B-roll就是 你在看一些YouTube影片的時候 有一些大家在講解 一些概念的時候可能會插入 類似符合那個概念的 影片嘛那這些影片呢 很快的就可以直接用這個 Sora生成了Sora 呢他可以直接根據你影片的 內容去客製化 出最符合 你的這個影片 最能夠講解你想講解的 概念的這個影片那我覺得這 應該也不用多講了回去講到這邊 就好了這很明顯啊大家都知道 好那接下來呢
(51:45~52:45) 另外一個會被顛覆的產業是什麼 是A片產業這邊其實也是 還蠻明顯的嘛就是生成這個A片 那為什麼我先說了 A片而不是先說 好萊塢影片呢 或者是這個Netflix的劇 為什麼是先說A片我覺得 A片一定會先出來 他會先被顛覆因為 首先這個技術上來看 就是你要產生AI的 A片他的這個 就是比較複雜度是比較低的嘛 就是畫面 畫面很單純就是動作 也很單純對那像是 這個好萊塢的大片呢可能就是 會比較複雜一點嘛然後另外一個 原因就是因為我覺得 A片這邊的需求是很強勁的 當然好萊塢那邊的需求也很 強勁但是大家不要小看這個 人類的慾望在娛樂 之前呢這個生理需求 要先被滿足那接下來一個 我覺得會被影響很大的產業 是遊戲業跟動畫業 這邊我知道很多人可能在說 什麼哇這些人要被取代啦
(52:45~53:45) 全部都不需要啦但其實 我覺得不是這樣我覺得這個 Sora呢會是這些人的 非常強的一個輔助工具 因為現在來看啦就是要用 Sora直接100%產生 一個完整的動畫片 還是不太可能的尤其你要 產生一個真正有商業價值 的動畫片大家真的 會想看的動畫片是絕對 不可能的這中 之間呢有很多人類 需要修改的地方嘛那像是 Sora這些模型呢現在他們 很大的一個缺點就是他們修改 的能力不太好當然 你可以做一些大範圍的影片 修改像是Sora有個功能就是 你可以給他一段影片然後 跟他講要改成什麼風格 他會直接改成那個風格這樣 你可以做這種大範圍的修改 但是今天當你要微調一個 小細節的時候你是很難調的 當然這部分的操控一定會 越來越進步嘛我們看這個 Stable Diffusion就知道了現在這個 Stable Diffusion的可控程度已經 非常高了嘛你可以改
(53:45~54:45) 一小部分的東西啊你可以 把它笑臉改成哭臉啊 之類的這些東西都可以做得到 那Sora未來也可以做到 但是你要產生一個 真的有商業價值大家看了 會覺得好讚的一個動畫 你要調的東西是很細微的 然後很多的 那這個部分呢就是傳統的這個 3D建模的方式 比Sora強很多的地方嘛 因為你把這個3D的世界 整個都建出來之後 你所有東西你當然都是可以 用滑鼠拉來拉去啊調整 但是這邊我覺得Sora還是 幫得上忙因為Sora可以 幫你產生第一版的這個 3D模型啊因為我們知道 這個Sora它有很強的 3D世界的理解能力嘛 它先產生這個第一版的 3D模型不管是這個人 這個物件還是這個 場景它先產生出來 然後你再用一些技術像是這種 3D Gaussian Splitting這種技術 把這個它產生出的這個影片 變成一個3D模型
(54:45~55:45) 然後它變成了這個3D模型之後 你再進行微調你是不是就 真的是快很多 我覺得不管是在這個 遊戲Game Development在建遊戲的 還是你在做動畫的 都可以因為Sora 而大幅提升你的生產力 好啦那今天的這個話題呢 我們就聊到這邊 聊了這個Gemini的1.5 Pro 也聊了這個Sora 那我覺得今天是有點 爆時啊就是 不小心就講到信頭上就講的 有點太久了啊然後好像技術的 東西講的有點太多了我不確定啦 但大家可以跟我說 就是我技術的東西 是不是講得有點太深了 太難了還是講得不夠深 或是講得太複雜了不好懂 好那最後呢就是 一樣如果你喜歡今天這則Podcast 一定要分享給你的親朋好友們知道 因為我覺得真的 很多人在看Sora這個東西但 沒有多少個人知道這個Sora 究竟是怎麼運作的或是這個 它的很多細節的部分啊
(55:45~56:15) 所以說分享給他知道 讓他找點知識然後讓他來 聽科技浪然後也謝謝今天的贊助商 NordVPN如果你也是想 要贊助科技浪的話 你可以在本集資訊欄的下面 找到這個科技浪的網站 點進去你會看到科技浪的聽眾 輪廓跟我的流量然後 同時呢你也可以 寄信給我我會跟你講我的報價 最後就祝大家有個愉快 的一週然後這個 下週呢有可能沒有科技浪 就去聽聽我之前的集數吧
