(00:00~01:05) Hello大家好,歡迎收聽科技浪,我是主持人哈利 科技浪是一個白話跟你聊科技的Podcast 希望可以用簡單易懂,但是又深入的方式,帶你了解時下最火的科技話題 本期節目由AWS贊助播出 我們上禮拜是跟大家分享了AWS的生成式AI平台Amazon Bayrock 本週是要跟大家介紹一個超棒的活動,叫做AWS Summit AWS台灣雲端高峰會 今年他們的主題是定調叫做 雲湧至深,沉浸於生成式AI的新境界 你從這個主題就可以知道今年會有非常多生成式AI相關的內容 但除此之外,當然也有包括傳統製造、金融服務、零售、娛樂、醫療和公共服務等產業領域的內容 然後還有轉型成功與創新的客戶案例的分享
(01:05~02:06) 2024 AWS台灣雲端高峰會即將在7月23日和24日在台北國際會議中心開展 匯聚了各行各業的開發者、企業專家與科技愛好者 為您帶來沉浸於生成式AI與雲端技術的專屬體驗 我覺得AWS這次的活動是真的很有料 他們裡面不但有豐富的論壇、互動式工作坊、主題實作、遊戲式體驗來幫助你學習科技新知 而且你還可以跟國內外的專家們面對面的交流 來了解這些寶貴的轉型實力以及AWS創新的服務與解決方案 這個活動是可以免費報名的 所以我這邊就邀請所有科技量的聽眾們現在就報名參加2024 AWS台灣雲端高峰會 來探索新思維掌握新技術吧 這個活動不只免費又有料 它還有一個額外的好康可以送給大家喔 就是你現在如果是早鳥報名的話 你去免費申請一個AWS的帳號 如果你是前200名的話 AWS還會免費送你一個專屬旅行收納組喔
(02:06~03:07) 所以說現在就趕快去早鳥報名 然後去免費申請一個AWS的帳號吧 報名的連結跟帳號申請的連結 全部都放在本集的資訊欄裡面 大家可以自行取用 本集業配就到這邊結束 謝謝AWS的贊助 好那如果你有在關注科技新聞 或者是你有聽科技量的EP40的話 你就會知道兩個禮拜前呢 Google跟OpenAI都在差不多的時間發布了他們最新的AI模型 這個OpenAI發布的是GBT-4O 那Google發布的是Project Astra 這兩個模型就是除了可能latency的進步 跟一些疑似memory的功能以外 他們最大的亮點就是他們是多模態模型 甚至這個GBT-4O是完全原生的多模態模型 但很多人不知道的是 其實在同一時間Meta也有發布他們最新的原生的多模態模型 但是因為他們真的很低調 就是他們只有發論文 然後還有一個部落格文這樣 所以說沒有媒體在報
(03:07~04:08) 然後絕大多數人都不知道這件事情 除非你是這個Machine Learning Community的人 你可能就會知道這件事 那我今天就是要帶大家好好的來了解一下 Meta被遺忘的AI模型 因為我覺得他的討論度雖然說是非常的低 但他對於我們了解現在最先進的多模態模型 多模態AI的研究究竟能做到哪裡 究竟是怎麼做的 是非常有幫助的 因為我們從這次Google的Project Astra 還有GBT-4O的發表我們就可以看得出來 很多人都開始說他們這些最先進的這些AI Lab AI公司 他們的研究成果越來越像 就是這個Project Astra的Demo跟GBT-4O的Demo 都是很類似的概念 一樣是一個語音助手 然後就開一個視訊給他及時去理解影像這樣子 所以就感覺好像大家在研究的東西都差不多 都在走同一條路這樣 只是有些公司走得比較快一點點
(04:08~05:09) 有些公司走得比較慢這樣子 然後確實除了這個多模態的研究以外 很多其他的研究領域也是差不多 大家可能都在用差不多的技術 然後做出差不多的成果這樣 然後這也合理啦 這幾間公司Google OpenAI Anthropic Meta 這些公司的研究員很多都跳過好幾間公司 很多OpenAI的研究員全部都是從Google那邊挖過來的 然後大家讀的論文也都差不多 都是在看的幾篇 所以說這個研究方向真的都還蠻像的 像是這次這個GBT-4O跟Astra 他們背後的技術可能就非常類似 儘管這個根據我的分析 Astra可能還沒有GBT-4O這麼原生 但是這個Gemini遲早會走到內部 所以他們的技術都很像 但他們都是碧圓模型 我們最多就只能在旁邊看他們Demo的影片 然後在那邊物理看花 在那邊猜一猜他們是怎麼做的這樣
(05:09~06:09) 但是呢 Meta他們這次發表的這個新的大型的多模態模型 它一樣我覺得是用差不多的技術做出來的 而且它是開源的 它是有發表論文出來 有發表Block Post讓大家去看這個技術 所以說呢 我覺得我們可以透過Meta它最新的這個模型 去了解到 最新的這一波大型多模態模型究竟是怎麼做的 這邊補充一下 臉書這個模型其實也不算是開源的 它只是發表了一篇Paper 然後在講這個模型 它怎麼做的 它的架構 它怎麼樣訓練的 然後它的成果如何 但它並沒有釋出模型本身 這個權重跟它的Inference Code什麼都沒有釋出 所以說我們唯一有的就是這篇Paper這樣子 我覺得這個就是今天的主題啦 來跟大家聊聊臉書的這個新的多模態模型 但除了這個以外 我覺得看時間吧 就如果還有時間的話
(06:09~07:11) 我也是想提到一些其他的這個AI研究的發展這樣 因為有一個領域就是這個Transformer Alternative這邊 最近有一些新的進展 我覺得蠻值得聊的 就是最近的這個State Space Model有一些新的發展這樣 但這邊我覺得會需要蠻多時間解釋 所以看今天這個Camellian講得怎麼樣這樣 好 那我們趕緊來講這個臉書的新模型吧 那我剛剛已經講了名字啦 它這個模型就叫做Camellian Camellian這個字在英文裡面就是變色龍的意思 那我覺得他們這一次的命名我覺得挺有趣的 因為我那時候看到這個這個模型叫做Camellian 我當然第一直覺得我是去找說 為什麼叫做Camellian嘛對不對 然後在論文中其實你找不太到 就是通常我原本會期待 它這個Camellian可能是一大堆這個 專有名詞的縮寫組成的嘛 就像是臉書的這個Lama的Model 它其實就是Large Language Model MetaAI 這串文字的這個縮寫組成的
(07:11~08:13) 但是呢這個Camellian我到處都找不到 這是什麼縮寫組成的 沒有他們其實真的就是把這個模型取叫做Camellian 因為他們爽 因為他們就覺得這個模型適合叫做一個變色龍的名字這樣 那我覺得這件事情其實很有趣喔 就是因為以前的模型 不管是哪一家Lab哪一家公司這個做的模型 大家都是用專有名詞的縮寫在取名字 他們可能就是從專有名詞裡面 想辦法去找出一些諧音 或者是把它組成一個這個比較有意義的名詞這樣子 像Lama就是這樣嘛 然後Google在Gemini模型之前的這個POM模型 P-A-L-M 它也是它原本是叫做Pathway Language Model 然後它取PA跟L-M這樣子 然後也有更懶的就是根本就不想想寫音 或者是名詞的那種就是OpenAI對不對 他們直接把他們的模型叫做GPT 就是Generative Pre-trained Transformer對不對
(08:13~09:16) 這些都是學術上面的專有名詞 但是呢我覺得最近這一兩年 我們都可以看到這些AI模型的名稱 已經開始越來越產品化 越來越大眾化了這樣 他們從原本的這些專有名詞的縮寫呢 變成是專門為這個模型去想一個名稱 像是這個Google的POM 現在他們下一代是變成Gemini對不對 那Gemini是什麼學術名詞的縮寫嗎 不是啊它就是一個他們就是想要取一個新興的名字啊 就是這樣 那那個臉書的Lama呢確實他們還是叫做Lama 但是你很有趣喔就是你去看臉書在Lama 2之後的所有文章 他提到Lama這個模型的時候 他的那個英文的大小寫 是跟他一開始Lama出來的那個大小寫是不一樣的 他原本那個最一開始的這個Lama呢 Large Language Model Meta AI對不對 他是LL大寫然後A小寫然後MA大寫 他是大寫大寫小寫大寫大寫
(09:16~10:21) 因為Large Language他的那個他的中間第三個那個A呢 其實是Language的LA 所以說他他其實對你如果是這樣縮寫的話你不應該把他大寫 但是呢在Lama 2以後他把這些大小寫的規則全部都取消 他就是把他全部就L大寫然後其他全部都小寫 就是你正常的拼一個這個Lama這個詞你會拼的這種方式 那我覺得這邊很明顯就是一個技術大中化的過程嘛 那個技術最一開始呢都是這些研究領域的人在看這些Paper在看這些Model 那當然他們就是命名就是會用這些技術的名詞下去進行縮寫嘛 然後他們縮寫呢其實把印湊一個諧音印湊一個這個名詞出來呢 其實也是為了讓大家對於這個論文有比較深的印象而已啦 那主要也都是給這些這個研究領域的人看的 但是當這個東西開始大中化之後 你就要有一個媒體好寫媒體好拼然後大眾好講大眾好認出來的一個名字嘛
(10:21~11:24) 那這個時候你就不用硬是從你的這些專有名詞去縮寫成一個有意義的名詞 然後你在拼的時候你也不用就是該小寫的小寫大寫的大寫對不對 像除了這個Lama以外這個Palm原本也是這樣嘛對不對 Palm的拼法是P大寫然後A小寫然後LM都大寫 那中間那個A小寫就是因為他那個A是Pathways的PA來的嘛 他A本身並不是一個字所以說他小寫 那這些模型呢在過去這一兩年非常快速的大眾化 然後大家全部的人都開始關注然後媒體都開始寫 甚至有一些開始商品化了就像是這個Gemini嘛 他已經都是變成一個消費者會需要直接接觸的一個產品了 所以說他們這時候當然不能再用這種 這種以前這個專門編給研究人員看的這種名稱這樣 但有一個例外啦就是OpenAI的GPT 就他們現在還是叫做GPT4、GPT4O、GPT3.5、ChadGPT 他們還是沒有離開GPT這三個字
(11:24~12:26) 為什麼呢因為這個GPT這三個字呢已經在過去這一兩年變得 變得是Culturally Significant了 就是他已經有文化上的意義了 就是有點像是Google他已經變成了一個動詞了 就我要Google一下這個東西Google那個東西 Google這個字原本就是一間公司的名稱啊 一個Social引擎的名稱啊 但現在已經變成一個動詞他已經有文化意義了 Netflix也是啊Netflix Central對不對 然後這個GPT呢或者你說ChadGPT 已經一樣被很多人當成是一個這個會語言的AI的代稱 就是懂語言的AI的聊天機器人 他們就說他是ChadGPT這樣 我發現很多人都這樣子用 而且不得不說這個GPT 雖然說他真的就是Generative Pre-trained Transformer 這三個很噁心的字合在一起 但他真的是怪順口的啊 就唸起來尤其是ChadGPT、ChadGPT唸起來真的怪順口的 而且我覺得他們那時候在想ChadGPT這個名字的時候 一定根本沒有花時間想
(12:26~13:26) 他們就是隨便就說這個GPT模型的Chad版 那我們就說是ChadGPT這樣 然後他們一定沒有仔細去思考命名這件事情 因為他們那時候發布ChadGPT 在這個2022年底的Nerbs的Conference發布這個ChadGPT 他們是以一個這個Research Preview的方式在發表這個東西 他們不是一個產品的launch 他們是根據他們的講法一個Lowkey Research Preview 但沒想到這個ChadGPT立刻爆紅了嘛 然後Same Moment接下來就立刻把它做成一個產品 然後在過程之中呢他們好像也沒有什麼可以改名字的機會了 因為他們就以這個名字爆紅了嘛 但就結果來看是還不錯了就挺順口的一個名字 好那我們回到臉書的Camellia模型 那這Camellia系列模型呢有很多不同的大小 有這個34 billion也有7 billion的參數的模型 然後他們有個很大的特點 就是他們在模態的處理這邊 他們的Input模態有文字跟圖片這兩種
(13:26~14:26) 然後是兩種可以同時使用的 你可以一篇文章裡面是文字圖片夾雜的 他們叫做Interleave的Texan Image這樣 那這其實沒有什麼特別的喔 因為Gemini 1.5 Pro、GPT-4O他們都可以做到這些事情嘛 然後甚至之前比較弱的GPT-4V也都可以做到這些事情 但重點來了他的Output的模態也是Image and Text 也是文字跟圖片然後也是可以Interleave的 那這邊就是我們比較少看到的 或甚至是說一般消費者能使用的所有AI模型之中 都沒有任何一個可以做到這件事情 就是沒有任何一個模型呢 他的Output可以同時Output文字跟圖片 就我所知是沒有 就是你們在用ChatGPT產生圖片的時候 並不是ChatGPT的GPT-4或GPT-3.5模型再產生那個圖片 他們是用一個叫做Dolly 3的模型再產生圖片 他有特別的產生圖片的模型 然後Google這邊也是一樣
(14:26~15:27) 你叫Gemini產生圖片的時候 不是Gemini在產生圖片 是Imagen 2或者是Imagen 3再產生這些圖片 我知道Google說這個Imagen的正確念法其實是Imagine 但我都把它念Imagen了Who cares 反正能夠同時理解圖片跟文字的模型很多 但能夠同時理解圖片文字 然後也產生圖片跟文字 而且甚至是Interleaved圖片跟文字的這種模型 就我們比較少看過 那這個Chameleon就是一個這樣子的模型 然後最先進的這個GPT-4O也是這樣子的模型 很多人沒有去他的官網不知道 GPT-4O也可以產生圖片 那這個Gemini呢 Gemini 1我也不確定他們接下來最新的Gemini會叫什麼 可能Gemini 2吧 Gemini 2也可以做到這件事情 所以很明顯的這一批最新的多模態模型 跟我們原本認知的多模態模型 或者是我們現在所有可以用到的多模態模型 是完全不一樣的 它能做到事情是不一樣的 然後等級也是完全不一樣的
(15:27~16:27) 就是他們不只能做到新的事情 他們在舊的事情上面還能做得比原本的模型 比那種舊的模型更好 像是我們從GPT-4O這邊 你們看到這個demo裡面 他們demo了很多新的應用 這些都是舊模型完全做不到的 但是在舊的模型能做到的這些事 就比如說這種純文字的Task GPT-4O還是做得比其他的模型都好 對吧 像是GPT-4O在Chapel Arena的排名是第一名 也就是說它是現在全世界所有的AI模型之中 能夠給出最好的文字回答的模型 那這個Chameleon也是一樣 儘管它可以做到很多新的應用 它的output又多了一種模態 但是它在原本舊的模態的事情上面 比如說純文字的Task上面 它可以表現得跟比它大一點的模型一樣好 比它大一點的文字模型像Mixro 8707B這樣一樣好 然後在原本的這種 舊時代的多模態模型的問題上面
(16:27~17:28) 就比如說就像是給它兩張青蛙跟蟬蟲的圖片 然後問它說這兩個東西差在哪裡 像是這種問題喔 它竟然表現得比GPT-4V還有Germany還更好 這個GPT-4V跟Germany原本就是在這種問題上面最強的嘛 這個GPT-4V應該是比較強啦 然後這個Chameleon應該是比這兩個模型都小非常多 34 billion 但它竟然可以表現得比較好 哇講到這邊也是不禁有點感慨啊 就想當初去年10月的時候 那個時候GPT-4V剛出來嘛 那時候是我們第一次看到 一個模型可以同時理解文字跟圖片 所以說它產出的 它的output只有文字這樣 但它可以同時理解文字圖片好厲害喔 然後我那時候就有出一集科技量嘛 科技量EP10 在帶大家非常詳細的解讀這個GPT-4V 它究竟有多強 然後它的背後的技術可能是什麼這樣 那個時候就覺得 哇 我們有GPT-4V這種模型真的是太扯了 太強了這樣
(17:28~18:29) 但沒想到現在這個Camellion 34 billion參數的一個模型 就可以打敗GPT-4V了 而且還可以做到更多的事 還可以產生文字跟圖片的模態 所以說 哇這個 也還不到一年喔 真的是發展很快 好所以我們這邊做個簡單的小節 我們原本有的這些多模態模型 就是包括GPT-4V包括Gemini 他們能做的就是 輸入是不同模態的資料 可能是圖片跟文字這樣 然後輸出永遠都只有文字的這個模態 那新的這一波多模態模型 從這個Camellion到GPT-4V到未來的Gemini 他們一樣是可以輸入很多不同模態的資料 但同時他們也可以輸出這些各種不同模態的資料 他們不是只能吐文字出來而已 而且他們在原本模態的任務上面 也表現得比原本的模型還更好 好那接下來重要的問題就三個嘛 第一個就是他們現在能做到什麼事情
(18:29~19:29) 第二個是他們為什麼可以這麼強 他們是這一代新的多模態模型呢 跟舊的到底差在哪裡 然後第三個呢就是 未來會如何 他們的下一步是什麼這樣 那我們先從第一個開始 這個Camellion現在可以做到什麼呢 這個GPT-4O能做到什麼我們上一集已經講過了 雖然說我們只講到一部分 我們只講比較多這個音訊的模態 然後產生圖片的或是文字的模態 但這邊我們就先不講 我們今天講Camellion Camellion它沒有辦法產生音訊 它只能產生文字跟圖片 所以它就沒有辦法像GPT-4O一樣 可以做出這種很像雲端情人的這種demo 那他們在文章裡面 他們在paper裡面給的一些 他們的一些模型的成果呢 其實看起來也沒有非常的exciting 就是它能做到什麼呢 比如說你可以問他說 可以介紹一些鳥鳥給我嗎 不同鳥的品種這樣子
(19:29~20:29) 然後他就會給你一個鳥鳥圖鑑 如果原本是text的模型 他可能就會列點式跟你說 某種鳥的特性是什麼 某種鳥的特性是什麼 但他會給你一個 Camellion會給你一個鳥鳥圖鑑 就是他會給你一張鳥鳥的圖片 然後旁邊寫說 這個是什麼鳥 它有什麼特性 然後再來另外一張鳥鳥的圖片 然後你寫說這是什麼鳥 它有什麼特性 然後就給你一個圖鑑這樣子 那這個就是因為它的output呢 不只是可以產生文字跟圖片而已 它是產生interleaved的文字跟圖片 意思就是說 它會知道要在哪裡插入文字 哪裡插入圖片 然後給你的是一個非常完整的 順序都正確的這樣子的一個鳥鳥圖鑑這樣 所以你可以想像 它也可以產生出很多這種 圖文並茂的部落格文 或者是有插畫的小說這種東西這樣 那這種應用的部分大家自己在想 我們來講講關鍵問題二 也就是它究竟是怎麼做到這件事情的呢 而且也不只是Camellian喔
(20:29~21:29) 這個GPD-4O他們背後的技術究竟是什麼 為什麼會跟舊時代的多模態模型差這麼多呢 我講到這邊我突然覺得有點好笑 就是我們把這些模型稱作舊時代 但他們其實根本不到一年前 就是這個GPD-4V去年十月發表的嘛 所以說舊時代其實也不舊啦 就幾個月前這樣子而已 但其實這個AI世界真的是變動太快了 他們幾個月已經感覺是一個時代過去了這樣 好那所以這個Camellian背後的技術究竟是什麼呢 我覺得一言一必之就是 讓這一個模型在最一開始剛要開始訓練的時候 就同時用不同模態的資料去訓練它 而不是在這個模型 先用文字訓練完了之後 再把它跟一個圖像的模型融合在一起 然後進行微調訓練 這是我們原本舊時代的模型的做法 我們現在新的這個Camellian 就從一開始就是一個多模態模型
(21:29~22:31) 那我覺得要讓大家更深的了解這個多模態模型的發展呢 我們就先從最一開始的最早期的多模態模型開始講 然後我覺得主要分三個階段啦 最早的那個階段呢就是那個Bard 也就是Google Gemini的前身 Google最一開始的聊天機器人AI Bard剛開始可以接受圖片input的時候 那個時候的Bard就是第一階段的多模態模型 那這個階段的多模態模型呢 基本上就是兩個模型在完結裡 一個單純語言的模型跟一個單純圖像的模型在完結裡 這個圖像的模型呢 會把使用者輸入的這個圖像轉變成一串文字 就是幫這個圖片上一個Caption 它就是一個Captioning Model 接下來Bard的本身呢 那個時候可能是用Prompto 它就會結合它使用者的Prompt 以及這一串Caption 然後產生純文字的Output這樣 那這個其實根本算不上是一個多模態模型
(22:31~23:32) 這個只是兩個不同模態的模型在協作而已 對不對 那這個方法呢很明顯的不夠好 因為它這個Image Captioning Model 它在把這個圖片變成一串文字的Caption的時候 其實這圖片它本身有一些重要的資訊 已經被它丟掉了 因為你這串文字敘述你寫的再怎麼好 也比不上一個人親自看這張圖片嘛 對不對 因為你一串文字你真的沒有辦法把這張圖片所有重要的資訊 全部都這個描述出來 而且描述的非常精準 就像是你今天腦中想任何一個畫面 然後你嘗試用語言去跟你的朋友去描述這個畫面 那你朋友想到的腦中的畫面跟你想到的會一模一樣嗎 百分之百不可能啊 所以當這個純文字的模型接到這一串 有點像是這個Image Captioning Model 轉述它這個圖片的樣子的時候呢 它就只能透過這一串這個轉述後的文字去做判斷 那這個時候呢它給出來的這個回答 就沒有辦法非常的好 那第一階段講完了
(23:32~24:33) 那接下來第二階段的呢就是GPT-4V了 就是去年10月推出的這個GPT-4V模型 那當然這邊其實也是我們的猜測 因為GPT-4V實際背後的技術我們都不知道嘛 但我覺得我們就是從它的表現 以及當時的技術 我們可以合理的猜測它應該就是這樣運作的 好那這個階段的多姆太模型呢 它不是兩個模型協作 而是兩個模型黏在一起 也就是說它確實還是有兩個模型啦 但這兩個模型到最後會合成一個 它最後只會用到其中一個 它只是一開始在處理模態的時候 是兩個不同的模型在處理這樣 那這個具體來說怎麼做呢 我們知道現在所有的大型圓模型 它背後的架構都是一個叫做Transformer的架構嘛 那這個Transformer呢 它的Input跟Output都是Token 這也是為什麼你常常聽到這些媒體在說這些 大型圓模型它的Context Length 有多少個Token啊 然後它這個API呢 1000個Token是要多少錢啊對不對 這就是因為這些Transformers
(24:33~25:34) 它在處理資料的時候 它在To資料的時候 它都是在處理Token跟ToToken 並不是文字這樣子 然後這個Transformer模型 把它的資料變成了一個一個Token之後呢 它會再把這些Token變成一串一串的項量 這些項量你繼續查一下是什麼東西 反正就是一串數學數字這樣子 那這串數字對於這個Transformer來說是有意義的啊 但人類是看不出來是什麼意思這樣 它裡面含的意義呢 就是存在這個Token 它的語意 就是這個Token它到底是什麼意思 然後有了這些項量之後呢 Transformer會再為這些項量進行一些加工 就比如說會加上 這些每一串項量它的一些位置的資訊 就是這一串項量 或者你說這個Token 在這一串文字之中 它是排第幾個 它的位置在哪裡 就是1 2 3 4 5這樣編好 它是有一個演算法去做這件事情
(25:34~26:35) 這個步驟叫Positional Encoding 然後這個步驟結束之後呢 這一串Token也就是很多很多的這些項量 就會被丟到Transformer的本體裡面 那本體呢基本上就是 很多很多的Transformer layer 每個layer就是由 Multihead Self-Attention、Feed Forward Network組成的 反正這些東西我就不細講了 反正你就把它想像成它的本體 好所以簡單來說 不管你是任何模態的資料 你只要有辦法把它變成一個一個Token 然後你把這些Token變成 一串一串有意義的項量 你的這個Transformer 你就有辦法可以處理這些資料 處理這些資料的意思就是說 你可以拿這些資料來Train這個Transformer 也可以讓它產生這些資料 這樣子 好那你現在有足夠的先備知識之後呢 我們回來講這個第二階段的 多模態模型 那我剛剛是說它是有兩個模型組成的嘛 這邊呢它一樣 第一個就是一個原本的 處理會處理文字的 這個Transformer model
(26:35~27:35) 這個文字的大型語言模型嘛 那另外一個模型呢 是專門處理圖片的 一個模型 這個模型呢它的專有名詞是一個 你可以把它稱作一個Vision Encoder 然後它具體來說 它會是什麼樣的模型 就是一個Vision Transformer這樣子的模型 第一個步驟呢 就是這兩個模型 分別處理各自的模態 這個文字的大型語言模型 先把它的這些文字 輸入變成一個一個 Token 然後把它變成一個一個項量這樣 這個Vision Transformer 它一樣把它這個使用者的圖片 變成一個一個Token 然後再變成一串一串的項量 那這個你可能問問說 就是要把文字轉換成Token 還算直覺嘛 對不對就每個字一個Token 但有一些字比較複雜就 拆成兩個Token這樣子 一張圖片要怎麼樣變成很多個Token呢 那這邊其實 用這個Vision Transformer的做法
(27:35~28:35) 其實是非常單純的 就有點像是我們在切豆腐一樣 我們就是橫著切幾刀 直著切幾刀 然後就變成很多很多正方形的小塊 就這樣就這樣而已 那個一個Transformer 可能橫著豎著被切 個個切個十幾刀 然後最後變成幾百個 小塊然後每個小塊就是一個 Token就是這樣子而已 然後它這個Vision Transformer會再把每一個Token 變成一串項量 那就是我們現在 有了這個圖片的 一大堆項量然後你 使用者輸入的文字也被變成了一串 一串的項量這樣子 那這樣是不是就可以把兩邊的項量全部都串 在一起然後直接讓這個 大型圓模型的 Transformer本體去 處理這些同時處理這些 文字跟圖片的Token 那概念上來講是這麼 做沒錯啦但是中間 會再多一個小步驟就是這個 因為這個圖片的Token 它是Vision Transformer
(28:35~29:35) 這個模型做出來的 然後這個文字的Token 是你原本這個 大型圓模型本身 它可能用的一個Embedding Model 做出來的反正總而言之 是兩個不同的AI模型做出來的 那他們這兩個模型做出來的 項量呢其實 是會有差異的那這個 差異呢你如果有修過這個線性 代數你就知道其實用一個 很簡單的數學方法就可以解決了 就是用一個 我們叫做Linear Projection Layer 具體來說就是 一個矩陣啦然後你用 這個矩陣去乘上 這個原本的項量 就可以把它轉變成比較低 微度的一個項量這樣子 這邊就是數學方法大家自己去查一下 那這些圖像被轉變成 Token然後再被轉變成很多 項量然後再被就是 投射到這個 跟文字項量是同一個 微度的這種項量 之後呢它就可以直接 跟文字的這些項量全部
(29:35~30:35) 黏在一起串在一起 一起丟進Transformer的本體 裡面進行處理那我們第二階段的這個 多默泰模型也就完成了 我這邊做個白話的總結 基本上呢它就是有兩個 模型一個模型是專門處理 文字另外一個模型是專門 把圖片變成跟文字 差不多的表示差不多的 這種樣態然後 同時跟文字合在一起 丟進這個處理文字 的模型裡面那這個做法呢 很明顯比上一個階段的模型 是更好的因為它它能夠 抓到更多的這個圖片的資訊 對不對上一個階段這個圖片 資訊在它被轉變成 文字的過程中它就已經消失 很多了但這個階段 它它這個圖片 是被先轉成很多項量 那這些項量呢 如果轉變的好的話它裡面 是保存了很多這個圖片的 資訊的那這麼這麼多 這個資訊的項量呢 跟文字是一起被丟進同一個 神經網路同一個這個
(30:35~31:35) Transformer的大腦裡面同時做處理 這樣好所以說它 很明顯的它不只是這個 圖片的資訊可以吃到比較多理解 比較多它還可以學會要同時 處理這個圖片的 Token跟文字的Token那這個 結果呢就是哇這個GBD-4V 它理解同時理解文字 跟圖片的能力真的是非常 強喔那具體來說 它可以做到哪些事情我建議大家 可以去聽科技量的EP10 我有做一個非常詳細的解說 不過呢它其實還是 有一些不足之處 它還是可以再更好為什麼呢 因為首先第一個就是 你的這兩個模型也就是你的這個 大型元模型跟你的這個 Vision encoder 它們是分開Train的 然後甚至你的這個大型元模型 它可能在預訓練的時候 它完全沒有訓練過任何的這個 圖像的Token它就是 純文字的一個模型但是 你只是之後用一個 Linear projection layer把兩個東西給黏在一起 然後進行了一些
(31:35~32:35) 微調對不對就是你 多用了一些有圖像 跟文字的input去 微調了一下這個模型讓它 多學會了怎麼處理這個 圖像的Token這樣子 所以你的這個大型元模型 它並不是出生就懂這些 圖像的Token它是之後 才被微調出這個能力的 而且呢它這個對於 圖像Token的理解呢 一部分也是來自於這個 Vision transformer它本身 對於這個圖像的理解 就是這個Vision transformer它有自己用 它的這個 訓練資料集去訓練過 那它之後把它產生出來的 這些項量在丟給這個 大型元模型的時候 就等於是把它自己對於這個圖像的 理解丟給大型元模型 那它理解這些 圖像這些Token的方式呢 當然就是跟大型元模型理解 文字的方式不太一樣嘛 雖然說它們的模態不一樣 但是就像我剛剛說的 它們產生出來的這個
(32:35~33:35) 項量的這個維度都不一樣嘛 反正你一個模型在消化 一些資料的時候你一部分 的資料還是別人消化 完了之後丟給你半圖 丟給你的這種資料 那你自己再去消化你當然是會 還是會出現一些問題嘛 那根據以上講到的 這些不足之處呢 總之這一部分第二階段的這種多模態模型呢 它 還是有個天花板在 就是你再怎麼train它就是只能做到 這些事情它不能再更強了 這樣子 你想要再更強你就必須進入下一階段 也就是我們現在終於要 開始講了就是 Camellia GBCO 他們這一階段的多模態模型 我們就把它稱作這種第三階段的 多模態模型 然後根據Meta的語言它是說 它是把它稱作Early Fusion的 一個model那這些Early Fusion的 model像這個Camellia 它最一開始就是 只有一個模型它從頭到 尾都沒有兩個模型它就是只有一個
(33:35~34:35) 模型然後從最一開始 在模型剛要開始預訓 練的時候呢它就是直接 把不同模態的資料一起丟 下去做預訓練 也就是說它在做一開始的 tokenization跟embedding的時候 它是直接用 同一個模型同時處理 其實不太一樣就是它 圖像還是有圖像的tokenizer 文字有文字的tokenizer 但這些東西被tokenizer完了之後 會被丟到同一個embedding model裡面 去製造這些像量 就是你從一個token 變成一個embedding也就是一串像量 的過程我們把它稱作embedding 那在第二階段的多模態 模型它是vision encoder 對vision encoder 或者是你說vision transformer 它來做出這個圖像的embedding 然後丟給然後再project到 這個文字的embedding這邊 但是呢在這個 chameleon這邊呢它是 文字跟圖像的token全部都用 同一個embedding model 把它同時做出
(34:35~35:35) 這個文字跟圖像的像量 然後這一大坨像量就是圖像跟文字 混雜的像量但是每個都一樣長 的這些像量就一樣 被丟進去進行 就是標準的一些處理過程 加上這個positional encoding 然後再被丟進transformer 的本體裡面對不對所以這個 模型它從最一開始就是 所有的模態一起學習 對它來說文字跟圖片的 模態其實基本上沒什麼差別 嚴格來說是有差啦但對它來說 就是在同一個它的字彙庫裡面 它的詞彙量裡面 包含了文字的詞彙跟 圖像的詞彙都在同 一個詞彙庫當中然後打從 一開始呢臉書就有丟這種 interleaf的訓練資料 給它進行預訓練 這個interleaf我剛剛也說過了嘛 就是那種文字跟圖片 交叉的這種文章嘛 它一開始就直接把這種文章 丟下去讓它進行預訓練 所以它打從出生 它就知道有這個文字 跟圖像這兩種不同的模態然後
(35:35~36:35) 兩者怎麼分然後兩者 有什麼樣的關聯它從打從 出生它就在學這些東西了所以 它會有幾個優點就是第一個 它不同模態之間的關聯會學得非常好 那這個 當然也不用多說了它 打從最一開始就是同時在學了 那第二個呢就是 第二個優點就是它可以output多 模態因為我們都知道transformer 這個模型它在產生 output的時候它是一個一個token 吐出來嘛對不對 這chartgb是一個字一個字吐出來嘛 然後它每一個字都是 就是based on前面所有的字 去算出一個它 字彙庫當中機率出現 機率最高的那個字對不對 它是從它的vocabulary 裡面挑出出現 機率最高的那個字那它現在 它的vocabulary裡面除了 文字以外也包含圖像 的token所以說它 outputtokens的時候本來就是每個tokens 都同時考慮所以它當然 可以產生文字的token也可以產生 圖像的token然後我剛剛有說了
(36:35~37:35) 就是一個圖像呢它會被 有點像切豆腐一樣把它切成一個 一個小小的正方形然後這樣子 一塊每一塊是一個token 那它產生output的時候一模一樣它就是 一塊一塊小正方形這樣吐出來 然後就是然後 從左到右由上到下 吐出一張完整的圖片這樣子 然後這樣做其實還有另外一個優點就是 不同模態之間的學習 其實會有加成效果 意思就是說你在圖像這邊 的學習其實可以幫助到 你在文字這邊的學習然後 vice versa對不對文字可以幫助到 那這個是為什麼 其實也沒有 我覺得應該也沒有什麼科學根據啦 應該說還沒有人就是實驗出來 但是我們從這個chameleon 我們已經可以看到這一點啦對不對 就是它明明參數少少的34 billion 而且還多了一種模態要學 但它在純文字的task上面 反而可以表現的比純文字的 模型還更好對不對 就我一開始說了嘛那 GPT-4O也是一樣的道理因為它 每個新的模態會加成
(37:35~38:35) 它原本模態的這個理解 所以說這些early fusion的 多模態模型真的是 有史以來最強的這些多模態模型 然後我講了很多優點 我們其實也是要講一點缺點啦 就是它其實也是有 不太好的地方第一個就是它 訓練上會比較困難一點 這個是臉書這個paper他們寫出來的 他們是說他們在 scale它的時候尤其是 當它參數超過8 billion 的時候然後訓練資料超過多少 它開始訓練 開始變得很不穩定之類的 他們有一些方法去 解決這個問題然後 有興趣可以自己在這邊看 我覺得這邊真的是太細節了我就不講 他們還有另外講到一個 我覺得蠻有趣的一個 挑戰吧就是 他們有發現說因為它現在這個 模型它的詞彙量包含 圖像的token以及文字的token 所以它有時候 在output的時候它output 那個圖像output到一半 可能這個圖像還沒產生完
(38:35~39:35) 它會不小心吐出一個文字的token 所以說這個圖像 就出來了一半 出來了三分之一結果突然跑出一個字 在中間這樣子 他們有發現這個問題 他們是有用一些外加 的方法去解決就是 用一些可能 人工的程式去 叫這個模型說 你現在在產生圖片 你只能給我產生圖片的token 不要去亂產生 文字token這樣子 我覺得它還有另外一個挑戰 是這個臉書的paper沒有講到的 因為他們確實 不太會遇到這個挑戰 當你要做像是openAI GP4O這種input output 同時都有三種 模態的這種模型你就會 多一個挑戰就是你會需要 很好的訓練資料而且是 同時有三種模態的高品質 訓練資料我覺得這種資料 全世界只有一個地方有最 讚的就是YouTube 那這個openAI他們每次
(39:35~40:35) 在說他們用什麼哪裡的 訓練資料他們都說 we use publicly available data 這就是他們講的 每一次都講這句話 那publicly available data 是什麼東西我覺得就是YouTube啦 對不對為什麼因為 YouTube是不是publicly available 對啊全世界的每一個人都可以 隨便上網就上YouTube去看 任何的影片嘛對不對 但是你能不能拿這個資料來 training我覺得這邊就 我覺得是不太行啊 老實說就你從一個 這個創作者的角度來看 你這麼辛苦去創作了一部影片 結果你的影片竟然被拿去 訓練一個AI模型然後 這個AI模型被openAI拿去賺錢 然後你一毛錢都沒分到你不覺得 這樣非常不合理嗎 所以我覺得openAI應該就是 就是爬了YouTube的資料 去training他的模型 但是他真的是不能講這件事情 所以說他就說 we use publicly available data 這樣講其實也沒有錯
(40:35~41:35) YouTube確實publicly available 但他並沒有 特別說是data that are publicly available to train 對不對結果 他如果換成這樣子的講法 他就沒辦法講YouTube啦 所以我這邊呼籲所有 在interview Sam的人 或者是任何這個 openAI的C級人物Mirah Murati 之類的你們下次要問 就是要問說 is GBT-4.0 trained on data that are publicly available to train not trained on publicly available data OK好那我們看完了 這三階段的不同的多模態 模型的演化從第一階段 我們從兩個模型在output 的這個等級彩合在一起 再到第二個階段我們在 前面處理這些 token在處理embedding的時候 就把他們合在一起 再更到第三階段這邊 我們打從出生最一開始 這所有的模態都合在一起 這個 我們看到一個趨勢就是這個
(41:35~42:35) Fusion越來越early 直到最後這個super early Fusion的這種chameleon這種模型 那我們也可以說是 到了這個階段模態的Fusion 模態的融合已經告一段落了 接下來要做的事情就是 開始不斷的scale了 不斷的擴大規模不斷的 擴大到新的模態 然後我覺得未來的目標 可以分成兩個階段來看 第一個階段是把文字 影片跟圖片 跟音訊這四種模態 做到完美input output 都有這四種模態 而且這個模型把這四種 每一種資料都學得非常好 這是第一階段 這個階段就是擴增模態 擴大到 這四種模態以外的包括動作 不同關節的移動 包括3D 這個待會再講 現在大家的目標都是第一階段 我覺得目前所有公司裡面 最領先的一定就是OpenAI 因為OpenAI這個GPD-4O
(42:35~43:35) 雖然說它還沒有完全釋出 但它在 文字圖片跟音訊這三種 模態已經 我認為應該是已經 蠻厲害了 我覺得還不到精通 但已經還蠻厲害了 他們可能再把這三種模態 練得更強一點 再增加影片的這個模態 它就可以變得非常厲害了 它就可以達到第一階段目標了 它是走在最前面的 接下來我覺得第二名這邊 可能就是Google跟臉書 他們可能是在差不多的位置 Anthropic不知道也可能是在這邊 也可能就是一些明星AI新創 包括這個 像是法國的Mistral 或是馬斯克的XAI 他們最近剛完成募資 我覺得XAI 真的很有可能是 一大潛力股 因為他們XAI有個其他公司 沒有的一個絕對的優勢 就是他們有 推特的資料他們有X的資料
(43:35~44:35) X的資料其實也是 超讚的他們有很多很多的影片 這些所有的資料 都是完全免費 可以給XAI train的 你不用像OpenAI 這樣偷偷摸摸去挖別人的資料 然後你也不用 就像OpenAI跟Google這樣子 付大筆的鈔票給Reddit 去買他們的這些訓練資料的 這個訓練的權利 或是買那個 他們還有買誰啊 還有那個OpenAI最近跟VOX 也有簽嘛反正他們就是跟這些媒體 或者是論壇去簽約 去要他們資料來train這樣 那這個馬斯克他們 哇真的是 作勇推特上面所有的資料 真的是非常棒 但反正無論如何啊 遲早會有公司可以做到第一階段 當我們有了第一階段的這種AI模型 也就是一個early fusion 然後同時可以理解 影片圖片文字音訊 這四種模態 然後同時也可以output
(44:35~45:35) 這四種模態的這種AI模型 我覺得到時候事情會變得很有趣 就是 我覺得那種模型呢 應該就是我們有史以來 唯一一個除了會講話以外 還會有表情的AI助理 就從我們有AI助理到現在 我們所有AI助理 都是只有語音嘛 我們就是語音而已 ChadGBT也是只有語音 但我們到時候會有一個有表情的AI助理 為什麼因為他可以 他可以直接產生出影片嘛 他就是直接產生出 他在講什麼樣子的話 什麼樣子的情緒 他就產生出相對情緒的一個臉 一個表情 這個影片 我現在是假裝什麼latency啊 這個processing power都夠 這些東西都不存在 那我覺得到時候就 真的是會很酷 這個可能是一個比較沒有太大 經濟價值的一個例子 一些比較有經濟價值的例子 就是可能是
(45:35~46:35) 像是這個創作者的產業 哇講到我自己要開始失業了 我覺得像是 那種可能你的內容 比較公式化比較單一的 這種創作者呢 其實真的蠻有可能直接被 這個AI去完全的自動處理掉 因為這個AI不但可以產生 影片還可以產生音訊 還可以產生文字 這個字幕對不對 所有的元素他都可以 一步到位這樣子 到時候 我真的很難想像 整個創作者經濟會變成怎麼樣 但我覺得真正會解鎖 超級龐大的經濟價值的 是第二階段 當我們擴增了我們的 模態擴增到動作模態 以及3D模態 這個時候 AI助理 他真的到時候 這真的是可以取代80% 90%的人力了 那所謂的動作模態呢 就是一個機器人他要做什麼的動作 他的手啊他的腳
(46:35~47:35) 要怎麼樣動 這個東西基本上你如果要把它變成 一個Token 讓Transformer可以process 其實也非常單純 對一個機器人來說每一個動作 其實就是全身上下 所有可以動的關節在某一個特定的 角度 你可以把這個動作 做所有的關節 到關節2到關節 30幾40幾 每一個關節他的角度是多少 就代表了這個動作 那我相信這些比較先進的人 型機器人的實驗室像是 特斯拉的Optimus 或者是可能FigureAI 這些公司他們的這些 機器人的動作模型 我相信都已經有在用Transformer 在process這些動作的 這些資料了 這個是FigureAI展示的 這個FigureAI他腦中的 模型還是分開的 就是他處理視覺跟處理 人的語音指令 的是ChatGBT
(47:35~48:35) 其實嚴格來說是GBT4V的 模型 他是這個模型去做API call 去call這個動作模型 那這個動作的模型他單純就是 處理動作 從模態的演變來看 他還在第一階段 他的動作模型跟他的 視覺模型以及這個語言模型 是分開的嘛 我相信他們接下來一定會進入第二階段 然後再進入第三階段 那進入第三階段呢 到這個時候我覺得 那真的是會很恐怖喔 就是一個人型機器人 他可以看得到可以聽得到 可以說話然後也可以動作 你仔細想一想 你就會知道這個東西究竟有多恐怖 他會非常誇張 非常有用 因為人類太多事情 是只需要這幾個東西就可以做到的嘛 對不對 而且他還有一個很厲害的大腦Obviously 就是他上知天文下知地理 那這個東西一旦出現 我覺得一個全民高收入的社會
(48:35~49:35) 絕對會出現 絕對是可能90%以上的 幾率我們會進入一個全民 高收入的社會 全民高收入就是指 全部的人在完全沒有 完全沒有進行任何 勞動的情況之下 都可以得到一份高收入 你可以無條件得到一份 很高的收入 比如說以現在的這個 標準來看可能是 你什麼事都不用做在家裡躺一個月 你就可以進帳十萬塊台幣 之類的 這種全民高收入的社會 就是我覺得這些機器人能夠產生的經濟價值 真的是大到 這些機器人背後的這些公司 或者是這些人 這些財團之類的 他們都已經拿了他們很豐厚的利潤之後 剩下還有一大筆 還是可以全部分給全部的人 然後每個人都拿到十萬塊以上 的這種高收入 我覺得這真的是極度 有可能發生 我覺得我講到這邊有些人可能會覺得
(49:35~50:35) 我共產主義還是什麼的 我覺得你仔細想一想就知道不是了啦 就這個全民高收入 就是一個放大版的社會 社會福利政策而已 我們現在有沒有社會福利政策 有啊我們有勞健保嘛對不對 然後我們有各種 政府補助嘛 這些東西也都是免費的錢啊 你會說我們現在是沒有資本主義嗎 當然不是啊資本主義還是建立在這一層 之上嘛並不是說 有資本主義就不能有社會福利政策 那這所有東西都是一個光譜 那當這個機器人 產生這麼高的經濟價值的時候 我們就算是每個人每個月都拿到 十萬塊的這個基本的高收入 我們在那之上還是有 非常非常多的這種 金錢啊資源啊是你可以去爭取的 那這一部分 就是要爭取的東西呢 就是靠資本主義下去分配嘛 就不是靠這個 共產主義大家都拿一份 好啦我覺得有點扯遠了 反正我只是想講說 我真的是非常非常期待這個
(50:35~51:35) 人型機器人跟 這個多模態模型的發展 那我覺得 如果硬要給個時間軸的話 我覺得可能第一階段 也就是一個模型 同時精通影片圖片文字 音訊這四種模態 我覺得這個第一階段可能在 兩三年之內一定會發生 甚至比較快的話一兩年之內就有了 好的我們真的離這個第一階段 已經很近了那接下來第二階段呢 也就是這個 動作模態跟其他模態的結合 這一邊我覺得 可能也不遠 但是會比第一階段更久一點 我猜可能是 五到十年之內嘛 所以說在努力打拼 存退休金的各位呢 給你們放心一下啦 在五到十年之後你就不用工作了好不好 五到十年之後 機器人養你啊好不好 你現在這個錢就多享受一點啊不要 好啦開玩笑的 我覺得我再講下去就出問題了 我這邊必須非常
(51:35~52:35) 非常嚴肅的跟大家說 我講的這些全部都是我的估計 都是我拍腦袋隨便想出來的估計 我真的沒有太多的根據 沒有人有那麼多的根據 就是預測未來這件事情 誰都說不準 反正我不負任何責任啦好不好 但我是真的覺得這些事情 就算不是在五到十年之內 發生在我們有生之年一定 也看得到絕對的 所以大家可以就是期待一下 好那我們今天這集本文的內容 就到這邊結束 我們接下來來唸一些留言好了 首先是這個 這些都是我自己挑出來的留言啦 就有一些可能 單純是給我鼓勵 或是給我感謝的這些 我當然我都會看 然後我也覺得好棒好棒好喜歡好喜歡 但那些我就不特別唸了啦 因為我唸出來就感覺我在自肥而已嘛 那我來回應一些 首先這一則是 那這一則是Apple Podcasts的 盲盲人海中的
(52:35~53:35) 清流 他留言說 優質節目 好那非常感謝這個 盲盲人海中對於我們這個 給予我們科技獎五星的評價 不過我這邊也要解釋一下 就你仔細去聽那一集啊 我並沒有說這個胺基酸是ATCG啊 就我那時候 是在說 應該是在說DNA吧 胺基酸不是ATCG 這個我也知道 那反正你要我的X帳號 或者是IG 你可以追我的IG啦 那本集的資訊欄最下面也是可以找到 就是Harrispeaks 然後一個底線這樣
(53:35~54:35) H-A-R-R-Y-S-P-E-A-K-S 然後再來下一則這個 艾莉爾Alice她說 她的標題就是一個讚這樣 然後她說近一個月聽到這個Podcast 從第一集追起現在終於 快要跟上大部隊的腳步 內容很紮實 不諱言有些地方 初聽人聽不太明白 需要倒回去多聽幾次來理解 然後 她的留言非常長 所以我直接跳到最後 她說 那我覺得 首先第一個part我會建議你可以去聽聽看 這個科技量EP15 那一集有多講 有講蠻多這個AI相關的法規這樣子 那從那個時候到現在 其實法規這邊 沒有說有太大的進展 所以說就你聽那時候的集數
(54:35~55:35) 也可以大概有個概念這樣 但我必須真的說就是 AI這個技術 真的是太新了 然後它產生內容的這種方式 也是我們前所未見的 所以說你要期待現在 有很完善的智慧財產權 的保護法 在保護AI訓練這邊 我覺得不可能啦 你看不到這種非常完善的 非常有依據的 這種法規 但我覺得你可以參考一些 最近這些關於智慧財產權的一些lawsuit 就比如說OpenAI 講錯這個New York Times告OpenAI 或者是Getty Images 告Stability AI 或者是就是所有這些 相關的lawsuit 你去看一下他們在法院他們是怎麼用 現行的這些法律去 argue這些case 好那接下來一則呢是來自 志祺北七 對不起志祺我不是在說你喔 這個是這個人的名字喔 那他的標題是很好的節目
(55:35~56:35) 他說他是從MULA一介紹 這個節目開始聽的觀眾 一聽就喜歡上這個節目真的非常棒 感謝您製作這麼優質的節目 那他有一個問題就是 EP39有提到Alpha Fold 新版使用一個模型就解決所有的問題 這段講的有點粗略聽不太懂 現在AI幾乎都是 採取多模型的方式進行開發 才能解決單一模型 也沒辦法解決的問題 比如說StableDiffusion的ControlNet 多模型才能解決各種圖片姿勢 產生出更符合使用者 需求的圖片希望可以針對 單模型與多模型的差異 做更多詳細的說明謝謝 好那Alpha Fold這一段 我覺得你應該講的是這個Alpha Fold 2 那Alpha Fold 2它 其實嚴格來說它一樣還是兩個模型 兩個不同的AI模型 組在一起但是呢 這兩個模型它在進行 訓練的時候是兩個是 同時在進行training的 更嚴格來說是 你在做Backpropagation的時候 你的gradient是從第二個
(56:35~57:35) 模型一路push回第一個模型 也就是說它這兩個AI 模型他們的所有的權重在訓練 的時候是同一時間一起 在更新的 同一筆資料它會兩個模型 一起去學它所以你把它 看成一個模型其實也可以 因為它你可以把這兩個模型 試成這同一個模型的兩層 而已因為他們在訓練的時候根本就是 一起訓練的他們只是架構上 有一點點不一樣而已 那你說現在的AI幾乎都是 用多模型的方式進行 開發這邊我 我必須要disagree 就我覺得現在AI發展的趨勢 很明顯就是往 N to N再走往單一模型處理 所有事情再走 像我們今天整集在講的內容 就是我們怎麼從 兩個模型再做多模型 變成一個模型處理多模態 對不對 除了這個以外像是Tesla的FSD 對不對他們現在也慢慢的往 N to N再走然後這個Alpha 4 也慢慢往N to N再走反正
(57:35~58:35) 我們人類已經發現一件事情就是 我們人類的智力真的不足以 去engineer出所有 的問題然後我們所有engineer 出來的很複雜的system 越複雜就是越 over engineer我們應該要 越來越簡單化然後越來越 把越來越多的事情 交給AI自己去學 不要讓我們人類插手 所以比起我們人類先去定義出 好幾個不同的AI模型然後去 學不同的東西最後再把它合在一起 不如就直接把 所有東西想辦法就是 用同一個管道 丟進這個模型裡面然後讓這一個模型 自己去學所有的東西這樣子 那當然也不是所有 問題都可以這樣解但我們 是往這個方向再走然後你 給出的這個例子其實也是 未來會被單一模型 取代的就是你說 的這個StableDiffusion會需要 ControlNet這個模型 才可以解決各種圖片姿勢 然後產出更符合使用者需求 的圖片那確實
(58:35~59:35) 就是你現在要用StableDiffusion 產出一個特定姿勢的圖片 你會需要ControlNet這另外一個 這個神經網路去 給StableDiffusion一個 額外的圖片condition對不對 額外的一個input這樣子 但你聽完了今天這一集的 科技浪你應該就知道我要講什麼了吧 就是這件事情 接下來會被一個模型解決 哪一個模型就是 Camellia嗎就是那個 可以同時處理文字跟 圖片模態的模型啊 你如果不信的話你現在去這個 GPT-4O的這個blockpost 你去往下滑你就會看到 它其實OpenNet已經有使用 GPT-4O做出類似的事情了 就是它單靠文字跟圖片的input 去把一個 原本是這個 一個真人版的圖片變成一個卡通版 好那這個ControlNet的例子 一樣啊你就拿一個 一個人做一個姿勢一個姿勢的這個 圖片然後旁邊的 文字prompt去打說根據 這個姿勢去做一個
(59:35~60:35) 欸我不知道 一隻狗狗的圖片 然後它就會 做到這個原本要用 ControlNet加StableDiffusion才能做到的事情 你直接靠一個 GPT-4O你就可以做到了 那當然我覺得短期 這個GPT-4O能做到或者是 Camellia能做到的一定沒有 StableDiffusion加ControlNet這麼好 尤其是現在 StableDiffusion就是其實 已經進步到很後面了啦 但這些EarlyFusion的多模態模型 都是還比較初期的階段 但我覺得最終 這個EarlyFusion的 模型一定會贏過 StableDiffusion加ControlNet 這就是我們一再看到的狀況 一個模型可以打贏多模型 這就是我們一再看到的狀況 好那我們今天留言就回復到這邊 剩下的Spotify的YouTube 我們就等到下一次再回復 好那如果你喜歡今天 這集Podcast一定要幫我五星評分 然後可以留個言跟我 互動一下然後也不要忘了
(60:35~60:47) 把科技浪Podcast分享給你的親朋好友們 幫助我們科技浪Podcast 的壯大 那最後呢也感謝一下今天的 贊助商AWS 然後祝大家有個愉快的一周
