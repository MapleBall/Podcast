(00:00~01:01) 【音樂】 哈囉大家好,歡迎收聽科技浪,我是主持人哈利 科技浪是一個白話跟你聊科技的Podcast 希望可以用簡單易懂,但是又深入的方式 帶你了解時下最火的科技話題 本集節目由奧加堡贊助播出 我相信很多科技浪的聽眾應該跟我一樣 平時的工作就是要長時間的盯著電腦看 這種生活形態長時間下來一定會感到各種酸澀不適 這個時候一個不錯的解方就是吃保健食品 今天就是要跟大家推薦奧加堡的兩支保健食品 分別是金盞花葉黃素加紅棗蝦紅素加B5的這支產品 以及三倍濃縮魚油 首先這個葉黃素加蝦紅素加B5 你聽它的名字就知道它跟一般的葉黃素不一樣 它還添加了保護力比葉黃素還高2.6倍的蝦紅素 這個可以幫助你放鬆
(01:01~02:05) 葉黃素跟玉米黃素採的是5比1的黃金比例 可以有效幫你舒緩酸澀 至於三倍濃縮魚油的產品 它一顆裡面就內含900毫克的Omega 3 也就是說你一天只要吃一顆就可以抵三顆的一般魚油 我們知道魚油根據不同的萃取技術 可以分成三種不同的型態 有TG型、E型跟RTG型 這三種型態之中 Omega 3吸收率最佳的就是RTG型 我們今天介紹的這個奧加堡的三倍濃縮魚油 就是RTG型的魚油 這款魚油真的是很多上班族都應該吃 尤其是這個健檢紅字族群真的是必備 它裡面還有的EPA可以幫助新陳代謝 DHA可以幫助你的思緒靈活 今天這兩個產品會包在一起賣 是因為葉黃素它是屬於脂溶性的營養 所以它搭配魚油一起吃的話 可以更能夠幫助吸收可以加強循環力 這個品牌奧加堡是有近百年歷史 也是澳洲的第一大保健食品的品牌
(02:05~03:05) 我在這之前其實也都是吃奧加堡的保健食品 真的是可以信得過的大品牌 他們的產品皆有通過30項以上的國際標準檢驗 絕對不含重金屬塑化劑 產品也全部都是澳洲原裝進口的安全油保障 這次科技浪跟奧加堡合作 獨家推出了水潤護明兩個月體驗組 限定六五折 非常高的折扣提供給科技浪的聽眾們 裡面是有包含三倍濃縮魚油跟葉黃素各60顆 還沒有在吃魚油跟葉黃素的朋友們 可以考慮用我們的獨家優惠去體驗一下 如果你平時就有在吃魚油跟葉黃素的話 奧加堡這次也有推出三入的最低八折方案 購買連結一樣在資訊欄裡 點進去就可以立刻購買了 最後也提醒大家一下 科技浪聽眾限定的六五折兩個月體驗組 活動期間只有到3月31號 所以說要買的要趕緊下手 除了這個活動以外
(03:05~04:05) 現在還有新客首單限折100塊 消費滿3000再送奧加堡維生素C30顆 本集業配就到這邊結束 謝謝奧加堡的贊助 好的那這禮拜呢 又是科技新聞滿滿的一個禮拜 雖然說你從標題應該就知道 我們今天主要要聊的就是這個 NVIDIA的GTC大會 必須我覺得它必須是我們今天的主要主題 因為NVIDIA真的是 我身為一個聊AI的Podcaster NVIDIA有發表會我怎麼可能不講 對吧但是我覺得這禮拜呢 也有一些其他有趣的一些 其他的小科技新聞 所以說我覺得我們今天一開始呢 我們就先來先聊一些這些 其他的科技新聞 然後待會再進入這個NVIDIAGTC的深入解析 那首先呢就是馬斯克的腦內晶片公司 Neuralink他們在上禮拜呢 有了他們最新的update 他們這個1號病人的update 因為你以後在聽科技講就會知道 就是他們在今年2月初的時候呢
(04:05~05:05) 他們進行了有史以來第一次的人體實驗 就是有史以來的第一次有一個真的人類呢 在腦袋中植入了這個M1晶片 那我們外界對於這整個人體實驗的過程呢 了解是非常少的 我們唯一知道的資訊呢 就是馬斯克的幾則Tweet 馬斯克的幾則X文 大概就是這樣 他就是發文說 第一個人類植入了腦內晶片 然後說他現在正在復原當中這樣 然後這個晶片呢 他現在運作的是挺正常的 然後我們大概就是知道這些資訊而已 就是這個Neuralink的官方呢 他們並沒有發布任何的消息 那當時呢就有一些人就開始有點懷疑 就是為什麼你們什麼東西都沒有公開 這個人還好的嗎 他是不是有些什麼重大缺失呢 甚至他死掉了是不是 你們都不公布 但是在上禮拜呢 Neuralink他們做了一個官方的直播 就是他們其中一個Neuralink的researcher
(05:05~06:06) 他就拿著手機直接開一個X上面的直播 然後跟這個植入腦內晶片的第一個病人呢 做一個互動這樣 那在這個直播的過程中呢 首先我們可以看出這個病人現在是還活著 很明顯嘛 而且他活得很好 很明顯這個手術是非常成功的 沒有留下什麼後遺症 這個是他自己也有講的 那再來呢 我們也了解到了這個病人的一些背景 然後他現在可以做到什麼事這樣 那他的名字呢 叫做Noland 那他在七年前的一次潛水的過程中呢 出了一個意外 讓他從肩膀以下呢是完全的癱瘓 然後完全沒有知覺 完全不能動這樣 所以說呢 他基本上就是一輩子要坐輪椅這樣 那他後來呢就是去報名了這個Neuralink的人體實驗嘛 然後他自己也有說 就是這個開刀過程啊是非常的順利的 他甚至是隔天就可以直接出院了 然後現在呢也是完全的就是康復了這樣 然後這個Neuralink的晶片呢
(06:06~07:06) 現在在他的腦袋中呢 也是非常正常運作的 那從這個直播中呢你就可以看到說 他在跟這個Neuralink的researcher互動的時候 整個過程他都是一直在玩chess 一直在玩西洋棋 然後他就是用他的腦波在玩 用心電感應在玩西洋棋這樣 就是真的是這個 因為這個researcher他在直播的時候 他會不停的切換他的鏡頭前後 所以說我們偶爾會看到他這個Neuralink他的電腦的畫面 偶爾是看到Neuralink的在講話這樣 那切到電腦畫面的時候你就會看到說 哇!Neuralink真的是用心電感應在控制電腦的鼠標喔 然後一直在這個下棋這樣 好那個感覺真的是 我覺得你如果是Neuralink本人你一定會真的是覺得 這是一個魔法 他自己也有說就是feels like magic 但是光是用看的 我就已經覺得真的很神奇喔 就是用腦袋在去控制這些事情這樣 就是你可以看到說
(07:06~08:07) 他電腦原本是在播一個微弱的音樂這樣 然後那個researcher就跟他說 Can you pause the music? 就把那個音樂停一下這樣 然後就看到他滑鼠就是移到上面 然後把那個音樂給暫停這樣 就覺得哇靠真的是太酷了 那我覺得從這次直播中呢 我們可以看到很重要的一點就是 M1的晶片現在已經成功在人類身上 做出了神經元的記錄這樣子的一個功能 那如果你有去聽科技量的EP26 你就會知道就是這個腦內晶片呢 他主要要做兩件事情 就是記錄神經元的活動 以及刺激神經元 那我們今天看到Nolan可以用心電感應去控制電腦的滑鼠呢 就代表說這個M1已經有把Nolan腦部的這些 重要的神經元活動呢成功的記錄了下來 並且傳給了外部的裝置 然後這個外部的裝置呢 我覺得很可能就是Nolan眼前的那台MacBook 他就會跑一個機器學習的演算法 把Nolan的腦內活動呢 Decode解碼成一個
(08:07~09:07) 解碼成這個電腦滑鼠的Movement 然後就等於是Nolan用心電感應在控制電腦滑鼠這樣 那M1晶片已經可以成功的記錄神經元活動這件事情呢 當然是非常棒 非常可喜可賀 恭喜這個Neuralink 但是呢很多真正這個腦內晶片的殺手機應用 是要靠這個M1刺激神經元的功能 單純記錄神經元的活動是不夠的 像是這個讓四肢癱瘓的人呢 能夠重新的使用他的手腳 重新讓他自己的手腳動起來 然後或者是讓這個看不到的人呢 眼盲的人呢可以獲得視覺 就算你一出生就眼盲 一出生從來沒有看過這個世界 你也可以靠著Neuralink得到視覺 那這些全部都是要靠著M1晶片刺激神經元的功能 那這部分的功能呢 當然應該就還是還在開發中啦 畢竟我們也沒有看到直播中 這個Nolan實際站起來嘛 但是我覺得我真的覺得 距離我們看到這個Nolan
(09:07~10:07) 可以實際站起來走到西洋棋桌前下西洋棋 以及我們看到一個眼盲的人呢 可以實際看到 我們距離這種未來啊真的不遠了 因為這個M1晶片呢 已經可以正常植入一個人腦 而且已經可以正常的在腦袋中運作 紀錄神經元的活動 我們真的差這個刺激這一步 我們就可以讓這些應用變成現實 而且我們也看到了Neuralink 在這個猴子跟豬上面的實驗 這邊都是蠻成功的 所以真的是非常恭喜Neuralink 這絕對會是一個改變世界的技術 然後呢這個 你如果想要多多了解 腦內晶片是怎麼運作的 我剛剛講的是一個非常高層的介紹 你想聽到更詳細的介紹 包括它究竟是如何刺激神經元 有如何紀錄神經元活動的 然後這些殺手機應用呢 究竟是怎麼做到的 你可以去聽科技浪Podcast的EP26 我有做一個蠻詳細的介紹 那除了這個Neuralink的新聞以外呢 馬斯克這邊另外一間公司
(10:07~11:07) 也有一個大新聞 就是它的XAI這間公司 XAI這間公司呢 跟OpenAI,Mixtrol這些公司一樣 就是做大型元模型的一間AI公司 那他們的這個招牌大型元模型呢 是一個叫做GROCK的模型 GROQ 啊講錯 GROK GROQ是那個晶片這樣 那他們在這之前呢 GROCK一直都是幣元的一個模型 他們並沒有公布GROCK的 很多的這個技術細節 然後也沒有公布這個模型本身 但是在上禮拜呢 馬斯克把GROCK給開源了 包括這個模型本身的權重 也就是這個模型的大腦這樣 然後以及這個模型的程式碼 你要使用它的程式碼 全部都是公開在網路上給大家下載使用的 那他們使用的license是 APACHE 2.0的license 那你如果知道的話 APACHE 2.0基本上是一個 非常非常permissive的一個license
(11:07~12:07) 就是它對於這個你使用它的模型 它的程式碼的這個限制呢 是非常寬鬆的 甚至是商用都可以喔 也就是說你可以把這個GROCK build到你的產品中 build到你的app裡面這樣 那馬斯克為什麼會突然把他們的大型模型給開源呢 我認為大多數人 他們認為的理由呢 應該就是馬斯克的面子掛不住 覺得自己有點hypercritical 也像是未君子的感覺 為什麼呢 因為前一陣子馬斯克不是告OpenAI嗎 我記得我們上禮拜 還是上上禮拜的科技狼也有講到這件事嘛 他告OpenAI說 你們一開始成立的時候是以一個OpenAI為理念 以一個開源為初衷 但是現在呢 你們都不開源你們的模型了 你們應該要改名成CloseAI 但是很多人呢 就開始指責馬斯克說 你自己的XAI公司都完全沒有開源GROCK模型 你憑什麼講人家 然後過不了多久呢 馬斯克就發了一篇X文說 GROCK之後會變成開源的 然後確實呢
(12:07~13:07) 然後確實呢 在上禮拜他們就真的做到了這件事情 把那個GROCK的GitHub給公開出來了 那大部分人應該就是覺得 就是馬斯克面子掛不住這樣 但其實我覺得從一個business的角度 這件事情也有它的道理在 因為你仔細看這個GROCK的模型 它是一個330 billion參數的一個MOE model MOE model就是Mixer of Experts的model這樣 那就是一種模型的架構這樣 它是一個330 billion參數的MOE 但是它的表現呢 跟Mixedrol也就是這個Mixedrol這間公司出的 一個大概47 billion參數的MOE模型 表現是差不多的 也就是說GROCK這個模型啊 它的表現跟腦容量比它小7倍的模型的表現是差不多的 那這個很明顯呢 感覺是出了一點問題嘛 那這個問題是出現在哪裡呢 如果是出現在這個訓練資料上呢
(13:07~14:08) 這個開源社群也可能比較沒轍一點 但如果它的架構上可能是有一些可以改進的地方 那這個就是開源社群可以幫助到的地方了 你把這個GROCK釋出把它開源 等於是全天下的人同時都可以幫你一起debug你的模型 讓你的模型可以進步這樣 所以說這個XAI釋出他們的GROCK模型 很可能也是要讓大家可以一起集思廣益 讓GROCK可以進步的更快這樣 像是這個Google前一陣子也釋出了他們的Gemma模型嘛 那這個Gemma模型就是他們的一個小語言模型這樣 那它釋出了之後呢 這個開源社群很快就發現說 很多的這些implementation的細節啊 是可以進行微調的 是可以更好的 然後這個Gemma模型呢就得到了進步 那GROCK模型呢可能也是可以因此得到進步 尤其是他們才剛釋出不到一個禮拜吧 那我剛剛去看一下他們的GitHub 他們已經有43000個GitHub Star了
(14:08~15:08) 你要知道這個43000個GitHub Star 對於一個GitHub的專案來說 已經算是一個非常高的一個數字了 已經算是很有名了 尤其你在一個禮拜之內就達到這個數字 所以說是很多人在關注GROCK 那它一定也會進步的很快這樣 好的那我們今天其他閒聊的新聞呢就聊到這邊 我們趕快進入今天的正題 也就是NVIDIA的GTC 那NVIDIA呢它每年或者是每半年 就會舉辦一場這個叫做GTC的發表大會 那這個發表大會的全名呢是叫做NVIDIA GPU Technology Conference GTC 但是最近這一兩年呢 這個NVIDIA的GTC啊基本上就是一個AI conference 你去點他們的官網 他們的官網自己就這麼寫 就是他們這是一年一度的AI conference這樣 因為他們發表的內容呢幾乎全部都離不開AI 全部都在講AI 那這個GTC的發表大會呢 有時候是一年一度 有時候是半年一度
(15:08~16:08) 像是它2022年就辦了兩場 分別是在3月跟9月 那它2023年呢去年呢它只有辦一場 然後今年呢是剛辦了第一場 不確定會不會有第二場這樣 那你大家要知道就是 它一年辦一場其實已經非常厲害了 應該說一定要一年辦一場 不然你一年都沒有開發成果 那還得了 但它有時候可以一年辦兩場 真的是非常非常厲害的喔 因為你看其他的科技巨頭 像是這個蘋果他們每一年就是只有一次的WDDC嘛 你看Google每年就是只有一次的IO 他們不可能每年辦兩次嘛 但是NVIDIA他們的開發速度既然快到 有時候可以一年辦兩次 其實真的是非常厲害的喔 尤其是他們在開發的這個東西GPU啊 真的是非常非常複雜的一個東西 然後他們也是走在產業的最最前緣 所以說他們有時候可以一年辦兩場 真的是很厲害了 大家真的是不能小看NVIDIA的Research的能力
(16:08~17:09) 那這個今年的GTC呢 我覺得發表的重點就是三個 然後依照重要度排名呢 最重要的最重磅的發表呢 當然就是Blackwell的架構 他們最新的GPU架構 Blackwell 那第二個重要的呢 是機器人相關的發表 那這邊當然有很多東西 從他們的這個Foundation Model Group 到他們的什麼Isaac Lab的一些新的東西 那這些我們待會再講 反正就是一些機器人相關的發表這樣 那最後呢第三重要的呢 我覺得就是他們有一些生成式AI的Micro Services 就是一些企業可以直接使用的API這樣 那這邊我就覺得比較還好 我覺得最值得聊的就是上面這兩個 就是這個Blackwell的架構跟機器人這樣 那我們就先從第一個Blackwell的架構開始好了 那這邊呢就是我必須先講啊 就是這些GPU的技術啊 我覺得技術門檻是比較高一點 我覺得一般沒有在注意的人呢
(17:09~18:10) 可能要聽懂我們接下來要講的所有東西 是蠻困難的 尤其是我想講到一些 是一般媒體是沒有注意到的東西 也就是說我會講的蠻深的這樣 但是呢我也是會盡量讓大家可以帶走一些東西 所以說我也會就是一開始盡量用一些 可能比較白話的方式去解釋一些高層次的概念 那如果你已經很熟GPU的話 就不要覺得不耐煩 我很快的就會進入到很細節的技術討論了 好那這次NVIDIA發布的呢 是他們新的GPU架構叫做Blackwell 也就是發布了一系列這些叫做Blackwell的GPU這樣 一直以來他們這些GPU架構的命名啊 都是用這些有名的科學家來命名的嘛 那這次的Blackwell呢是出自一個叫做 David Blackwell的美國數學家這樣 那他們這次推出了很多Blackwell的GPU產品嘛 那他們最主打的然後也是最基本 基本款的一個Blackwell GPU呢 是一個叫做B200的GPU
(18:10~19:13) 那這個是跟去年有點不一樣的地方喔 就是去年主打的是Hopper架構的這個GPU嘛 很多人應該都就是有在Follow AI的應該都知道 去年大家都在瘋搶H100的GPU嘛 那去年主打的這個Hopper架構主打的呢 就是這個H100的GPU 然後後來呢才在2024年他們做出了一個新的 H100的進化版叫做H200的GPU 但是這次他這個Blackwell的架構呢 他一開始主打的基本款呢就是B200的GPU了 那反而是他還是有推出B100的GPU 但B100的GPU呢基本上是弱化版的B200 這不是他要主打的GPU 這只是為了跟原本的H100的一些伺服器可以相容這樣 那為什麼會有這樣子的差異呢 我們待會會再講到 那接下來呢這個基本款的B200之後呢 他們有把這個兩個B200跟一個Grace CPU Grace CPU就是NVIDIA他們自家開發的一個CPU 把他們這三個東西結合在一起 這邊的GB當然就是Grace跟Blackwell
(19:13~20:13) 那再來你可以再把36個GB200的SuperChip 組合在一起變成一個GB200MVL72的一個Rack 他是整個伺服器的架子這樣 那我覺得他們最主要推出的產品就是這三個 那當然這三個中間還有一些就是組合的 就是他們的GPU 然後還有他們的GPU 那當然這三個中間還有一些就是組合上還有一些其他的步驟 然後這三個之後也可以繼續Scale 就是你堆成一個這個GB200MVL72的一個Rack之後呢 你還可以把很多這些Rack組合在一起 再變成一個超級電腦 然後再把好多超級電腦組合在一起 變成一個超級大Data Center whatever 但反正那邊我們就先不講 你想去看的你可以去看他們發表會Keynote裡面的一段動畫 他們都會做這些超酷的這個GPU的動畫 就是我也不知道他們怎麼做的 我覺得真的是超酷的 然後也解釋的非常清楚 你們可以再去看那個
(20:13~21:14) 那個是比較詳細的 但我覺得最主要講的就是可以講這三個單位 因為這三個是可能比較重要的 我再recap一下 首先就是這個B200的一個GPU嘛 然後再把兩個B200放在一起 再加上一個Grace的CPU 變成一個GB200的Super Chip 然後再把36個GB200的Super Chip合在一起 變成一個GB200MVL72的一個Rack 那很明顯的呢 你可以看出這個B200的GPU啊 就是他們所有系統的這個Building Block 都是從這個B200GPU開始 去慢慢把它疊出更強大的GPU這樣 那我們就先從這個B200的GPU做個詳細的介紹 那這個B200的GPU啊 它究竟表現如何呢 你如果去看老黃的Keynote 你就會聽到他說 這個B200的GPU跟上一代比起來 真的是一個很大很大的大躍進啊 非常大的一個進步 他們在Keynote裡面呢 是把B200跟H100做比較 然後他說 首先這個電晶體的數量呢
(21:14~22:16) 變成了2.6倍 這個電晶體是什麼呢 電晶體就是這個電腦啊處理器裡面 它這些能夠代表0或者是1的這些小開關 一個電晶體呢 就是可以代表一個0或者是一個1這樣 那我們知道電腦所有的檔案 所有的操作所有的運算 全部都是以0跟1在進行的嘛 就是它最最原始的狀態呢 都是這些0跟1在動 那也就是說這個電腦在運算的時候呢 就是在不停的翻這些電晶體 把它們從0切換到1 再切換到0再切換到1 然後來做這些運算這樣 那之所以這樣子就可以做運算 是因為一些電晶體呢 它們可以組合在一起變成一些logic gate 就比如說它們可以perform一些簡單的 邏輯運算像是and or not這種運算 就比如說and就是 前面兩個input都是1的話 它的output就是1不然output就是0 大概是這樣 那很多這些基本的logic gate
(22:16~23:17) 又可以在一起組合在一起變成一個複雜的一些logic 然後這些複雜logic又可以再組合在一起變成一些 更複雜的我們叫做instruction 更複雜的這些指示 反正一層一層上去 你終究會來到我們寫code程式語言的level 講了這麼多我其實只是想講說 這個電晶體就是電腦運算的一個 可以說是最小最小的單位吧 那這個電晶體的數量有多少 它就直接的決定了這個電腦有多大的算力 就你同時有的這些0跟1比較多 你就可以做比較多的運算 你就可以處理比較大量的資料這樣 那這個B200它跟上一代H100比起來 它電晶體數量是變成了2.6倍 這個H100它是有80 billion個電晶體 那這個B200它有208 billion個電晶體 那當然這樣子的進步聽起來是非常的可觀嘛 非常誇張的一個進步
(23:17~24:22) 但是這樣子的比較其實是有一點點不公平的 當然從NVIDIA的角度出發 他們就是使用這一代的主打的基本款B200 跟上一代主打的基本款H100做比較 這樣子比較從一個產品的定位來看算是蠻公平的嘛 但是其實這是有一點點不公平的比較 這個是因為這個B200它其實是兩塊裸金合在一起 然後H100是只有一塊裸金 那這邊是什麼意思呢 我們待會會再詳細的講 我們先看一下它們算力的差別 首先我們再評估一個處理器的算力 我們是用一個叫做FLOPs的單位 FLOPS 它是Floating Point Operation Per Second的一個簡稱 意思就是說它每秒可以做的浮點數運算有多少次 你可以把一個浮點數運算想像成兩個有小數點的數字相乘 這樣就是一個浮點數運算 比如說2.2乘1.5這樣 從老黃的Keynote裡面你就會聽到說
(24:22~25:23) 老黃說這個B200跟H100比起來它的FLOPs進步了五倍 這個H100它理論最大值是四個Petaflops 一個Petaflops就是十的十五次方個FLOPs 就是一秒它可以做十的十五次方次的浮點數運算 那H100是四個Petaflops B200可以做到二十Petaflops 這個是五倍的進步喔 那你會說這是不是有點奇怪 因為它的電晶體只進步了2.6倍 那為什麼它的FLOPs可以進步五倍呢 那這邊當然也是有一些不公平的地方 就是我覺得他們是用了一些Trick這樣 那這些Trick基本上就是它降低了這個資料的精度 那這邊具體差在哪裡我待會會再講 但反正你現在可以先把它想像成 這個B200它做的這些運算它的Loading是比較低的 H100它的運算Loading是比較高的 那假設你做相同的運算呢
(25:23~26:23) 就是用完全一模一樣的精度去做運算的話 其實B200它理論最大值也就只有十個Petaflops而已 也就差不多就是這個H100的2.5倍的進步 也就是跟它這個電晶體數量的變多是差不多的這樣 那這邊比較是單個晶片單個GPU之間做的效能的比較 那老黃也有做整個系統的比較 就是你把這些一大堆GPU組在一起 去訓練一個超大AI模型的時候的效率的差異 具體來說老黃在比較的是 你在訓練一個1.8 Trillion參數的MOE模型 這個MOE的大型圓模型的時候 你使用Hopper跟Blackwell的差別 那它說你使用Hopper來做這件事情的話 你會需要8000個H100的GPU 8000個Hopper的GPU 然後你要訓練90天 然後你會花上15MW的能量
(26:23~27:23) 那假設你今天是使用Blackwell的GPUB200的話呢 你只需要2000個就好了 從8000個變2000個 然後一樣是在90天內 三個月內可以Train完 然後你只需要花上4MW的能量 相較於Hopper的15MW是降低非常多的 也就是說在一樣的時間內 訓練一個一樣大的大型圓模型呢 Blackwell只要花上Hopper1%的GPU 然後1%的能耗就可以做到這件事情了 那很明顯這也是進步了超級多嘛 那我們接下來就來看看老黃講的這些超級大進步 不管是這個GPULevel的比較 還是這個系統Level比較的大進步 究竟是來自哪裡呢? 通常我們想到這個晶片的進步 尤其是這個電晶體數量的增加呢 我們直接聯想到的是製程上的進步 就是這個我們常聽到的這些 可能什麼7奈米、5奈米、4奈米、3奈米製程 越來越小的製程
(27:23~28:25) 那這些製程越來越小、越來越先進 它代表的呢就是 它可以把越來越小的電晶體 印在這個晶片上面 那你這些電晶體越來越小 你當然在同一個面積下 你可以塞越來越多的電晶體 那這個時候儘管你的晶片大小沒有增加多少 你的電晶體數量呢也是可以增加很多 因為你是可以靠著這個製程的 更先進的製程去增加你的電晶體這樣 那我們俗稱的這個摩爾定律呢 在講的也是這個製程上的進步 但我們回來看這個BlackWall的晶片 BlackWall的晶片它的進步並不是來自製程上的進步 它用的製程跟上一代Hopper用的製程是一樣的 一樣是台積電4奈米的製程 嚴格來說是4Np的製程 是這個4奈米的一種refinement 那既然製程上沒有進步 它只能靠這個晶片的大小來取得進步 就是你製程一樣你能印出來的這些 feature 這些電晶體的這個size是一樣的 所以說你在同一個晶片的這個面積之下呢
(28:25~29:26) 你能塞的這個電晶體數量是一樣多的 所以說你要進步你只能把你的晶片做得更大 對吧,你就有更多電晶體了嘛 但是你要直接做一個比H100大非常多的晶片 可以做到嗎?其實不行喔 這是因為這個H100它的die size 其實已經很接近理論最大值了 那這個die size是什麼呢? 首先一個die我們在講的 中文翻成這個裸晶 我們在講的就是這個在封裝之前的這個晶片 你還沒有被封裝 封裝之後它就變成一個chip叫做IC 但是在封裝之前呢 我們就把它稱作一個裸晶這樣 那這個H100它的裸晶的大小 它的die size是814毫米平方 那我們這個艾斯摩爾的EUV機器呢 我們知道這些先進製程啊 就可能台積電4奈米的這種製程 我們都是需要用到艾斯摩爾的最先進的光刻機器 來進行光刻這個lithography的過程這樣
(29:26~30:26) 那這個EUV機器呢 它的理論最大值是8508毫米平方 也就是說這個H100它的die size 其實已經非常接近理論最大值了 那也就是說這個今天你單純要做一個這個更大的晶片呢 其實進步空間也不大了 你在最大的die size也大不了這個H100多少嘛 所以說這個時候呢 NVIDIA就剩下一個方法 就是把兩個die拼在一起 直接把這個die size變成兩倍 所以說嚴格來說啊 這個Blackwild的GPU呢 它其實是兩張晶片合在一起 然後H100是一張晶片 所以說就是當然這個Blackwild的GPU呢 它by default它的這個電競體數量呢 就會至少是兩倍這樣 那它可能die size也有變大一點點啊 所以說它整體這個電競體數量的增加呢 是2.6倍 那根據老黃的說法呢 Blackwild GPU的這兩個die 它們之間溝通資料的速度是10TB per second
(30:26~31:26) 所以說基本上是非常快的 基本上你可以把這兩塊想像成同一塊GPU這樣 因為它太快了 它中間溝通資料速度太快了 老黃有說一句就是說 這兩個die They don't even know which side they are on 他們根本不知道它自己在哪一邊 因為這兩邊感覺就是合在一起變成一塊這樣 所以這個B200的GPU呢 它電競體數量之所以會多這麼多 就是因為它是兩塊晶片合在一起 那這也是為什麼我一開始說 它們電競體的數量這樣子直接比較是有一點點不公平的 那剛剛有說到就是 B200呢 它還有另外一個這個進步的來源呢 是來自於計算精度的下降 那首先呢 我們知道在電腦的世界中呢 所有的資料呢都是二進制的 我們就是都是用0跟1來代表嘛 那一個0或1這樣子的一個數字呢 我們會把它稱作一個bit 然後這個8個bit會組成一個byte 然後很多很多個byte 我忘記可能1024個byte
(31:26~32:27) 你就說它是一個kb 然後在1024個kb 就會你把它說這是一個mb 這邊就進入大家可能比較熟悉的這個答案格式了嘛 答案大小 那我們在運算AI的時候呢 我們要進行極大量的浮點數運算 這些非常基本的運算 像是1.1乘以0.9 然後再加0.3之類的這種簡單的數學運算 我們要做極大量的這種數學運算 那這些數學運算中的每一個數字 你要用幾個bit來代表 其實是你可以控制的 假設最標準的 就是你用32個bit來代表一個數字 也就是說每一個數字呢 你電腦會需要分配32個bit給它來代表這個數字 那你有這麼多個bit來代表一個數字 你能代表的所有可能的數字數量 當然就是非常非常大嘛 因為你想想看就是 每個bit都有兩種可能性 然後你有32個
(32:27~33:27) 那這樣可以做出多少種排列組合 非常非常多種嘛 那你當然就是實際上也不是直接 就是進行排列組合 這些data format呢是有很多學問在裡面的 就是你可能分配一個bit來代表它的正負 然後分配幾個bit來代表這個數字的規模大小 然後用幾個bit來代表這個數字 可能小數點的部分 比較精確的數字的部分這樣 那反正這邊細節我就不講了 那反正我們在運算AI的時候呢 最基本最精準的這個資料精度呢 就是這個FP32 也就是32bit的浮點數 但是你每一個數字都要佔用32個bit 也就是32個電晶體 這樣其實是真的蠻佔記憶體的 然後也是運算效率也沒有那麼高對不對 然後說真的我們真的要這麼高的精度嗎 就是你對於一個這個有 一兆個parameter 就是一兆個參數的這個AI模型來說
(33:27~34:27) 它其中一個參數是 0.3128764578 跟它是0.3 欸有差嗎 沒有什麼太大差別吧 所以我們其實真的是可以犧牲一點精度來換取更高的 更大的這個運算效率這樣 那這個Hopper的GPU 它能支援到的最低資料精度呢 是FP8 也就是說它用8個bit來代表一個浮點數 但是這次呢BlackWild的GPU呢 它可以支援到更低的精度 它可以支援到6FP6 跟FP4 最低是FP4喔 用4個bit來代表一個浮點數這樣 那其實這個老黃在Kino裡面講的就是這個 這個BlackWild它的GPU有20個petaflops的算力 這個其實是使用FP4的精度在算 那它是跟H100使用FP8的精度 然後做到的這個4petaflops在比 那這樣子的比較當然就不是Apple to Apple的比較
(34:27~35:29) 這樣是不太公平的 因為我們知道這個精度你只要下降一倍 你的petaflops就會直接上升一倍嘛 因為你每個數字要使用的這個電晶體的數量 直接少了一半嘛 人家是用每個數字用8個bit來代表 你每個數字直接用4個bit而已 那你的flops當然是直接是人家兩倍啊 所以說我剛剛有說了就是這個 真正公平的比較呢是比較這個G200跟H100 他們在FP8 同樣用FP8的精度在算的時候的petaflops 那這個時候呢 這個G200呢他的petaflops就會從20petaflops降到10petaflops 那這個就是合理的一個數字 因為它的電晶體是人家的2.5倍2.6倍 所以說你的petaflops當然也是差不多人家的2.5倍 但我必須要講清楚喔就是 這個G200跟H100的比較flops的比較有一點作弊沒錯
(35:29~36:34) 但我並不是說G200這個能夠支援FP4跟FP6的精度 是沒有什麼了不起的 其實是挺了不起的喔 因為你要知道就是你從一個高的精度換到一個低的精度 你對於記憶體的你記憶體要求的降低跟你flops的增加都是線性的 也就是說你這個精度減少一倍呢乘以二分之一 你的這個記憶體要求就直接乘以二分之一 然後你的flops就直接乘以二 這些都是線性的 但是呢一個很大的重點是 模型的表現並不是線性的降低 就是這個模型它的智商呢不會直接降低一倍 它不會降了那麼多其實 就是比如說你拿一張圖片來比較好了 一張數位的圖片像是假設一張狗的圖片好了 你今天把你的解析度從可能4K降到2K可能就降低一半這樣 那對你來說呢你在看這張照片時候的感受 你感受到了這個清楚度有降低一半嗎 我覺得應該沒有吧
(36:34~37:36) 應該察覺不到就是應該察覺得到一點點但是察覺不到太大的變化 但是對於這個電腦的記憶體來說呢它是直接降低一半的 那這次NVIDIA BlackWire GPU它可以支援的FP6跟FP4 它的資料型態其實是一個很特別很厲害的一個資料型態 叫做Micro Scaling Data Formats 我們把它簡稱MX Data Formats 這個MX Data Format它是這個很多家科技巨頭聯合做出來的一個新的資料型態 包括這個微軟AMD Intel MEDA NVIDIA Qualcomm 他們集在一起找就是世界各地的專家出來集思廣益做出來的一個新型的資料型態 那這個MX的資料型態呢完全就是為了AI而設計的 嚴格來說是為了這個深度學習而設計的 也就是說對於這個深度學習的應用來說 他們要怎麼樣合理的分配這四個bits或是這六個bits 才可以達到最高的效果最好的效果這樣
(37:36~38:36) 就是比如說這個對於一個神經網路來說呢 它的這些權重它這些參數呢通常都是介於可能負二到正二之間 所以說你並不需要去很好的代表大於十的數字 因為一個wait通常不會到十啊 你不需要就是有很多的bits來代表比十更大的數字 你可以把這些你有的這些bits更妥善的分配在負二到二之間這樣 那當然這是我自己的一個舉例啦 他們做的應該是類似這樣子的事情 但是他們做的應該是更複雜 說真的他們有把他們的paper放到網路上 放到這個archive的網站上面 但是我還沒有細看我只有看他們的diagram 就是他們的圖 就是這個就是看論文的一個小技巧就是 你如果沒有時間看全部的話 你直接看他的圖就好了 所以我就只有看他的圖 所以我不確定他們到底是怎麼做的 但是他們應該就是用這樣子的概念這樣 那你如果也有去看這篇論文的這些diagram這些table的話
(38:36~39:36) 喔對了先講一下 這篇論文的名字叫做Micro Scaling Data Format for Deep Learning 所以說大家自己也可以去找一下這篇論文 然後你去看他的這些table 你就會發現說 其實在Inference就是這個AI的推論 跟AI的training AI的訓練上面呢 你把資料型態降到FP6跟4 其實它的成效不會降那麼多喔 像是在推論這邊呢 其實FP6的精度呢 跟FP8甚至是FP32的精度比起來呢 在各種的task上面 不管是翻譯還是image classification 還是speech recognition 其實都沒有差太多 就比如說我們講這個speech recognition好了 也就是這個語音辨識的task 語音轉文字的task 你如果用32的精度 完整的精度去算這個模型呢 你的WER
(39:36~40:38) WER就是word error rate 就是你的出錯率 那這個出錯率當然是越低越好嘛 你如果用FP32的精度的話 你的WER是18.9 但是你如果換到這個FP6的精度的話 你的WER呢其實是20.63 有上升一點點但是沒有那麼多喔 就是可能是你每翻200個字 你會多錯3個字 就這樣而已 我覺得是可以忍受的一個範圍 因為你要知道 你從FP32到FP6 你直接降了5倍以上 就是5630嘛 你直接降了5倍以上 所以說你要用的記憶體是小5倍以上 然後你的運算效率也是大5倍以上 但是你的錯誤率呢 只有在200個字多錯3個這樣 但是呢通常在推論這邊啊 你如果用到了FP4 就是你一個字只用4 一個數字只用4個bit來代表的話 你其實你會看到這些錯誤率呢 就開始大幅的提升了
(40:38~41:39) 這也是一個非線性的結果 我也不知道為什麼會這樣 但是這個像是這個Speech Recognition這個Task呢 剛剛講FP6它的Word Error Rate是20.63嘛 那你今天你用FP4的話 你的Word Error Rate會變成42.62 就直接double了 那剛剛講的這些是AI推論的部分嘛 那AI的訓練呢 訓練這邊又更驚人囉 那這邊他們有測試的呢 就是拿FP32的訓練 跟FP4跟FP6的混合精度訓練去做比較 混合精度訓練在講的就是 某一些數字它是用FP4來代表 某一些數字它是用FP6 有一些可能因為對於訓練來說 我們知道我們對於某一些數字 它的精度要求是比較高的嘛 也就是這些gradients 這些T度的數字呢 我們會想要更精確一點 因為這些的T度就是真的是
(41:39~42:40) 在調整這個模型權重的數字 所以這些T度呢 我更精確一點會比較好 那這些權重本身呢 我們就用這個FP4來代表就好了 低一點精度就比較沒問題 那從他們的table中我們可以看到 今天在訓練一個1.5 billion 參數的一個大型語言模型的時候呢 你用FP32的精度去訓練 你的loss是2.74 loss就是我們在評斷一個模型 它的訓練成效好壞的一個指標 它比較技術一點啦 但就是你就先大概知道就好了 它的loss是2.74 那今天呢你用FP4跟6的混合精度去訓練 你的loss只會增加到2.76 loss當然是越低越好嘛 但是你只增加到2.7 從2.74增加到2.76 是非常非常小幅度的上升 但是你從FP32降到4跟6的混合精度 你的一樣啊我剛剛講過好多次了 但是我再講一次就是
(42:40~43:41) 你的這個運算效率是增加非常多的 所以這個MX的data format是真的很厲害啊 然後那個這個Black quad GPU呢 這次也是打頭陣 先來支援這個FP MXFP4跟MXFP6的這個精度這樣 雖然說在它之前呢 微軟的Maya可能也有支援 然後可能也有其他 I don't know 其他的一些ASIC或是whatever有支援 但是這個NVIDIA呢 是第一次採用這個技術這樣 那當然啦 你真的要計較的比較嚴格的話 這個MX的data format呢 它也是它也不是NVIDIA的技術 它是這個多加科技巨頭 就是共同合作研究的結果這樣 所以說你真的要計較的很 很精的話 這個B200的GPU 它運算AI不管是推論 還是訓練AI的這個成效呢 很大的進步呢 其實還是來自就是 並不是他們自己的技術這樣 其他科技巨頭也可以使用MX format
(43:41~44:43) 好那這個B200晶片它的進步呢 大概就講到這邊 就是來自兩個主要的來源 一個是兩個帶拼在一起 另外一個是資料運算精度的下降這樣 那基本上就是這樣子在進步啦 但是當然除了這個晶片算力的進步呢 NVIDIA有一個非常非常厲害的一點 就是他們的interconnect的技術很強 不管是這個GPU跟GPU之間的溝通 還是這個GPU跟這個記憶體之間的溝通 都是非常強的 那很多這些這個GPU 它的這些interconnect 跟GPU之間的很多這些networking 我們把它稱作頻寬好了 我們一律用頻寬講 這個頻寬呢對於AI運算來說 是非常非常重要的 它其實是可能是比這個算力 是更大的一個瓶頸喔 因為你在訓練一個AI模型的時候 尤其是你要訓練一個很大的AI模型 需要非常非常多GPU
(44:43~45:43) 共同去平行訓練這個模型的時候呢 其實你很多時候是在等資料過來 你並不是在算這個模型 並不是真的讓這個模型在training 你是在等資料到這個GPU上 然後也在等這個GPU 一個GPU它算出來的結果 要跟其他所有的GPU一起share 一起共享然後一起update模型 你其實是在等這些資料流來流去 所以如果你的頻寬是可以變大很多的話 就是各種頻寬都變得很多的話 你的AI運算的成效 尤其是訓練這邊 推論這邊比較沒有那麼嚴重 但訓練這邊的成效是可以改善非常非常多的 那這邊其實就是NVIDIA一直以來都做得非常好的部分 那這一次呢他們其實也在這邊做出了很多有感進步 他這邊做出的兩個最大的進步呢 首先第一個呢就是這個 他使用到更多的這個NVLink Gen5的技術 NVLink Gen5是NVLink的最新一代 上一代是Gen4
(45:43~46:46) Obviously 但是這個NVLink呢 他本身是一個GPU to GPU之間的interconnect的一個技術 增快這個interconnect的一個技術 就是加速GPU跟GPU之間互相溝通資料的一個技術這樣 那他們這次是有了這個新的NVLink技術Gen5 然後同時呢他們也在同一個GPU上面 使用更多的這個NVLink connection 那頻寬這邊的另外一個大進步呢 就是他們新的這個NVLink Switch Gen4的chip 那這次就是名字聽起來很像嘛 是NVLink開頭 但NVLink Switch他其實是一塊實體的晶片 這個NVLink他比較像是一個可能一種協議溝通協議這樣 但NVLink Switch他是一塊實體的晶片 就是在管理整個系統他的routing啊 這個connection啊這種的晶片 那這塊晶片啊很厲害 他可以把576個GPU給連在一起
(46:46~47:46) 然後是用非常高速的這個頻寬把他們連在一起 他可以讓這些所有的500多個GPU呢 有1.8TB per second的all to all bidirectional bandwidth all to all就是所有GPU可以跟其他所有的GPU進行溝通這樣 然後是bidirectional的就是可以雙向的溝通 那這個all to all的connection呢 在AI訓練的時候其實是非常重要的 就是我剛剛有說了很多時候就是這些每一個GPU他算出來的這些gradients啊 他要跟其他所有的GPU同時共享 然後一起去update模型 這時候就是要用一些all to all的connection這樣 那靠著這兩項技術呢 NVIDIA就做出了這個GB200MVL72的這個Rack 就是這個伺服器 我們剛剛有說了嘛 就是一開始說的就是這個Grace 講錯 這個Blackwell的GPU呢 他最小的單位就是一個B200的GPU
(47:46~48:48) 然後他會變成兩個組成一個SuperChip 然後18個SuperChip再組成一個GB200MVL72的一個這個Rack 那這個Rack呢 他裡面就是用到了這個MVLink Switch Gen4的這個晶片 以及MVLink Gen5的這些connection的技術 然後有著這些技術呢 這會讓這整個伺服器這整個Rack裡面的這36個GB200SuperChip 我剛剛是不是講成18個 反正我現在更正就是他裡面有36個GB200SuperChip 他靠著這些技術 他可以讓這36個SuperChip運作的時候感覺像是一個GPU 因為他們中間的連結真的是太快速了 所以說感覺真的是像一個GPU在運作而已 那我們剛剛不是一開始有說了嗎 就是你訓練一個1.8 Trillion的模型 你要用8000個Hopper 但你只要用2000個Blackwell GPU就好了 這個4倍的降低呢
(48:48~49:48) 很大一部分我認為應該是來自這些MVLink Switch的這些技術 好那這個Blackwell的架構呢 我覺得就可以差不多介紹到這邊 那很明顯的就是我們這集已經要結束了 然後我們機器人是完全沒有講到 更別說這個甚至是AI的Micro Services 那我覺得就是我最近有一個想法就是 我每次都會畫大餅嘛 就在最一開始我就說我要講ABCD 然後最後才講了A或是AB這樣 但我覺得就是我原本在想說這會不會是一個問題 但我後來想說這樣其實是好的 因為我可以把每一個話題都講得非常的深入 分析得很深 讓每一個人甚至是可能沒有背景的人 也可以帶走一點東西 所以說我覺得我應該要維持這樣子的作風 那沒有講到的有些可能就是遺珠之憾 那這次呢像是這個機器人啊 我自己還是我還是蠻想講的啦 所以說我覺得我可能下個禮拜 我們可以來講這個機器人這樣
(49:48~50:50) 也不確定會不會改題目 要看下禮拜就是有沒有什麼大新聞出來 但沒意外的話我們下個禮拜呢 就是可以來講這個NVIDIA的機器人 因為這邊確實是蠻多可以講的 那我們今天最後呢就在針對這個 Blackwell這邊再給一些我自己的想法啦 就是我可能剛看完這個Keynote的一些心得這樣 那我覺得大家這個首先想問的問題呢 應該是這個NVIDIA還可以風光多久 還可以囂張多久 我沒有要嘴他們的意思啦 但我會說囂張是因為他們真的是 把這GPU定價定很高然後還可以賣得出去 就是他們Bargaining power很強這樣 那這個NVIDIA可以囂張多久呢 我自己是覺得啦就是在這個AI的訓練這邊 尤其是非常大的模型的訓練這邊 NVIDIA絕對還是短期間內 我看不到有誰可以挑戰他們 因為我剛剛也說了就是在訓練這邊
(50:50~51:50) 真的你不只是需要很強大的算力 你還是需要非常非常厲害的 頻寬的技術 Interconnect跟Networking的技術 那NVIDIA在這邊真的是太強了 他們真的是把幾百個這種這麼強的GPU 連在一起變成好像一個GPU這樣 這種事情是真的是非常非常厲害 然後他們也當然是布局了很久才可以做成這樣 那當然還有一些其他因素 像是這個CUDA生態系的這個支援 CUDA的憨屍庫這些 都讓這個NVIDIA的AI訓練這邊 真的是做得非常非常好 那我覺得目前看下來 在訓練這邊比較可能可以挑戰NVIDIA的 應該就是Google的TPU了 應該說除了他以外我真的想不到其他人 然後說真的Google的TPU 其實真的沒有比NVIDIA的系統爛太多 我覺得真的就是可能軟體的生態系沒有建立起來而已 真的就是這樣 他們其實也是非常非常厲害的 就是Google所有的這些模型
(51:50~52:50) 這些所有Gemini的模型 包括Gemini Ultra這種最大的模型 全部都是用TPU訓練出來的 他們根本不鳥NVIDIA的DGX 他們NVIDIA的這些系統 他們不鳥 因為他們的這個TPU真的很厲害 但當然啦就是他們的TPU沒有賣 他們沒有拿去賣 他們可能有賣雲端的這個算力 但他們就是你不能實際買到這個TPU 然後你要用TPU 你就只能透過Google的GCP Google Cloud Platform這樣 那在這個AI的推論這邊呢 我覺得就是不一樣的景象了 這邊其實啊 我覺得NVIDIA長期是會慢慢弱下來的 會不會慢慢被這些新秀給打敗 這邊的新秀呢我在指的當然就是像是 GROK這種GROQ 這種公司做出來的新型的ASIC 像是LPU 那這個LPU呢 究竟為什麼會在這個推論這邊
(52:50~53:50) 有機會可以打贏NVIDIA的GPU 你可以去聽科技量Podcast的EP29 我這個奇數開始累積起來了 我就要開始瘋狂的自我推薦 Self Reference 然後你可以去聽 對這個TPU呢 我看一下喔 TPU你也可以去聽這個EP19 EP19有介紹TPU OK 那這邊就大家自己去study 那我剛剛講到哪裡 對就是這個推論這邊啊 我覺得NVIDIA是有可能會被 新創公司挑戰的 也不只新創公司啊 我覺得Google的TPU也有機會挑戰他們 因為TPU有這個TPU51嘛 不懂的就去聽EP19 那反正我覺得會這樣啊 就是因為這個 你要知道AI推論的市場 跟AI訓練市場是非常不一樣的 他們要求的點是很不一樣的 推論這邊比較注重訓練的效率 就是能耗跟成本 然後也比較注重這個
(53:50~54:51) Token per second 那這些東西呢是 由ASIC是更容易做到的 比起像GPU這種 我們說比較通用型的處理器 General Purpose的處理器 那今天的話題呢 我覺得我們就大概講到這邊 那如果你喜歡這集的話 歡迎給我們科技狼五星的評分 然後可以留言跟我互動 雖然說我知道就是 我可能沒有很常念留言 因為說真的就是 我覺得大部分的留言呢 念出來其實 就是我自己看了很開心啦 但是我覺得念出來就是感覺 有點像是我在自肥 就有點討人厭 所以說我沒有習慣念留言這樣 但是我都會看 然後我知道也有一些問題 可能被我忽略了 但是我之後一定會找時間回答這樣 那大家想問問題也是可以啦 雖然說我覺得很多問題啊 我沒有回答 其實是因為我自己覺得 我沒有辦法回答得很好 我覺得超出我的能力範圍的問題 我就不會回答了
(54:51~55:34) 因為我不想誤人子弟嘛 但當然就是 在我能力範圍的問題呢 我就會盡量回答 那也謝謝今天的贊助商 傲家寶 大家可以在本集資訊欄裡面 找到傲家寶的賣場連結 就要記得 如果想買的話 真的要記得在這個 這個優惠結束之前呢 使用我們科技浪聽眾的獨家優惠 那如果你想要贊助科技浪的話 也可以在我們的資訊欄連結裡面 資訊欄裡面找到這個 科技浪的網站 你可以先進去看一下 我們的聽眾輪廓 以及我們現在的流量 再來決定要不要跟我們合作 要合作的話 就直接寄信給我們 我們會有專人聯絡你 那最後呢 就祝大家有個愉快的一週
