(00:00~01:01) 【字幕】 哈囉大家好,歡迎收聽科技浪,我是主持人哈利 科技浪是一個白話跟你聊科技的Podcast 希望可以用簡單易懂,但是又深入的方式,帶你了解時下最火的科技話題 【字幕】 本期節目由有肉Succulent贊助播出 有肉Succulent是一個我自己非常喜歡的,在賣植物禮品的品牌 今年的中秋節也快來了嘛 有肉就有推出他們全新的中秋禮盒,叫做圓滿之鏡的Path to Perfection 這個有肉他們是一個專注於多肉植物與植栽設計的植物品牌 他們有一個非常漂亮的實體門是在台北捷運大安站附近 他們這次的中秋禮盒是跟甜品品牌卡柏蒂聯名推出的 這個禮盒裡面就會有一個月兔造型的植栽 然後還有卡柏蒂的布列塔尼酥餅 有肉每一年都會設計中秋節禮盒
(01:01~02:03) 每一年都有不一樣的巧思跟變化 今年的中秋節禮盒我覺得真的超級好看的 他們這個月兔的設計是紫色的 背後的想法就是天空的顏色即將進入天黑滿月前的晚霞 就是那種天藍色的天空慢慢變成橘紅色 然後再慢慢變成粉紫色的那種過程 你現在如果點這集資訊欄的連結去看一下這個產品 你就會知道我在講什麼了 這個晚霞的顏色真的是超美的 而這也是為什麼有肉把禮盒裡面的這隻兔子取名叫做俠月兔 禮盒裡面除了這個俠月兔造型的多肉植物植栽 他們有卡柏蒂的甜點 這個卡柏蒂是一個精品級法式甜點品牌 然後他們這次是特別選擇了滿月造型的布列塔尼 作為這次中秋禮盒的甜點代表 然後有三種口味 有蛋黃 有黑糖 有特濃可可 同時有肉也主張永續的概念 他們在禮盒裡面使用的是樂土水泥 也就是那種來自回收再利用的水庫淤泥的材料
(02:03~03:05) 然後他們也不會使用不可再生的泥碳土 我自己本人是有收過有肉的端午節禮盒 然後我那時候收到的時候我真的是覺得超開心的 因為他們的禮盒真的非常的有質感 然後我很喜歡這種非常有質感的東西 我覺得不管你是想要中秋節 用你的客戶還是員工還是朋友家人來表達心意 都非常適合送有肉的中秋節禮盒 那他們這次的中秋節禮盒總共有三款組合 從590的到1490的都有 直接點擊本集資訊欄的連結你就可以看到了 那他們現在到7月31號之前 他們官網都有早鳥九折的優惠 然後這邊也給科技浪的聽眾一個再加碼的優惠 你只要輸入科技浪的專屬優惠碼 TECWAVE 你就可以再想九五折 那今年中秋節想要送一個很有質感的禮盒的話 大家不要錯過有肉的中秋禮盒 本集的配送就到這邊結束謝謝有肉的贊助
(03:05~04:05) 大概在一兩個禮拜前有一間公司叫做Edged AI Edged就是時刻的Edged E T C H E D 這間Edged AI它突然爆紅了一下 就真的只有一下 大概是有兩三天的時間就很多人在講它 然後兩三天過去之後就沒有人在鳥它了 好像已經沒有什麼人在講它了 雖然它的討論聲量已經下降了 但我覺得還是得跟大家聊一下這間公司 因為畢竟它還是有紅一下嘛 而且我覺得確實它是蠻有趣的一間公司 然後可能也是一個大家值得關注的一個方向 這間Edged AI是一間2022年在矽谷創立的一間新創公司 他們主要是在做AI晶片 但他們的AI晶片跟其他公司的AI晶片有一個最大的不同 就是他們只能跑一種AI 這種AI就是它使用Transformer架構的AI 其他使用不同種架構的AI它就沒有辦法跑 其他的這些AI晶片通常都是大部分的AI
(04:05~05:05) 或者是幾乎是所有的AI他們都可以跑 他們的公司名稱之所以會叫做Edged 其實就是因為這些創辦人他們想講說 他們把Transformer這個架構時刻到晶片上面了 就直接刻上去 所以說這個晶片只能跑Transformer 但是這也代表說這晶片的一切 都是為了Transformer這個架構做最優化 所以說這張晶片跑Transformer是其他任何一張AI晶片都完全不可及的 它跑的是最快然後也是非常便宜的 那這間公司之所以會爆紅主要就是兩個原因 第一個原因就是它產品真的很有趣 就我們第一次看到這種只能跑一種AI架構的這種AI晶片這樣 那第二個原因呢就是他們最近剛募完一個120M的一個Round 那這個Round的投資人裡面有包含一個美國的傳說級 大名鼎鼎的這個美國的企業家跟投資人叫做Peter Thiel 所以連Peter Thiel都砸錢下去的東西
(05:05~06:06) 多少應該關注一下對吧 那這就是為什麼我們今天要帶大家來關注一下Edged這間公司 那會在今天這集Podcast裡面嘗試回答大家所有想知道的關鍵問題 就是包括為什麼他們創立了Edged這間公司 以及他們的這個晶片也就是叫做Soulhood的晶片 究竟可以靠著這個Transformer only的特質做到多強 以及為什麼可以這麼強 然後就算它真的很強它做得到嗎 以及它做到了之後未來又會如何呢 我相信大家想問的問題應該就這幾個啦 那今天呢就想跟大家聊一聊我自己看到的資料 以及我自己的一些想法這樣子 好那我們就先從第一個開始 就是為什麼他們會創立Edged這間公司呢 換言之就是為什麼要做一個只能跑Transformer的晶片呢 那接下來的這個部分呢 我主要是會參考他們Blockpost講的內容 然後來講給大家聽 這個主要是他們的故事線 就是大家可以去他們的網站 可以找到這篇Blockpost
(06:06~07:07) 在講他們為什麼要創立Edged這間公司 以及他們的優勢啊之類的這樣 那他把這個故事線全部列出來 然後是非常明確的 那我自己看完了之後呢 大部分我也是非常認同的 所以我就整理了一下來講給大家聽這樣子 那中間有蠻多內容也是我自己補充的啦 然後有一些可能內容的排序也是我自己順過這樣子 那大家要去看原版也可以 我會把這個Blockpost放在這個本集資訊欄的下面這樣 那整件事情呢其實要從Scaling Law開始講 這個Scaling Law呢就是大家近幾年 在Train這些AI模型的時候發現的一件事情 就是大家發現說 你給這個模型越來越多的參數 就是你把它做得越來越大 然後同時你給它越來越多的訓練資料呢 它就會變得越來越強 而且它變強的程度呢幾乎是可以預測的 那這個Scaling Law呢其實是非常強大的 因為它給了我們一個非常明確的方向嘛 然後確實我們在過去的這幾年呢
(07:07~08:09) 也主要是用這個方向 透過Scaling Law再提升模型的表現 臉書今年要推出的這個LAMA3 400B模型呢 它的架構跟OpenAI2019年推出的GPT-2模型幾乎是一模一樣的 架構基本上沒有什麼改變 就微調一些這些數學函式這樣子而已 但是它們最大的差別在哪裡 就是在它們的Scale 它們的規模不同 這個GPT-2呢只有1.5 Billion的參數 然後LAMA3 400B就很明顯有400 Billion的參數 然後會需要用到比GPT-2多5萬倍的算力來訓練 那我們從現在看過去是這樣 但我們從現在看未來也是一樣的道理 就是這個Scaling Law呢現在還沒封頂 也就是說未來我們的模型在做得比現在更大 然後再找更多的訓練資料呢 它就是會比現在更強 那確實這些AI公司呢也都會這麼做 那這些更大的模型呢 就會需要更大更強的資料中心來訓練來使用它們嘛
(08:09~09:11) 然後確實我們也看到各個雲端服務提供商呢 都在為下一個世代的資料中心做準備 甚至有一個OpenAI的前員工叫做Leopold Aschenbrenner 他在他的一篇論文裡面是說 他預期到了2026年的時候呢 我們會建大概10Billion規模的資料中心 會用到1GW的能量 這個1GW呢基本上就是一個大型的核能發電廠的能量這樣子 然後到了2030年的時候呢 我們甚至會花1trillion來做資料中心 然後那個資料中心呢會有100GW的能量 會用到100GW的能量 然後也就是差不多是大於20%的全美國的用電量這樣子 非常非常誇張 那這邊我覺得也可以插出來 就花個一兩分鐘小聊一下這個Aschenbrenner這個人啊 因為他的東西也是蠻有趣的 那他是OpenAI的前員工嘛 之所以會是前員工呢不是他離職 而是他被OpenAI給開除
(09:11~10:12) 那OpenAI那時候開除他的原因呢就是說 他洩漏機密啊講一些不該講的東西啊這樣子 但他自己的解釋呢是 他覺得是因為他的理念 他要走的路線跟OpenAI有點不合 那他的路線呢就是跟Iliya一樣 他是比較偏safety的那一塊 因為他在OpenAI也是在做這個 他也是Super alignment team的一員這樣子 只是他就是一個比較小的員工 那Iliya就是一個大老級的人物 所以說OpenAI沒有辦法開除Iliya Iliya是自己離開 但他這隻小的就可以直接讓他走掉這樣子 那這個Aschenbrenner在離開了OpenAI之後呢 他就寫了一篇超級無敵長的論文 150頁的論文 然後也開始上一些Podcast啊之類的 在聊一些他的理念這樣子 他的一些想法 那他的這個想法啊就是有趣的地方啦 你從他的文章裡面就可以看得出來 他是一個AGI timeline非常短的人 AGI timeline就是
(10:12~11:12) 就是很多人會問這些AI researcher 你的AGI timeline是什麼啊 就是你覺得這個AGI什麼時候會來啊之類的 他覺得他AGI很快就會來了 為什麼 因為他很相信一件事情叫做intelligence explosion 就是他覺得這個GPT2呢可能就是幼稚園的智商 然後GPT3呢開始有些小學生的智商 然後GPT4呢是一個聰明的高中生的智商 然後接下來呢根據Scaling law 我們持續的把這些模型做得越來越大 我們接下來就開始會有這個大學生的智商 然後研究生的智商博士生的智商 然後很快的我們就會有有這個AI researcher的智商等級的一個AI了 根據Austin Brenner他自己的預估呢是2028年 我們就會有一個AI researcher等級的AI了 我就跟大家說他的這個AGI timeline是非常快的 那當我們有這個AI researcher等級的AI之後呢 我們就可以讓這些AI來做AI research對不對 也就是讓這些AI來把AI變得更強
(11:12~12:14) 然後變得更強之後的AI再把AI變得更強 然後就會無限的輪迴 然後就會出現一個intelligence explosion的情況 就是基本上人類的這個科學的進展會非常大幅度的進步 然後當然AI的進展也非常大幅度的進步 然後很快的就會有AGI了 然後有AGI就會有ASI就是Super Intelligence這樣子 然後就是未來就是那個未來了 就可能全民高收入啊whatever 或者是人類毀滅whatever 反正他就是很喜歡說一些很狂的話啦 然後他也不是一個笨蛋 他絕對不是隨口亂說 大家可以去看一下他這個150頁的這一篇論文 叫做Situational Awareness 我會把它放在本集的資訊欄的最下方大家可以看一下 反正他就是看他論文就知道 他是真的是非常非常懂這個industry 然後他確實也是從OpenAI出來的一個人 所以他知道他自己在講什麼 但他就是確實是偏樂觀的一個人這樣 那我自己是覺得蠻有趣的這樣
(12:14~13:16) 好那我們回來這個原本的話題 就是剛剛說到這個資料中心呢 他們會變得越來越大嘛 那這個時候呢我們就會需要越來越多算力 也就是說這些晶片呢會需要持續的進步 但是這邊問題就來了 這個GPU的進步呢似乎已經開始撞牆了 那我們知道這個GPU它背後進步的原動力呢 原本是這個摩爾定律對不對 這個摩爾定律在講的就是 每過18個月同一個面積的晶片上面的電晶體數量會double 那這個double的意思就代表它的算力double嘛 因為我們知道一個晶片呢它是用電晶體在做運算的嘛 一個電晶體就是一個0跟1 它可以用電晶體來代表資料 然後來就是組成一些邏輯單位去做運算這樣子 那這個電晶體的數量double了 你的算力就會大概double 它不會就是非常精準的直接double 但是就是會以大概的比例再這樣子提升這樣 那這個摩爾定律呢真的就是過去這50幾年
(13:16~14:16) 這個半導體產業持續進步的原動力啊 就是這個應該不是說是原動力 但是就是它有點在follow的一個規則這樣子 就是我們美國一兩年這個同樣大小的晶片呢 它的能力它的算力就變成接近兩倍這樣子 然後每一兩年變兩倍 每一兩年變兩倍這樣持續的 有點類似負力的概念持續下來呢 我們現在的晶片跟可能10年前的晶片比起來已經強非常多了 但是呢在最近這幾年我們似乎開始看到摩爾定律停滯了 尤其是GPU這邊看的是非常明顯 根據Edge的說法呢 過去四年來一張晶片一張GPU的算力密度呢 只有提升15%左右 所謂的算力密度呢就是 每平方毫米的晶片它可以做的運算量有多大 也就是這個摩爾定律它主要追蹤的指標 所以這個運算密度成長的停滯呢 就代表了摩爾定律的停滯
(14:16~15:16) 那我們這個GPU的運算密度停滯 我們從這個NVIDIA最近的一些發表 我們就可以看得非常明顯了 最明顯的就是他們今年這個GTC發表的 最新Blackwell架構的GPUB200嘛 那你如果有聽那一週的科技浪 也就是這個科技浪的EP32 你就會知道 我那時候就有跟大家說 這個B200的進步並不是來自摩爾定律這種運算密度的上升 而是來自他們把兩個帶拼在一起 以及他們把資料精度下降 那不只是NVIDIA 很多其他在做GPU的或是AI加速器的公司 我們也看到同樣的狀況 包括AMD的MI300X 包括Intel的高D3 包括AWS的Trainium等等 所以在這個摩爾定律開始停滯的狀況之下 你如果還要持續做出更強的晶片 但是你又不想要無限的放大這個晶片的面積 然後你也不想要無限的下調你的資料的精度 這個時候你應該怎麼辦呢
(15:16~16:18) 這個時候Edged就跳出來說 這個時候就是要做專業化 你為某一個特定的應用 或者是甚至是某一個特定的架構 某一個特定的演算法去做最優化 然後嘗試把你要做的這件事情做到最專精 這就是我們大家常說的ASIC在做的事情 ASIC就是Application Specific IC IC就是Integrated Circuit 這個大家應該都知道 所以說就是專門為某一種應用設計的IC 像是Google為了AI運算 特別開發了他們所謂的TPU Tensor Processing Unit 然後那個時候大家在挖比特幣的時候 也特別為了這個挖礦設計了這個挖礦的IC 對不對 原本大家都是使用這個GPU 但大家發現其實ASIC是比較好用的 所以就開始使用ASIC這樣子 那Edged他們要出的這張晶片 他們把它叫做SOHU 有點像是大陸的那個SOHU 但是它是SOHU
(16:18~17:18) 他們這張SOHU的晶片 它也是把它叫做ASIC 但他們的ASIC跟其他人的ASIC不一樣 他們說他們的ASIC是Algorithm Specific IC 就是不只是Application Specific 它也是Algorithm Specific 它只跑一種演算法 或者嚴格來說它只跑一種神經網路的架構 也就是Transformer 那會選擇Transformer是因為現在很明顯的 這個Transformer的架構 基本上是稱霸整個AI的世界 稱霸Machine Learning 你看到的所有的大型元模型都是Transformer 同時我們也看到原本越來越多 不是用Transformer的這種應用 也開始使用Transformer了 Transformer也在那個領域做到最強了 就像是產生圖像的模型 像StableDiffusion 3就是用Transformer 產生影片的模型這些Sora 這些可能大陸的一些東西 也都是用Transformer的架構 甚至連特薩的FSD也是用Transformer
(17:18~18:18) 雖然說我不知道 雖然這個是Ash的講的 然後我不知道他們的資料來源是什麼 但我也覺得非常有可能他們就是用Transformer這樣 那Transformer會這樣橫跨各個領域的稱霸呢 我相信你如果是科技黨的長期聽眾 你應該是蠻熟悉的 你應該知道是為什麼了 因為我覺得我可能前前後後 在不同的集數裡面都講過很多次了 但反正這個Transformer的架構 它就是一個Sequence to Sequence Model 它在Predict Next Token這樣子 做Next Token Prediction 那一個Token可以是任何的東西 任何的模態的資料它都可以處理 它可以是一個文字 它也可以是一個圖像的一小部分 然後它也可以是一個機器人的一個動作 它任何東西都可以學得會 而且呢它在訓練上有一個絕對的優勢 就是它是可以平行訓練的 因為它最主要的這個注意力機制呢 是直接可以平行運算的 這其實也是為什麼我們發現了Scaling Law
(18:18~19:20) 就是呼籲到前面講的這個Scaling Law 就沒有Transformer的話 我們沒有一個可以Scaling到很大的架構 我們也不知道有Scaling Law這個東西 就是因為有Transformer 然後我們把它Scaling到一個程度發現 它好像還在變好 那我們再把它變大一點 然後再變大一點之後 它好像還持續變好 然後再把它變大一點 然後我們就一直把它變大 變大到現在這個一個 隨便一個頂尖的模型 就是幾Trillion的Parameter這樣 反正這個Transformer現在稱霸的就是因為 它什麼模態都學得會 而且它可以Scale 然後它這個Attention機制 注意力機制呢 確實也可以把資料學得很好 這個資料的前後的Cosation啊 Dependency可以學得很好這樣子 所以說這就是為什麼Transformer 基本上稱霸了整個AI世界 而且有一件事情是非常驚人的 就是這個Transformer架構 幾乎沒有什麼改變 我們從 就這我一開始有講嘛 就我們從2019年使用的 GBT-2的這個Transformer 跟我們現在最強的這些大型元模型的Transformer
(19:20~20:22) 它們基本上是同樣一個東西 我們現在就只有把一些原本舊的數學函式 把它替換掉 把它變得就是更有效率之類的 就像是我們現在用這個 ROPE嘛對不對 ROPE的Position Embedding 然後用GQA 這個Group Query Attention之類的 我們現在有一套這種業界的標準 然後大家都Follow這個東西 但這個Transformer的架構呢 基本上是沒有什麼改變的 所以這也是為什麼 最近這幾年大家的注意力跟金錢 都是放在如何把Transformer變得更有效率 而不是想辦法去給它一個新的Component 或者是想一個全新的架構這樣子 然後同時呢 也再次呼籲到我們前面說Scaling Law那邊 我們現在的這些Transformer Model 已經越來越大了 接下來我們要Train的這些AI模型 從1 Billion到100 Billion都有 那當AI模型到了這個規模 你更不可能去嘗試新的架構對不對 因為太花錢了嘛 不如賭一個現在已經被證實是可以成功的
(20:22~21:24) 而且Scaling Law還有在持續的架構啊 所以呢Edged就想說 未來這個Transformer會越來越稱霸 甚至稱霸各個領域這樣子 然後同時呢這個Transformer的架構 也不太容易被替換掉 所以呢是可以重壓這個架構的 那這也是為什麼他們做出了SOHU 也就是這個只能跑Transformer的一個晶片 未來只要這個Transformer持續長紅的話 他們就有很大的市場 但未來如果我們換到了一個新的架構 可能是我上一集講的這個SSM State Space Model或是什麼RWKV之類的 那這個時候這個Edged這間公司呢 就掛了 他們的產品就沒有人要了 那如果真的要我壓一邊 我當然是壓Transformer 我覺得Transformer持續獨佔的可能性 絕對是比較高的 但我覺得你有聽上一集就知道 我們也是不能小看其他這些Transformer Alternative的 那未來誰都說不定啦 我覺得這個Edged他們是有點自信爆棚了
(21:24~22:24) 那像這個SSM就確實是在小規模看 有可能可以打敗Transformer 所以我覺得這邊我們就再看下去吧 那這邊的邏輯我基本上已經幫大家順完了啦 那基本上做個小結的話 就是根據Scaling Law 模型會越來越大 然後也會需要越來越多的算力來跑 那要支持這樣的算力呢 我們不能靠摩爾丁率 因為摩爾丁率已經開始停滯了 我們要靠的是專業化來讓晶片進步 那要做專業化呢 就要選擇專精Transformer 因為Transformer就是未來 所以這個就是為什麼Edged開發了SOHU 好那我剛剛講了十幾分鐘 我現在大概十秒鐘就把它講完了 那接下來我們來聊聊這個SOHU的晶片呢 它究竟有多強好了 老實說我們不知道 我們應該說我們只有他們公布的數據 但我們沒有辦法親眼看到 因為他們沒有做任何的Demo 也沒有任何的Proof of Concept 這個就是我覺得大家對於他們有點懷疑的地方
(22:24~23:24) 因為像是Grok 它一樣是一個在做LLM的ASIC 他們就是有特別架一個網站 來讓大家親自體驗看看他們Grok的LPU究竟有多快這樣子 那大家如果想要多了解這個Grok的LPU的話 你可以去聽科技量的EP29 我有做一個非常詳細的解析 跟大家說為什麼他們這麼快這樣 那這個Edged的SOHU呢 就沒有網站可以大家使用 然後他們甚至也沒有任何Demo的影片之類的 連Internal的Demo影片都沒有 他們就是只有他們自己公布的數據 然後就真的就只有那幾個數字而已 就他們講了算這樣 那他們的這些數字是哪裡來的呢 我猜啦應該是他們理論上的數字 就是因為他們如果真的有做出這個晶片 然後真的有跑出這樣的成績的話 他們為什麼不錄一個Demo給大家看 我知道他可能沒有辦法大量製造 然後沒有辦法去真的去像Grok一樣做出一個網站
(23:24~24:25) 去讓大家使用 他沒有那麼多的晶片 但你如果已經有晶片的話 然後有跑出這個成績的話 你直接螢幕錄影給大家看也可以啊 但他們連這件事情都沒有做 然後也沒有給什麼他們實驗上面環境的一些設定 他們就是就給出一個數字 他們有多少Token per second這樣 那我是覺得他們很可能是理論啦 覺得他們可能晶片設計的差不多了 但是還沒有實際做出來 然後就算做出來他們也會需要這些軟體的支援啊 什麼有的沒的 那這些東西都還沒有ready 所以他們就先算一個理論值出來這樣 那這邊是我自己的猜測啦 我也不確定他們實際狀況是如何 但我覺得很可能是這樣 那如果你有這些額外的消息可以提供的話呢 你也可以跟我說我下次的科技央再跟大家update 反正我現在的想法就是他們大概是理論的估算 那根據他們提供的這些數字呢 SOHU究竟有多強呢 那這邊要先跟大家說一件事情 就是SOHU的晶片
(24:25~25:26) 他是專門設計來做AI inference的AI的推論的 他不是為了訓練而做準備的 那在這個推論這邊的表現呢 Edge是拿SOHU跟NVIDIA的H100跟B200來做比較 然後他給出了一些非常驚人的一些數據喔 首先他講了一句話就是 一個八張SOHU的一個server rack 可以直接取代160個H100 GPU 這個是因為SOHU比H100快了20倍 這邊頻段快慢的指標呢是tokens per second 也就是一秒鐘他可以產生出幾個token 那一個八張SOHU的server rack 他跑一個拉馬70B的模型呢 他的tokens per second是50萬 然後H100的一個server呢 則是23000 就算你拿最新的這個B200來比 他也只能跑到45000 就跟50萬還是有很大的差別嘛 那同時呢Edge也說SOHU會比H100跟B200偏一非常多
(25:26~26:26) 但這邊他們就沒有什麼實際的數據可以提供 那一樣可能是這個晶片還沒製造出來 你也不知道這個會有多貴 但他們是說會便宜很多 那假設我們現在相信他的數字 接下來的一個問題就是 他們究竟是如何做得這麼快的呢 簡單來說呢就是因為他們只專注於Transformer的架構 所以他們可以捨棄掉一個GPU上非常多不必要的複雜度 他們有舉一個例子就是說 你看現在NVIDIA的一個H100的GPU 其實很大一部分的這個晶片面積呢 是為了programmability而設計的 就是為了讓這個GPU能夠被program去做各種不同的事情 然後去算各種不同的AI模型 他們有講到一點我覺得講得還蠻好的 就是這個H100的GPU啊 它上面主要有分兩種不同的核心啊 一種叫做Cuda cores 一種叫做Tensor cores 那這些Tensor cores就是專門處理這個矩陣運算的
(26:26~27:27) 也就是這種AI最需要的運算的核心 然後這個Cuda cores呢 則是就是什麼運算都可以做的核心這樣子 那一張H100的GPU呢 上面有500多個Tensor cores 但是有一萬多個Cuda cores 一萬八千多個 我一點忘了 一萬八千多個Cuda cores這樣子 那他們有換算一下這個從電晶體的角度來看 這個比例究竟是多少 那他們算出來的結果是 這個一張H100只有3.3%的電晶體 是專門為矩陣乘法設計的 也就是專門為了運算這個神經網路的AI模型設計的 那其實你這張晶片如果要有足夠的彈性 可以算各種不同的AI模型 就是除了Transformer以外 包括這個CNN Convolutional Neural Network 包括RNN based的這些模型 LSTM或是SSM 這個我們上一集都有講到 你如果要這些東西全部都能算的話 你其實做到這樣就差不多了 你不能再做得更好了 但是今天這個SOHU的晶片
(27:27~28:29) 它只會算Transformer 它絕對是不會算任何其他的這個架構 所以它就是只有一個核心 然後就是把Transformer印在上面 它就只算Transformer 把這些就是 Transformer要做的各個階段的運算 全部都直接全部定義出來 烙印在晶片上面這樣子 當然它還是有非常小幅度的這個彈性啦 就像是你可能要換其中某一個階段的一個韓式啊 要用Group Query Attention啊 或者是你換一個Activation Function之類的 你這些事情你都還是可以做得到 但你就是換不了其他架構這樣 然後除了這個可能是 比較在這個運算單元這邊的優化呢 他們也是可以省掉很多其他的複雜度 就包括什麼Control Flow Logic之類的 反正整體來說 他們的晶片呢 把這些東西全部去除掉 讓這個運算的密度提高 然後把不需要做的判斷全部都刪除掉 他們最後的結果呢就是 5萬TOKENS PER SECOND
(28:29~29:30) 那這就是為什麼這個SOHU的晶片可以這麼快 那這個更具體的東西呢 我其實也沒有辦法跟大家講 他們就只有釋出一個Block Post 然後大部分的內容也都是給投資人看的 所以說就真的 沒有太多Technical的東西可以跟大家分享啦 那反正這個SOHU為什麼這麼快呢 我們就大概講到這邊 那我覺得接下來這個SOHU 他們究竟能不能做出來 我覺得這個就是大家可以自己去想想看 就我覺得大部分人現在是抱持一個比較懷疑的態度啦 就尤其他們這個Founder 就是一些這個20出頭歲的這些Harvard Dropout 所以有一些人就覺得 這個這麼Hard Tech的東西 你們真的搞得出來嗎 那我自己不負責任的猜測呢是 我覺得他們是搞得出來這個東西 但他要真正能夠做到 他們說的這個5萬TOKENS PER SECOND呢 可能會需要一兩年 或者甚至是更久的時間 那我會這麼講呢是因為
(29:30~30:30) 首先你要做出SOHU 就分硬體跟軟體嘛 這個硬體的部分就是這張SOHU的晶片嘛 那其實我覺得硬體的部分是比較簡單的 ASIC跟General Purpose的Processor比起來 真的好設計很多 像跟CPU比起來或是GPU比起來 ASIC他的要做的事情真的是單純很多 所以這個就是為什麼你看那個很多的 這個科技巨頭啊 像是什麼Meta、Microsoft、AWS 甚至Tesla他們都可以自己研發晶片 因為他們在做的都是ASIC 就是比較簡單的東西嘛 那像是這個SOHU的晶片 他又是ASIC裡面更單純的 就是直泡Transformer 那他有可能就 比其他ASIC比起來還更好設計一點 所以我覺得硬體這邊 他們設計應該是還好 那接下來的製造什麼東西 他們在他們的Block Post裡面也說了 就是他們有跟台積電 已經預定產能了 那我不知道他們是怎麼做到的 我不知道他們是怎麼做到的
(30:30~31:31) 他們就融到100Million這樣子 我不知道他們怎麼買到台積電的產能 然後他們是說他們訂了台積電4奈米 那這邊我覺得可能是Peter Thiel之類的吧 那在這個HBM這邊 也就是這個記憶體這邊 他也說他們已經跟Supplier談好了 然後也已經預定了 第一年的Production所需要的所有產量 我是不知道他們是怎麼搞到的 反正他們已經把這些產能都預定好了 所以硬體這邊我覺得是還好 那軟體這邊呢 我覺得是會比較挑戰的地方 那當然是我自己不負責任 拍腦袋的亂想啊 我直覺啦 但大家要知道就是 今天你一張晶片他可以算AI算得多快 很大一部分是軟硬整合在決定的 你的軟硬整合做得多好 做得多優化 並不是你這個晶片算力本身 當然算力本身也很重要 但是軟硬整合是非常非常重要的 那這個就是NVIDIA做得非常好的地方嘛 他們就是從2006年的時候呢
(31:31~32:31) 就開始開發一個平台 叫做Cuda 這個Cuda就是專門在做軟硬整合的啦 就是如何把這個各種 包括AI啊 各種科學的程式嘛 最優化的在GPU上面跑 那這個Cuda呢 他並不是就是一個軟體而已 他是整個平台 包括了非常非常多不同的Component 就是從他們有自己的NVIDIA這個Compiler 好像叫做NVCC這個Compiler嘛 然後還有他們的 這些Cuda Kernels嘛 這些韓式酷啊 然後各種韓式酷啊 包括酷DNN啊 酷BLAST啊 有的沒的 那這些東西加起來 真的是會需要非常非常大量的人力 才去開發 才能把它做到最優化的 那雖然說確實啦 就是SOHU要開發的這個軟體呢 絕對比Cuda簡單 非常非常多 因為Cuda會很複雜 是因為這個GPU本身是非常複雜的嘛 對不對 幾萬個Cores啊 然後各種不同的數學運算
(32:31~33:32) 都要可以Support啊這樣子 那這個SOHU呢 他絕對是單純很多 但我覺得這種事情 你不是說你這個硬體單純 你的軟體就絕對很單純 不是這樣就結束了 你真正你做的這個晶片 要有人可以使用 你必須要跟其他大家常用的 開發者工具去做整合 就是你要跟其他人寫的 Sofer去做整合 就是包括最最最明顯 就是這個PyTorch嘛 你要讓這個寫PyTorch Code的人 可以在SOHU上面安排他的 這個Training Job 或者是Inference Job之類的 對不對 那這邊可能就是 複雜度會出現的地方了 因為PyTorch 以及這整個這個開發者的生態系本身呢 真的是非常非常複雜的啦 就是這個PyTorch 光是Convolution就有一千多種做法之類的 那雖然很明顯的 我也沒有做過類似這種Cuda這種軟體 所以我也不確定就是
(33:32~34:33) 你要跟PyTorch做一個整合究竟有多困難 但我自己是覺得這應該是 不是幾個月內就可以做出 最優化版本的事情 就是幾個月內你可以幹出幾個版本 但你不一定是最優化的 你可能要一兩年之內 你才可以做到最優化之類的 那接下來呢 我們最後要聊的話題就是 假設SOHU今天真的做成了 那它會有多大的成功 那要分析這種事情呢 我最喜歡用的一種框架就是 問兩個問題 第一個問題就是 SOHU的市場有多大 然後第二個問題就是 SOHU能吃到這個市場的幾% 其實你今天在分析NVIDIA的時候 你一樣是問你自己這兩個問題這樣 那這兩個問題當然是很難回答的 然後大家都有各自的想法 那我今天就給大家我自己的想法這樣 好那首先這個市場究竟有多大呢 我們這邊講的市場呢 指的是這個AI的應用端啦 我們從這個應用端去 回推這個推論晶片的需求這樣子 那從這個應用端來看呢 我覺得這邊很多人會 互相disagree啦
(34:33~35:33) 那我自己的想法是 我覺得目前來看 AI應用端還不是很成熟 我們大部分看到的是一些 proof of concept 一些demo 真正已經開始在創造價值的AI產品呢 其實沒有很多 那我覺得這邊我喜歡用 一個3P的框架來思考 這個是哈利的3P框架 這是我自己想出來的 那我覺得我們可以大致把現在 目前看到的所有AI應用呢 大概分類到這三個P裡面 這三個P分別是 第一個product產品 prototype原型以及prospect前景 那首先第一個product產品呢 它在講的就是 已經有在創造價值的AI產品 這些產品呢 已經可以發揮很大的價值 然後大家已經把它融入 它的工作中或是它的生活中 然後時常的使用 然後也覺得確實有改善到生活 或是改善到生產力的 那我覺得就現在來看
(35:33~36:33) 可以被歸類為product的AI產品呢 其實真的不多 我覺得認真想呢 其實就主要就兩個 第一個就是getup copilot 第二個就是chat gbt 或者是其他的chatbot包括cloud 包括gemini這樣子 這兩個都是大家自己會用 然後公司也會幫員工買 然後大家都覺得非常有用 然後被證實了確實會大幅提升生產力的東西 可能還有別的我沒想到 但這兩個就是主要代表這樣 那第二個P呢prototype 就是即將到來的產品 那這些就是那些已經有proof of concept 或者是有個簡單的demo 或甚至是有在做小規模的測試 的這些產品 那他們現在呢都還很破爛 但我們知道他們都是 可以被做出來的產品 也都知道還差哪一些因素 通常都是我們需要 更快更便宜的influence 或需要模型能力的提升 或者是需要更高的controllability 之類的 我們知道要哪些因素
(36:33~37:33) 然後也知道這些因素在未來的幾年 慢慢出現了 所以說我才會把他們稱作這個 即將到來的產品這樣子 其實很多我們平時看到的這些 AI工具我們在聊的這些AI工具 我覺得他都會被歸類在 prototype這個類別裡面 最明顯的就是一個devin 我覺得devin就是這邊的 就是代表的產品這樣子 你如果不知道devin是什麼的話 你可以去聽科技上的ep31 反正他就是一個AI coding agent 這樣子 所以他們這個廣告不實 就是嚴重的cherry pick devin現在根本做不到 這種程度 但是我覺得他們確實 在這個 influence變得更快更便宜 然後模型能力提升了之後 他們絕對是可以做到 這些事情的 其他也包含在prototype這邊的產品 就像是sora這種影片生成 模型或者是 像gbd4o或是 google project astra這種
(37:33~38:33) 即時的multi model agent 或者是 甚至我覺得像adobe firefly 或者是figma 最近demo的一些AI產品 甚至我會說microsoft 365 copilot跟google workspace 的gemonite也都算是 prototype而已 嚴格來說 microsoft 365的copilot他們 已經越來越多人開始使用了 但我覺得他們都還 沒有發揮真正的價值 當今天的influence變得更快更便宜 然後模型能力提升 他們會變得越來越有用 所以其實絕大多數我們生活中 聽到的這些AI工具這些AI產品 它都會被我歸類在 prototype的類型 他是一個toy app他是一個proof of concept 他是一個小規模的 產品測試 他們都還沒有發揮真正的價值 最後一個類別我把它叫 prospect就是前景 這些產品現在連一個雛形都沒有 但是這個是我們努力的方向 這邊就是包括像是
(38:33~39:33) AGI或者是 一個general purpose的 humanoid robot 像optimus這種 什麼事情都可以做的人形機器人 這邊就是 比較未來的東西比較前景的東西 所以我們從這個3P的角度 來看現在這個AI應用的市場 目前這個市場 有明確的具體的 AI推論需求的就是 product這個類別的產品 像是chapbiz跟getup copilot這些東西 真的他們用戶 的user base不小 這種chatbot 類型的軟體一堆人在使用 但是真正最大量的 潛在的推論需求 現在都還是存在這個 prospect的階段 隨著這個 模型的變好就是scaling 的持續以及隨著 這個硬體的進步 這個越來越多的prospect 會變成product然後甚至會 有新的prospect出現
(39:33~40:33) 也就是說這個推論的市場會有 很大很大的成長潛能 然後這個sohu的晶片它本身其實 很有可能就是 能把很多的prototype變成 product的一個關鍵 像是剛剛講的這個devin 如果你今天有sohu這個 很便宜而且非常快速 的這個推論引擎的話 我覺得這個devin就成了70% 甚至80%了 因為現在這個devin最大最大的 一個問題就是你用它 是超貴的就你 你要叫它完成一個任務呢 假設你完全不干擾的話 它可能要prompt gpt4幾千次 之類的那這個apicode 加起來就真的很貴嘛而且你要花 可能20 小時之類的你才可以把它 完成一個事情這樣子 那sohu的晶片可以快20倍 然後也可以 不知道把cost可以cut到哪個程度 這樣子那它絕對會讓devin 的可行性變高很多 然後一些其他的prototype像是
(40:33~41:33) 可能即時的語音啊 或是甚至是即時的影像 生成也都會變成product 或是離這個product 再更進一步這樣子 你能想像有即時影像 生成的工具嗎 就像是我不知道它的應用長頁是什麼 但我突然天馬行空想到一個就是 假設你今天在一個演講 然後你沒有準備你的ppt 那這個時候呢你就可以直接 用這個ai去即時的 根據你講什麼話 產生出相對應的內容 就比如說你再講一個 關於如何克服工作的 無力感的一個演講 這樣子那你就講到 當你今天感到非常 無力在辦公室什麼事都不想 做你講到這句話的時候 你背後的螢幕就 產生出一個人在辦公室非常 無力的畫面 你不覺得非常酷嗎 當然這我是沒想到 一定還有更多非常厲害的應用 所以市場大小不是問題 那接下來的重點就是
(41:33~42:33) SOHU究竟可以吃到幾% 那這邊呢首先有一部分的市場 是會被那種edge model 給吃掉這樣子 因為SOHU它是一個 Data Center的ASIC 它不是裝在消費者的 裝置上面的所以說 有一些可能比較 簡單的應用呢就 會先被邊緣裝置上面的處理器 給處理掉這樣子 那這邊究竟有幾%大家都說 不準嗎但我相信 絕對是有一定的比例的 未來一定是混合的 一定是有一些應用是在 邊緣上面算 有一些應用是在雲端算 那三個主要的好處就是 它有更好的Privacy 跟Security 它有更低的延遲 它是免費的 這三個主要的好處 雲端則是沒有 記憶體跟算力的限制 所以可以跑很大的模型 兩邊是真的很不一樣的 未來不一樣的應用
(42:33~43:33) 會有適合不一樣的運算方式 未來一定是混合的 只是這個混合的比例究竟是多少 就是要看未來的狀況 我們從Apple Intelligence 從Core Palette Plus PC 我們很快就可以抓出 一個合理的比例了 不管是幾%的應用被 Edge 給處理掉 反正剩下的這些應用 你也要先刪除掉Transformer以外的 Transformer以外的 應該真的不多啦 覺得它多數應該還是Transformer 就連這些繪圖的Diffusion Model都慢慢往Transformer 這邊走 變成Diffusion Transformer 這種DIT的架構 像是Stable Diffusion 3 所以說Transformer 應該還是主要的 扣除掉了極少數非Transformer 的應用之後 剩下的市場份額 基本上就是 SOHU要跟NVIDIA 以及跟其他競爭者競爭的部分 這邊呢
(43:33~44:33) 先講講NVIDIA 我真的是覺得NVIDIA 在這個推論的市場 可能真的最終會輸給ASIC 因為AI的訓練 跟AI的推論所需要的 硬體設備 真的是非常不一樣的 或者應該這樣說 你關注的點是非常不一樣的 你關注的會是像是Networking 像是整體的Throughput這種東西 但在Inference這邊 你關注的會是Cost 跟Latency這些東西 在AI訓練這邊 真的有蠻多的互成何的 尤其在Networking這邊 像他們有Mellanox跟NVLink的技術 但是在這個推論這邊 Cost跟Latency 我怎麼想都不覺得 他可以贏得過ASIC 所以老實說我覺得現在很多 AI的推論他背後還是使用 NVIDIA的GPU 可能是因為兩個原因吧 第一個原因就是可能ASIC的 晶片開發的還不夠成熟 這個選項也沒有非常多
(44:33~45:33) 第二個可能就是NVIDIA 有比較完整的開發者生態系 這個Cuda的平台 他們已經開發很久了 然後也都跟各種的工具 都做了非常高程度的整合 大家最熟悉的就是 使用Cuda toolkit 去跑這些AI模型 所以我覺得當這個 SOHU的晶片真的開發完成了 然後確實跟他們講的 一樣強的話 所以在推論這邊 NVIDIA真的很可能讓出很多的 Market share給他 然後剩下SOHU自己再去跟 GROCK以及其他的 LLM specific的ASIC 去做競爭 當然我也必須說 這並不代表NVIDIA現在的 市值全部都是泡沫 之後一定會爆跌之類的 因為我必須說NVIDIA在 模型的預訓鏈這邊 它的優勢真的是太大了 那沒有人是 甚至是接近他的 很多要挑戰NVIDIA都是挑戰
(45:33~46:33) 他在推論這邊的霸權 在訓練這邊真的是太困難了 然後我們開頭也討論過了 就是Scaling law之後 是會持續的 可能根據 OpenAI前員工Aushan Brenner 他的預測 接下來再過個幾年可能就會有一個 trillion dollar的 Data center 光是吃訓練這邊的DEMAND 就可以justify他的 三兆多的 市值了 也有可能啊這個很難說 而且誰說NVIDIA不能自己做ASIC的 他們也可以啊 尤其現在他們這麼有錢 你看這個SOHU 他們才募了多少 120 million 對NVIDIA來說是P一點小錢 他們隨便就可以放一個 差不多的project了 所以我覺得未來都難說啦 好那今天呢 就是跟大家簡單介紹一下SOHU 這個晶片啦 那你如果喜歡今天的節目的話
(46:33~46:57) 可以給我一個五星評分 然後把科技量分享給你的朋友們 然後如果你還在思考這個中秋節 要送什麼禮品給你的客戶 老闆還是這個 員工的話呢 絕對可以參考一下有肉他們的 中秋節禮盒 我覺得他們禮盒真的超有質感的 那今天呢就在本集的資訊欄裡面 大家自行取用 最後呢就祝大家有個愉快的一周
