(00:00~01:02) 【音樂】 哈囉大家好,歡迎收聽科技浪,我是主持人哈利 科技浪是一個白話跟你聊科技的Podcast 希望可以用簡單易懂,但是又深入的方式 帶你了解時下最火的科技話題 本期節目由戰雅贊助播出 那牙齒矯正呢,是很多人都會選擇做的一件事情 因為它的功效真的是太多了 除了最直接的就是它提升你的笑容的美觀 然後提升你的自信以外 它還可以改善很多健康相關的問題 就是比如說它可以改善你的咬合問題 減少你的牙齒磨損跟這個二關節的疼痛 然後同時呢,它也有助於預防牙齦疾病跟蛀牙 然後維持你的整體口腔健康 所以說它真的是提升生活品質的一個很好方法喔 那我們今天的贊助商呢,戰雅 它是做隱形牙套的品牌 它是來自新加坡的一個品牌 創立於2018年 然後在2020年的時候落地台灣市場
(01:02~02:02) 那雖然說他們是在疫情期間來台的 他們仍然是在短短的幾年之內呢 用他們創新的商業模式跟客戶需求導向的服務 獲得了超過12萬以上的用戶信任 然後跟200位以上的本地醫師合作 那戰雅為什麼可以做出這麼好的成績 它的優點在哪裡 我覺得主要有三個 首先第一個就是它合理透明的價格 那它是根據你的牙框複雜程度分成三種矯正方案 每一種矯正方案都是固定公開收費的 絕對沒有隱藏費用 那它也是市面上唯一可以做到均一收費的品牌 不會因為區域、診所、醫事的不同 然後有不同的價格 那他們的付款呢 也可以做到最高12期零利率 每天最低只要200塊 那我相信這個價格的透明與公開呢 是很多牙套用戶的最大需求 所以說戰雅有做到這一點是非常好的 那戰雅的第二個優點呢 是他們的透明設計 那就如其名呢 這個牙套是隱形的 它是完全透明的 戴上去之後呢
(02:02~03:02) 你不仔細看 其實一般很難察覺 然後它可以貼合你的牙齒 所以說不容易刮傷你的口腔 那最後一個優點呢 就是這個戰雅的服務是非常專業的 戰雅它會確保每一個顧客呢 都經過國家考試認證的牙醫 親自診斷 然後製作專屬治療計劃 療程中呢 你也可以隨時透過Zenium App 跟專業的團隊溝通 然後記錄並且提醒你的這個療程進度 那你如果聽完了剛剛的介紹 你覺得有點心動 你可以直接線上進行免費的評估 看你適不適合使用這個產品 過程是非常簡單的 你只要上傳四張牙齒的照片 五分鐘之內就可以完成評估了 那這個評估的連結呢 就在本集的資訊欄裡面 而且這個連結呢 是科技浪專屬的連結 所以說即日起至3月31 你用這個連結進行免費評估的話 你就可以無條件獲得 1000元的療程折扣 那你如果在完成這個品牌 社群互動分享的小任務的話
(03:02~04:02) 你可以疊加折扣金額到最高 13000元 心動的朋友就趕緊使用本集資訊欄的 科技浪專屬連結進行免費評估吧 本集業配就到這邊結束 謝謝Zenium的贊助 好的那我們今天呢要來聊一些 AI晶片相關的內容 但在我們進入這個正題之前呢 這個比較深入比較技術的部分之前 我們先來聊一些 最近的一些時事 那這邊一樣就是如果你不想聽的話 你就直接到本集資訊欄裡面 找那個timecode 然後直接快轉到這個正文的部分就好了 不用硬聽 首先呢是想跟大家聊一些這個 娛樂的新聞 那有一直在聽科技浪的話 大家就知道我有在追一個娛樂新聞 就是這個 台灣有一個叫做Ray的男高中生 在國外爆紅嘛 那他會爆紅呢就是因為他意外的認識了 國外一個非常有名的直播主 美國的一個直播主 叫做Kai Kai Sennett 那這件事情又有了最新的發展
(04:02~05:02) 就是這個Kai Sennett 他最近是來到了台灣 就在過去這個禮拜 他來台灣那為什麼會來台灣呢 他有去找Ray嘛 他跟Ray一起直播 然後跟Ray一起在台灣逛街 然後拍很多的內容 他們有去西門町 然後這個Kai甚至還有去 Ray的高中跟他上了一天的學 然後與此同時啊 Ray他自己的這個twitch帳號呢 也是經營的有聲有色啊 他有很努力的在開直播 然後他現在的這個追蹤人數呢 已經到達了40萬人 欸40萬是什麼概念 Stanley的概念 大家應該知道Stanley吧 就是那個在玩英雄聯盟的直播主 我覺得算是台灣直播主裡面 最有名的人之一了 Stanley他才44萬的追蹤 這個Ray 他才開直播 一兩個月吧 可能還不到兩個月 然後他已經有接近Stanley的追蹤人數了 然後其中呢也有差不多將近4000個人
(05:02~06:04) 有付費訂閱 所以說就是差不多是1%的轉換率 那這個1%的付費轉換呢 在twitch裡面我大概比了一下 我覺得算是非常健康的一個轉換率 然後因為訂閱的價格呢 是一個月5塊美金 所以說你自己算一下你就會發現說 這個頻道呢現在能夠為Ray 每個月帶來差不多40幾萬台幣的收入 這邊是扣除掉twitch的這個分潤 那我覺得這個Ray啊 他現在已經算是某種程度上的台灣之光了 他已經請到這個 全世界最大的直播主之一 開SENET來到台灣 還跟他一起在他家吃飯 一起跟他去上學什麼樣的 我覺得這段期間這個Ray 對於台灣國際知名度的貢獻 可能比很多政府的計劃都還高非常多 真的是 你看一下這個開SENET 他的youtube影片的流量 才短短過兩天的時間 就已經超過200萬了 那除此之外呢在這個端鷹平台呢 那個流量又是更加的恐怖 所以說這個 必須說是一個非常大的
(06:04~07:04) 所以說這個必須說是一個非常成功的國民外交案例 那同時呢也恭喜Ray 把他的頻道做得這麼好 我看到他的成功呢 我也是感到非常的欣慰 畢竟我是從最一開始呢就在追蹤Ray了 那最後呢也是老話一句啊 就是Ray祝你未來可以越來越好 那我會持續的關注你的 那大家如果好奇Ray的這個twitch帳號的話 網路上直接搜尋 RayAsianBoy 中間沒有空格 就可以找到他的twitch帳號了 那另外一個想跟大家閒聊的話題呢 是我們來聊聊Google這間公司 老實說如果我是Google的公關部門的人 老實說如果我是Google的公關部門的人 我這兩個禮拜應該會感到極度厭世 事情是這樣 我在上一集呢有跟大家說 Google有一個非常厲害的AI突破 叫做Gemini 1.5 Pro 詳情你可以去聽上一集啦 但反正它就是一個context-length的一個大突破 也就是說這個 一個大型多模態模型 它一次可以吃的這個資料的長度 得到了很大的突破 它可以一次吃非常多的資料這樣
(07:04~08:04) 那這個是 算是一個非常大的AI突破 但是呢Google在發表這個技術的 同一天OpenAI 釋出了他們的Sora模型 那這個Sora模型呢 想當然就直接 佔掉了他們所有的光彩 因為這個Sora模型呢實在是太厲害了 而且是大家一眼 就可以看出它很厲害的東西 就算是完全不了解AI的人 他看到這個影片是AI生成呢 他也會感到非常的驚訝 至於這個context-length是什麼東西呢 可能就大部分人就不知道 但是隨後Google又釋出了 一個新的AI突破 也就是一個叫做Gemini的AI模型 這個Gemini呢算是size偏小的 一個語言模型喔 有些人會把它稱作這個 小語言模型這樣 它的尺寸呢依照參數數量分成兩種 7個billion參數的 那兩個billion參數的就是差不多 可以在手機上面跑的這種size這樣 那這個Gemini模型有兩個特點 第一個特點呢就是它的表現非常的好 就是它是所有小語言模型之中
(08:04~09:04) 表現最好的 就是它比原本的7billion的王者 也就是這個 Mistral 7b的模型 表現還要好 不管是在coding還是在math 還是reasoning還是question and answering 都比較強 另外一個特點呢就是這個模型 是開源的 它的權重跟它的模型架構 它的論文都是 有發表在外面給大家下載使用的 這個對於Google來說呢 真的是非常難得一見的一個舉動 那Gemini這個模型呢 其實本身也是蠻不錯 蠻值得討論的 但是又沒有人在討論它了 為什麼因為同一時間Gemini 延上了 那Gemini因為政治正確而延上這件事情 我覺得算是最大的頭版 所以說我相信應該很多人已經聽過了 但是我覺得應該還是有少部分人沒聽過 所以我這邊還是快速的講一下 那反正呢就是Google的聊天機器人Gemini 它有產生圖片的功能 然後大家呢就發現說 它產生出來的這些圖片都很woke
(09:04~10:04) woke這個詞呢就是在形容 遊戲人他過度的追求政治正確 他追求DEI到有點誇張的程度 DEI就是diversity equity and inclusion 就是這種多元啊 種族多元啊包容啊 這種的東西 種族多元啊包容啊這種的概念 那這個DEI的概念 它的出發點當然是好的 但是任何東西到了一個極端 它就不好了 DEI也是一樣 當你開始變成很woke的時候 就會變得很奇怪 那這個Gemini奇怪的地方呢就是 它不敢產生很多白人的圖片 就比如說有人問他說 請產生美國開國國父的照片 那美國開國國父呢 就是那些George Washington Thomas Jefferson Benjamin Franklin 我們稱之為The Founding Fathers 那些人 那這些人呢全部都是美國的 白人男性嘛 但是你問Gemini Gemini竟然會產生出黑人 那個時候搞不好馬丁路德基因都還沒出生
(10:04~11:04) 想也知道不可能是 這個Founding Fathers裡面會有黑人 那另外一個例子呢 就是有人問說 產生一些Vikings的圖片 這些維京人的圖片 那維京人呢就是北歐的族群 那就是一些白人嘛 但是呢這個Gemini竟然產生了 一些黑人的維京人 跟亞洲人的維京人 然後還有人叫他產生這個 教宗的圖片 然後他就產生了一個黑人的教宗 跟一個這個印度女性的 教宗 那教宗當然就是一直以來也都是白人男性 然後很好笑的就是 你如果叫他產生的是原本就是 有數民族的文化他會非常的尊重 就比如說你叫他產生 Samurai就是日本武士的圖片 他就會全部產生亞洲人的臉孔 那你叫他產生 南非的一個民族叫做Zulu Warriors的圖片呢 你就會看到就是 全部都是黑人 沒有白人的Zulu Warrior 然後說這個圖片生成的功能 很Woke以外
(11:04~12:04) 其實Gemini文字的部分 也是蠻Woke的 就是大家也有做一些測試 然後就發現他有一些 他會講一些很Woke的話這樣 那Google這邊第一時間呢 他是先把這個Gemini生成 人像的這個功能給取消 你還可以生成其他的圖片 但是只要跟人有相關的他就會拒絕產生這樣 然後同時也發了一篇文 就是跟大家道歉 他說我們不小心 搞砸了就是這個Gemini 這樣子的行為呢我們也認為是 不OK的我們會Do better 那這件事當然對於Google來說看起來 真的是挺糟糕的嘛 然後對我個人來說也是這樣 就是我覺得我真的不會想 使用一個產品是裡面有反應了 某一個特定族群的意識形態的 我想要得到的就是 最貼近我問的問題的答案 我問什麼東西你就給我那個東西 這樣就好了然後有沒有什麼偏見 有沒有刻板印象這些東西我可以自己去判斷 假設我今天是一個歷史老師 然後我再做一個關於美國歷史的PPT 我要一張開國國父的照片
(12:04~13:04) 你就是給我 我George Washington那幫人 那幫白人就好了你如果給我一個 黑人在裡面你就會是一個比較 爛的聊天機器人我就會換到另外一個 服務使用那我相信 絕對大多數人也是跟我同樣的 想法所以Google這樣真的是 在拿石頭砸自己腳然後 我覺得其實是很諷刺的一點 就是當年Google search 可以打贏Yahoo search 就是因為Google search它有更好的 的演算法它用那個PageRank的演算 法嘛所以說它秀出來的 資訊它的Temple links是 更加相關的跟你搜尋 的問題是更加相關的 結果現在Google面臨的這個 新的LLM的戰場Google 竟然自己把這個產品 的可用度下降 自己把這個產品給搞爛了 我覺得真的是蠻諷刺的 尤其現在Google的投資人真的都 很緊張因為這個Google search 這個產品真的是Google最 賺錢的一個產品也是毛利 最高的一個產品那這個 產品現在很明顯的就是面臨這個
(13:04~14:04) LLM的威脅對吧 我自己現在做這個分析的時候 我在研究一些話題的時候 我這個搜尋的過程 已經完全的改變了我現在 永遠就是會開著一些 LLM的網站 不管是ChatGBT Gemini還是 Perplexity AI我會開著 這些東西然後跟著Google 一起並用原本的這個 搖籤數受到威脅然後新的這個 戰場好像又沒有做得很好 又有些政治風波這樣 所以說有些投資人真的是感覺 很擔心也甚至開始懷疑 這個Sundar Pichai的 帶領的能力了那針對這件事情 Sundar Pichai其實也有寫一封信 給Google內部的員工 那當然這事情弄到這麼 大了這種內部信一定會被流出來 所以這個內部信就被流出來了 那裡面的內容呢 基本上就是說我們這次 真的搞砸了但是呢 我們之後會做得更好因為我們要開始進行 組織上的重整 那這個組織的重整呢 我個人覺得應該就是要砍一些這種
(14:04~15:04) DEI的人 就是大家有在說嘛就是Google 裡面有一個可能是 叫做Responsible AI之類的Team 那他們會參與很多 的這些AI的決策 流程那這些人呢可能就是 一些意識形態比較偏左 比較偏激一點的這些 人好那他們就是 會把他們自己的這些意識形態注入 Google的各種產品當中 然後他們的權力也非常大 所以說一旦有人質疑 他們的做法 那個人就很有可能會被開除 所以說大家都不敢質疑他們 那Sunarpitchai呢其實一直以來都不怎麼管 這些人應該說這些人應該也是 他帶進來的這個整個文化呢 也是他們他發展Google的 其中一個方向之一那現在是 很明顯有點過頭了所以說 他自己本人也說他要進行 一些改革了那這個改革 結果究竟會如何我們就繼續看下去 那除此之外呢 我其實又想到另外一個插曲 就是去年11月的時候我其實就 已經有在這個OpenAI的
(15:04~16:04) 圖片生成模型Dali3 上面發現這個 很多政治正確的問題但我那時候其實 也沒有說這個是多大的問題啊就是 我覺得算是一個小觀察吧 就是那時候就有發現說 欸你叫他產生的 我是測試職業啦就是 叫他產生CEO他可能就是 10張CEO男女比是5比5 然後種族比例呢就是大部分 都是少數族群白人呢 只有佔1% 儘管實際的數據呢是完全 相反就是白人男性應該是 佔90%然後同時他也有一些 比較雙標的情況出現就是 比如說你叫他產生護士的圖片 他就會產生10個裡面男女 是5比55個但是你 叫他產生工地的工人的圖片 他就全部產生男生 那那個時候呢我覺得你如果做一樣 測試叫他產生教宗的圖片 或者是這個 叫他產生維京人啊之類的 他可能會做出很類似 Gemini的行為我覺得是 有機會做到這一點的但是那個時候 我覺得我也不知道為什麼就是
(16:04~17:04) 除了我以外我沒有看到其他人 在測試這個東西所以說 那時候就沒有延上這樣然後現在Google Gemini延上之後呢 我又回去測試一下這個Darly3 然後我發現他已經把這些 DEI的東西全部都改掉了 其實你現在叫他產生10個CEO的圖片 他會給你10個白人男性 就是這樣反正就是覺得 有點好笑啦就是不知道他們是 什麼時候改的反正我覺得 應該是在最近啦就是可能 是從去過去的一兩個月 開始就是從那個Bill Ackman 他說哈佛大學的校長 有一點DEI太誇張了那個 時候開始大家 就開始對於DEI的這種 想法有點逆風了 這個Google呢Gemini的事件 又是DEI逆風的高峰 所以說他可能是在這一段期間之內呢 趕快把這些feature給修掉了 當然這樣子究竟是不是比較好 我覺得還是有討論空間的 就是真的要產生10個 白人男性嗎 說真的現在我們看到很多CEO 也真的是少數出去
(17:04~18:04) 尤其是可能幾間 科技巨頭有印度人 有台灣人那這邊的平衡 究竟該怎麼抓我覺得就是待討論的 一個話題啊但我自己呢 我是會偏向我希望能夠 越真實的反應實際比例 越真實反應實際世界 是越好的 好那接下來呢我們就進入今天的正題 今天呢是要跟大家介紹一間 叫做GROCK的公司 GROQ不是GROK GROK就是那個 馬斯克的XAI他們的聊天機器人 不是那個喔是GROQ 的GROCK 那GROCK這間公司呢是一個在做 AI運算引擎的公司 什麼叫做引擎呢 他們的Solution有包含了他們自己的 晶片然後他們自己的 編譯器跟他們一些Networking Technology 那這邊你聽不懂沒關係 我們待會會再做詳細的介紹 反正你現在就是要知道就是 GROCK他們有整套的技術可以讓你跑 大型魚圓模型跑得非常的快 那GROCK這間公司呢其實2016 年就成立了但是一直到
(18:04~19:04) 上個月他們都還是默默無聞的狀態 1月的時候真的 沒有人聽過GROCK可能有些人 非常是非常Insider的他會知道 但99%的 一般大眾都不知道 甚至是一直在Follow非常密切Follow AI資訊的人也都不會知道像 我也沒聽過但是在2月的時候呢 GROCK的影片直接在X上面 爆紅了我一開始 也是從這個X看到他們的影片 我看到就有立刻傳在我的 IG上面 發一個現實動態這樣那我 發完了之後呢隔天很多 的這些主流媒體呢也都開始 就是注意GROCK然後也開始講 GROCK了那在這個影片中呢你會看到 有一個人在一個這個聊天機器人 的介面問這個聊天機器人一個 問題好那這個聊天機器人 這個LLM呢當然就是 跑在GROCK的硬體上面 這樣然後他問完這個問題 之後哇那個回答簡直 是跟閃電一樣一兩秒鐘 整篇文章的回覆全部 都產生了這個就是跟一般 你在使用這些ChatGBT
(19:04~20:04) Gemini是有非常大的 對比啊對吧你看ChatGBT你問他 一個問題他你還是 看得出來他一字一字跑出來的 樣子嘛那這個GROCK的回覆呢 真的就是你一眨眼整篇文章 就全部出來了真的是非常的 快速那我們也拿一個實際的 數字來比較一下好了那我們在 比較這些LLM的推論的速度 的時候我們用的一個指標 是TOKENS PER SECOND就是 這個LLM呢他一秒鐘 能夠產生多少個TOKEN 好那一個TOKEN呢就是大約 等於一個字或是一個標點符號 或是一個字的這個其中一部分 這樣有些字會被拆成兩個TOKEN 這樣那ChatGBT的TOKENS PER SECOND 大概是13個TOKENS PER SECOND 那這個你在GROCK 上面跑的這些模型呢 我們隨便抓一個LAMA2好了 LAMA2 70B的模型 他的TOKENS PER SECOND是 280個TOKENS PER SECOND 有些人可能會說欸哈利你這樣子比 不公平啊這個ChatGBT他 本身這個模型比較大一點 然後同時呢他們可能也
(20:04~21:04) 這個產品呢也沒有為TOKENS PER SECOND 做最優化 那一個更好的比較呢可能是拿GROCK 跟其他的這些API PROVIDER 做比較這些API PROVIDER 呢就是他們是在賣他們的 大型圓模型API 服務他們有自己的硬體 然後在這個硬體就是這些晶片 在這些晶片上面跑這些大型 圓模型然後用API的方式 把這些大型圓模型租給 其他企業使用嘛 GROCK在做的是一樣的生意只不過他們是 使用自己的晶片然後 自己的這一套Solution 那其他這些API PROVIDER呢 他們大部分人呢就是使用 NVIDIA的晶片在跑這些大型圓模型 那比較出來的結果 如何呢其實這些API PROVIDER他們的TOKENS PER SECOND 也跟GROCK是完全沒有辦法 比的GROCK比他們 通常是比他們快可能6倍 7倍左右然後原本最快 的這間公司叫Together AI 他們的TOKENS PER SECOND AVERAGE是65TOKENS PER SECOND 跟這個GROCK的185
(21:04~22:04) 也是差了3倍 而且這個185TOKENS PER SECOND 我自己覺得有可能有一點低估啦 那因為我自己測了很多次 我覺得大部分其實都是超過 這個的像是我剛剛說就是 我自己測的時候LAMA270P 跑起來是280TOKENS PER SECOND 所以說這個 這個RANGE可能再更廣一點 那我最近其實看到有一些人對於GROCK 他的產品是有點誤解的 就是有一些人呢 誤把GROCK當作一個聊天機器人了 因為GROCK他們是有開放一個頁面 是可以讓所有人去 測試他們的晶片的效能的 他們就是建了一個類似 CHATGBT的這個介面我剛剛講的 就是他們影片GO VIRAL那邊 也就是他們這個介面上面的 結果這樣那有些人呢 不懂GROCK後面在幹什麼 他們看到這個介面他們就以為這是一個 新的聊天機器人公司他們覺得 就是繼CHATGBT GEMINI QUAD還有什麼 INFLEXION AI之後 又有一個新的這個聊天機器人出來了 然後這個聊天機器人呢
(22:04~23:04) 他的特點呢就是他非常的快 然後我也收到一些私訊就是有人問我說 這個GROCK好不好用啊 為什麼我測試了一下我感覺 他好像比CHATGBT笨一點還是什麼的 所以我就在這邊幫大家清楚一些 這些誤解啊就是GROCK 他不是一個聊天機器人的公司 他完全就是一個AI推論 AI推論引擎的公司 他是在做晶片跟晶片上面的軟體的Solution 那你之所以會看到這個聊天的介面呢 他只是一個GROCK的 Proof of Concept 就是他們要證明他們的這一套 Solution的能力 然後他上面跑的這些大型元模型呢 全部都是開源的這些大型元模型 包括META的LAMA2嘛 然後還有MISTRAL的MIXTRAL 80007B 那你如果覺得他笨呢就是這些模型的問題 這些開源模型畢竟他也是拿這種 這種off the shelf 就是他完全沒有經過其他的 任何的fine tune的這些模型 直接這個拿現成的這個模型 來跑 因為他們也不care這些模型的成效如何 他們只是想讓你看到
(23:04~24:04) 他們晶片跑這些模型可以跑得這麼快 好那解除了這些誤解之後呢 一些大問題就跑出來了 第一個呢就是GROCK他們究竟 發明了什麼的技術 為什麼可以跑這些大型元模型 跑得這麼快 那就是這個GROCK這間公司 會不會對於NVIDIA有產生威脅 他對於整個AI運算市場 究竟會產生什麼樣的影響 我覺得絕大多數人 在看GROCK這間公司 他們有的疑問應該就是這兩個 尤其是最近NVIDIA長成這樣 那我們就先從第一個問題 開始好了就是 GROCK他們究竟發明了什麼技術 為什麼可以跑得這麼快 那我覺得如果要一句話概括他們在做什麼 可以這樣講就是他們從AI出發 做出了一個專門為AI設計的 AI推論引擎 從頭到尾都重新設計 從這個Compiler到晶片 到Networking的技術 那我剛剛講的這三個東西呢 Compiler晶片跟Networking基本上就是你在 跑一個大型元模型的時候 會影響你的速度得三個
(24:04~25:04) 最重要的因素 你如果這三個東西可以做到最優化 那你就可以跑得非常的快 那這三個東西的任務呢分別是這樣 Compiler在做的事情呢 或者你中文把它稱作編譯器 比較習慣用英文我們就用英文講 Compiler在做的事情呢就是 他要把你的AI工程師 寫出來的這些高層次的AI的指令 就是他可能是寫 Python的用PyTorch 寫的這些AI的程式碼 要把這些高層次的程式碼呢 轉變成電腦可以讀得懂的 可以執行的指令 通常我們會把它稱作 Machine code這樣 那其實在這個Compiler的過程中呢 Compiler也會做一定程度的優化 把這個程式碼呢 變成最適合他現在在跑的 這個硬體的指令 那嚴格來說呢在跑這些程式的過程中 有時候也不只用到一個Compiler 有時候會有兩個Compiler 比如說一個Python的Compiler 然後還有一個是專門Compile Kuda Kernels的這個Kuda的Compiler 這邊再下去就更技術了
(25:04~26:04) 但是反正你要記得的就是 Compiler在做的事情 就是把這些高層次的程式語言 翻譯成電腦讀得懂的程式語言 而且在翻譯的過程中呢 會盡量讓這些硬體能夠 最有效的執行這些指令 那接下來第二個我們說到的是 晶片嘛,那晶片這個就不用多講 他就是這個在跑AI的 這些處理器的本身嘛 那這個晶片本身他的設計如何 也會很大程度 很大程度的影響你跑AI的速度 那最後我們講到的是這個 Networking的技術 那這個Networking的技術呢就是 當你要訓練,你要跑的模型是非常大的時候 你可能要把很多個晶片給串在一起嘛 那這個Networking的技術呢 就是把這些晶片串在一起的技術 這個技術如果越好 也就是說這些晶片能夠串在一起 串得越好,能夠更合作無間的話 那你就可以跑得越快 那這三個會影響 跑AI模型速度的這個重要面向呢 Grock他們都有重新設計過 而且他們在設計的過程中呢 是同時一起設計的
(26:04~27:04) 就是他並不是像MVD 一樣,他們是先有他們的GPU出來 然後再為這個GPU設計 這個Cuda的軟體嘛 他們不一樣,他們是全部一起設計 那他們在設計的時候呢 老實說啊,一開始也不是為了AI 做準備 就是因為他們一開始是在2016年 就成立了嘛 那那個時候就是AI8自己還沒一撇 對吧,嚴格來說 是有一些跡象出來啦 就是可能Alpha,AlphaGo已經出來了嘛 但是對於這個LLM來說呢 都是都還沒有出現這樣 那他那個時候設計 其實是為了一些 像是高頻交易這樣子的 應用場景設計的 他們那時候設計的理念就是他們是專門處理 sequential的workload 像是這個LLM本身 他就是一個sequential性質的 的一個workload對吧 我們頻道常常講到他是一個 sequence to sequence model 他是從一個sequence預測另外一個sequence 然後預測另外一個sequence的過程呢 他是一個token一個token產生這樣
(27:04~28:04) 所以那時候這個LLM出來的時候 他們也算是中了頭獎 就是他們的這些晶片 真的完全完美的 可以跑這些LLM這樣 然後他們也因此把他們的晶片給改名了 他們原本晶片叫做什麼 TSP還是什麼東西 那他們現在把它變成LPU Language Processing Unit 雖然說一開始呢根本就是 沒有language in mind 好那接下來我們來細講一下 具體來說 Grok在他們的compiler他們晶片跟 networking究竟做了哪一些 革新呢這邊呢 我想先從晶片開始講 雖然說我剛剛有說嘛他們其實 這些東西是一起設計的然後 硬要說的話其實是從軟體先開始 他是先從compiler先設計 然後才去把他的這個 晶片的設計給finalize 但是我們先從晶片講我覺得大家比較好理解 那他們的晶片呢他是 把他取一個名字叫做LPU Language Processing Unit 剛剛也有講到嘛 那這個他會取叫做LPU呢很明顯就是要跟GPU
(28:04~29:04) 做一個區隔 那這樣子的取名其實也合理 因為他確實是在設計的方面呢 是跟GPU是差 蠻多的這樣那我們先從他們 比較一樣的部分開始講好了 他們一樣的部分呢就是在於 他們都是專門設計來處理一些 matrix math就是這個 矩陣數學矩陣乘法 這些運算的這個 處理器這邊當然不意外啦 因為你如果要跑AI軟體的話 你當然是需要非常大量的矩陣乘法 的運算嘛當然除了這些以外 還有其他的但是這個 最主要的呢就是矩陣乘法 那這個LPU的晶片呢是Grock 跟Marvel一起設計的 然後他們使用的呢是Global Foundry 14奈米的製程 有些人聽到就是可能會你對於 半導體有點概念的你就會覺得 14奈米怎麼感覺 有點古老的感覺 NVIDIA呢他們都已經用台積電 4奈米的製程在做他們的晶片了 那我是不知道他們用這個14奈米 的具體原因是什麼 可能就是那時候各種的
(29:04~30:04) 限制做出的決定 我不確定他那時候這個設計 晶片是什麼時候但反正呢 他們現在已經有在跟三星在談 4奈米的製程了所以說 這部分呢你可以把它想像成 他是有一隻手綁在身後 那再來我們要講一些就是他跟 GPU比較不一樣的地方了 我們這邊直接拿他跟NVIDIA的A100 H100做比較好了 首先有一個最不一樣的地方呢就是 他們使用的記憶體是一個叫做 SRAM的記憶體 那這些GPU呢不管是A100 H100他們使用的都是D RAM的 記憶體嚴格來說是一種叫做 HBM High Bandwidth Memory 的D RAM那這個D RAM跟 S RAM的差別在哪裡我們做一個 快速的比較這個S RAM 比D RAM快非常的多 他在存取資料的時候是快很多的 但同時呢他是比較貴的 然後比較不能夠在你的處理器 旁邊疊一大堆S RAM 那相較於這個D RAM呢 就是完全相反嘛他 存取資料的速度是比較慢的 但是他比較便宜然後你可以疊
(30:04~31:04) 很多在同一張這個 GPU上面所以說呢這個LPU 用S RAM取代D RAM他確實 得到一個快非常多的記憶體 他的資料傳輸的速度 也就是他的頻寬有80 TB per second 這個速度呢是比NVIDIA的H100快100倍喔 真的是快非常多 但是同時啊他也是 有做出一些犧牲就是 每一張晶片周圍能夠 擺的這個記憶體的數量呢 容量變得小非常多 那一張LPU的 晶片只會有230Mb 的S RAM 那一個NVIDIAH100的GPU呢 有80GB 的HBM那這樣大概是 小了300多倍好所以說 LPU呢有比GPU快100倍 的記憶體但是 有小300多倍的容量 那給大家一個概念好了就是 我們知道在跑AI模型的時候 這些記憶體啊在做的事情就是 你在跑這個模型的時候 你必須要把這個模型的權重 放到這個記憶體上面
(31:04~32:04) 你的這個處理器呢才能夠 很快的使用這個模型 的權重做推論嘛 那這個也就是說你在跑一個 AI模型的時候你必須 要有至少超過這個 模型大小一點點的這個 RAM那我們用這個 Mixtrol 8707B的這個AI 模型做舉例好了今天如果要跑這個 模型你要的H100 大概是兩張那因為這個 模型它的大小是接近100GB 那你用兩張的這個 H100呢你就有160GB 的記憶體就非常的足夠了 然後你如果是要在這個 LPU上面跑這個 模型呢你會需要576 LPU 兩張相對於576張 是差非常多的那最後呢 這個LPU跟GPU還有一個很大的 差別就是LPU 它的結構是非常 簡單而且deterministic 的它是deterministic 的hardware那GPU呢 是non-deterministic的 deterministic這個詞呢中文
(32:04~33:04) 我看了一下應該是翻成確定性的 很確定的 但是我覺得這個翻譯沒有說 很精準啦我來講解一下 它的意思好那這個LPU 是deterministic意思就是說 這個LPU在運算的時候 它的每一個指令什麼時候做完 每一個資料從就存在 哪裡然後從哪裡流到哪裡要花 多少時間全部都是非常 確定非常精確的 相對於GPU呢它是 non-deterministic的因為它的結構 是比LPU複雜很多的 究竟複雜在哪裡這邊 我們就不細講了但反正它很 複雜因為它的這個設計呢 是比較general purpose的 它並不是專門為了AI設計 的一個處理器 它是原本是為了這個 圖像然後才被 老黃變成一個比較general purpose的processor但反正它的結構 是比較複雜的這就導致你在 使用GPU的時候很多 事情是你沒有辦法預測 的這些資料是存在level 1 cache還是level 2 cache
(33:04~34:04) 這些這麼多核心同時 都要向這個HBM 要求存取資料 誰可以使用這些頻寬這邊就有一些資源 的衝突嘛所以說GPU 它在運算的時候其實是有一些 資源沒有辦法善用的問題 有可能有一些核心 被延遲了他們要做的事情 被延遲了因為他們在等其他核心把 事情做完之類的那這個LPU呢 因為它是deterministic的 所以說它可以把這個資源運算資源 運用到極致你們可以想像 一個畫面就是在 我們現在的這個交通的世界 就是你想像下班時間 然後在城市裡面的主要 道路那那個交通 然後尤其是下雨的時候 那個交通是非常沒有效率的 很容易堵塞然後前面 一個人慢後面就所有人都跟著慢 那這個就是GPU處理 AI運算的時候的狀況 然後你今天想像一個另外一個 世界所有的車子全部 都是AI自動駕駛 100%AI自動駕駛然後 有一個指揮中心在分配所有人
(34:04~35:04) 要走的路然後要開的速度 就他們會吃每個人要去 哪裡然後 算出一個最有效的方法就是 利用道路最有效的方法 是可以讓每一個人都在 最短的時間內達到他們想去的地方 而且這個精準度是極高的 這些車子會怎麼走完全沒有 任何的不確定性都是確定的 就好像任何物理 任何意外全部都可以model出來的這種 概念所以說不需要 任何的交通號制因為所有車子會 怎麼走全部都是由中央 指揮中心都已經算好的 然後這些車子都可以以 幾乎是最快的速度去進行 因為他們都已經算過了 這樣子走不會撞到任何人 那這個情景AI智慧程式 就是LPU在運算AI 的樣子所以說有 deterministic的特性讓GROK 可以完整的發揮LPU 它運算的效能 所有的頻寬然後所有的運算 都可以發揮到極致 以上就是GROK的LPU的介紹 最主要的重點就是他們使用SRAM
(35:04~36:04) 跟他們是deterministic的hardware 除了這些以外 也有一些設計上的差異 這邊就不細講了 再來我們來講講GROK的Compiler GROK的Compiler就是跟其他的Compiler 一樣它的最終目的 就是把使用者的程式碼 轉變成機器可以 跑的語言但是他們在Compile 的過程也就是說 他們在Pytorch非常高層次的語言 跟比較低層次的語言 像是Kernels之間 它又增加了一層 叫做GROK G10 Ops的東西 這個GROK G10 Ops 就是十個最基本 最優化的運算 為誰優化當然是為了LPU優化的運算 這十個運算是 LPU能夠最好的處理 最快的處理的十種運算 這十種運算 基本上已經涵蓋了所有這種 你在做AI深度學習 會使用到的數學 很多人不知道就是 大家在看這些AI軟體的時候都會覺得 哇他們這麼厲害
(36:04~37:04) 他背後在算的數學一定是極度 困難複雜的數學 但其實事實並不是這樣 他們在算的數學其實是非常單純的 當然你要了解他背後所有的理論 是非常複雜的 就是這個Multi-Head Self-Attention是什麼 這個Position Embedding是什麼 這個Multi-Layer Perceptron是什麼 但是你如果不看這些專有名詞 你單純看他在電腦上 要執行的這些數學指令 你會發現這些複雜的概念包裝 裡面的這些數學運算 其實真的都是很單純的 就是一些基本的這個 矩陣乘法的運算 然後還有一些可能Sine Cosine 然後還有Activation或者是一些 Normalization這樣的運算 很多這些都是非常單純的數學函式 尤其是你只是在跑推論的時候 就是這個Grok的LPU 他只能拿來跑推論 他不能拿來訓練AI模型 但你在跑推論的時候 你要用到數學運算又更少了 你就不用用一些什麼偏微分的東西 所以說Grok他就做十個數學運算 然後把它稱作Grok G10 Ops
(37:04~38:04) 然後把所有這些 大型元模型他在進行推論的時候 會要進行的數學運算 全部Map到這十個運算裡面 所以就變成是 你原本在PyTorch裡面寫這個 模型的時候 PyTorch定義你要做的運算可能有一千種 但是這個Grok 他直接把這一千種運算 全部Map到十種裡面 這個Grok G10 Ops裡面 那這個LPU 變成只要執行這十種 被最優化過的運算 就行了 那Compiler就介紹到這邊 當然就是一樣很多細節的部分 我沒有辦法講到 但就是給大家一個這樣子的High Level的Overview 那最後就是來講講Grok在Networking 這部分的一些技術 那這部分技術呢 其實很多都是電機在他的 軟體跟硬體的實力之上 就是這個Grok的 G10 Ops Compiler 還有Grok他的LPU 是Deterministic的這個重點 當你每一個硬體在做的事情都是
(38:04~39:04) 非常單純而且好預測的時候 都是確定的時候 這時候你當然可以最佳的 使用你所有的資源 然後同時他們在Networking這邊也有一些 像是他們這個Dragonfly Topology 那這個Dragonfly Topology 就是他們怎麼樣排列 這些LPU的方式 然後他們還有做一些像是 Router跟Processor的Combination 他們把這個Router合在他們的Processor裡面 那這些東西呢 他整體為Grok做出來的成績 是Grok晶片到晶片之間的 頻寬呢是差不多30GB 也就是30GB這樣 那NVIDIA的H100是高非常多的嘛 NVIDIA H100他們有用他們 他們自己的MVLink的技術 那可以做到差不多 900GB per second 但是Grok在做AI運算的推論的時候 他使用這個頻寬的 效率是比這個 H100、A100是高非常多的 注意喔這個是在推論的情況下 才成立當你今天 是在訓練AI模型的時候 你的Batch Size會大非常多
(39:04~40:04) 那這個時候呢這個 A100、H100呢他就可以運用 他的頻寬到很高的程度 但是在Batch Size很小的時候 也就是說你一次模型 只會跑個位數的運算的時候 這個時候Grok的LPU 他的使用效能是高 真的高非常多 那我們這邊做個快速的總結 反正Grok在做的事情呢就是 他們在晶片這邊他們設計出了一個 Deterministic的LPU 然後同時呢搭配著他們 專門為LPU設計的這個 Grok Compiler 然後還有他們的一些Networking的技術 像是這個Dragonfly Topology 他們同時把AI的運算最簡化 然後同時把硬體的這個 使用效率最大化 也因此讓他們能夠做到這個 500 Tokens Per Second這種 可怕的成績 那這邊要注意的一點呢就是 Grok的晶片呢目前只能跑 AI的推論AI有分 推論跟訓練嘛我現在才講 這個會不會太晚 我是耳熟大家都已經知道了
(40:04~41:04) 因為我前面的Podcast也講過很多次了嘛 但反正AI有分訓練跟推論 訓練的部分呢就是 你在訓練AI模型 讓他學會世界的知識的這個過程 那推論呢就是你 使用這個已經被訓練好的AI 去實際的幫你 給出一個答案幫你解決 問題的這個過程 那Grok的晶片呢以及他們的技術 目前只能拿來做AI的推論 訓練這邊是完全不行的 但是他們在AI的推論這邊 我覺得他們真的是做到了世界第一 就是比這個NVIDIA的還快 而且他其實不只是快而已喔 他的能源效率 也比NVIDIA的高非常的多 他有一個數據就是 他的Jewel Per Token 就是沒產生一個Token 他使用的Jewel量就是這個能量啦 是NVIDIA GPU的 十分之一然後他們的API價格 也是非常便宜的 我們拿Mixtrol來舉例好了 他們跑Mixtrol這個模型的價格 是0.27 美元
(41:04~42:04) 你沒產生一Million的Token 你只要花0.27美元 這個價格在市場上 也幾乎是最低的是非常 有競爭力的那這邊當然很多人 在算這個所謂的Grok 的Tokenomics 最近很炫炮的一個詞 叫Tokenomics就是 在跑LM的 推論的引擎呢 背後的這個成本結構是怎麼樣 那這邊當然就只能是 就是大家的猜測了很多人是很認真 建了一些財務模型在分析 這些事情但是很多裡面的 這些Assumption 這些參數都是大家隨便亂猜的 所以說我們不知道這個Grok 他訂這麼便宜的價格究竟 是在狂燒VC的錢 還是就是真的他們在成本上 是非常有競爭力的所以說 很多人就開始討論一件事情 就是Grok對於NVIDIA 的威脅究竟有多大 那接下來這邊當然就是比較 商業分析的部分 先跟大家說絕對不是任何的財務 的建議不要因為我
(42:04~43:04) 去買一些股票或是賣一些股票 我覺得純粹跟大家分享我的 想法這樣而已OK 那我自己是認為Grok 對於NVIDIA的威脅絕對是 需要被考慮的當然我現在 不會說這個威脅非常大Grok 也是一間可能1Billion 2Billion Valuation的這個 小新創跟NVIDIA這種 兩兆的公司是不能比的 但是他們確實 在這個技術上面 有一些真的是做得比NVIDIA好的 那首先就是在推論 上面NVIDIA我覺得 真的比不過Grok Grok的速度真的是太快了 然後你今天如果要NVIDIA做到 類似的速度或者是至少把NVIDIA 他的推論的速度 推到最大這個時候NVIDIA 成本會比Grok高非常多 這部分的分析我是看Dylan Patel 的Semi-Analysis Report 這個Semi-Analysis 就是一個專門分析半導體的一間 顧問機構 我覺得算是現在最有名的 這個顧問機構
(43:04~44:04) 他們有出這些免費的Report 當然只有一部分免費後面我就沒有看 但免費的部分你就會看到 這張比較的圖 結果就是Grok勝利 這邊有經驗的分析師就會說 哈利你說這個Grok在推論上 是贏過NVIDIA的 但是推論跟訓練的AI運算 是怎麼樣分佈的 這整個AI運算的市場 有幾%是推論幾%是訓練 這邊我們當然很難抓一個明確的數字 出來但是我認為 推論的運算量絕對是 比訓練大很多的 我們有一個Quote 可以參考的就是微軟的CEO Satya Nutella 他有說 就是好以前都不好笑 反正他有說就是 他說在Azure上面的AI推論 是遠遠大於AI訓練的運算 假設他在講的是 全部Azure的運算分配的話 這個其實是蠻驚人的 因為你要知道 OpenAI的模型都是在Azure上面訓練的 包括他們最近出的Sora
(44:04~45:04) 也都是在Azure上面訓練的 這麼多模型的訓練量 竟然跟推論比起來 還是推論大非常多 由此可見AI推論是更大的市場 然後還有另外一點要考慮的就是 Grok的進步應該會蠻高的 至少在這一兩年內是會非常快的 應該是會比NVR的速度更快 這是因為 首先第一個我一開始有講 就是他們有一隻手綁在後面 就是他們使用14奈米的製程 他們明年就會換成 三星4奈米的製程 那這一部分就是 他們的效能又會提升 然後另外一點就是 Grok這兩個月爆紅之後 他們的開發者生態系 真的是在快速成長中 在二月之前沒有人聽過Grok Grok一個客戶都沒有 但是在這兩個禮拜呢 Grok爆紅之後 Grok開始有了很多的客戶 然後Chamoth 也就是All in Podcast的主持人之一 Chamoth Polyhapetia 他也在這個禮拜的
(45:04~46:04) All in Podcast上面說 他說Grok現在已經有超過 一萬個Developer了 有一萬個開發者正在使用Grok的 科技在開發軟體 Chamoth會知道當然就是因為他是 Grok的早期投資人 Grok是他一手養大的公司 反正那不是重點 重點是Grok成長很快 雖然說他在一兩個禮拜內 有超過一萬個Developer 還是跟NVIDIA的 CUDA有超過四百萬個Developer 這是完全不能比的 但是他們至少在可以看得出來 在一兩年內呢他會成長 極快那NVIDIA當然一直以來 也是以他們的開發速度聞名 他們有一陣子呢 甚至就是每一年會有兩場GTC 就是有兩次的 發表會這樣 你有看過蘋果有兩次的 WWDC嗎或是這個 Google有兩次的Io 不可能嘛但NVIDIA可以在一年 之內搞出兩次的發表會 他們開發的速度真的是很快 然後也真的是有世界上
(46:04~47:04) 最厲害的一個Team 但是這個Grok我覺得絕對是不能小看的 然後應該很多人 在追這方面的新聞的人都會知道 就是Grok並不是 在AI推論想要 威脅NVIDIA地位的公司之一 許多的科技巨頭 也都有在研發自己的AI推論 的晶片像是比較有名的 就是微軟的Maya, AWS的 Inferentia跟Trainium 然後還有AMD的這個MI300X 那這邊啊 這所有人當中啊 其實我覺得Grok最有 可能產生最大的威脅 就是因為首先啊 AMD它基本上就是走NVIDIA的路線 而已然後只是走得比較慢 它就是一樣是用GPT 在啊講錯 用GPU在跑AI模型 然後它走得比較慢是因為它開始的 比較慢人家老黃 早在2006年就開始搞 CUDA然後2012年就開始 搞一些AI的韓式庫什麼 Cudnm, Cublast那些東西 那AMD開始的比較晚
(47:04~48:04) 他們有一個CUDA的相對應 產品叫做ROCKCAM 但這個ROCKCAM呢就是 目前開發進度是跟CUDA比 是慢非常多的然後同時 他們也是為了要加快進度呢 又把這個ROCKCAM給開源 讓大家所有的這些 開源的工程師們一起來 幫他搞但是他們還是 在追尋的這個NVIDIA 的路線那你們要知道 Grok是走完全不一樣的路線 它不是GPU它是LPU 所以我覺得AMD是沒有辦法 像Grok這樣威脅到NVIDIA的 然後其他那些 科技巨頭像是微軟的Maya AWS的那些東西 他們我自己覺得 還並不是很大的威脅 因為你知道這種晶片我們把它稱作ASIC 就是Application Specific IC 這些晶片 它的開發是 需要很長一段時間的 它並不是有錢就夠囉 這些科技巨頭都不缺錢 但並不是有錢就可以 開發出一個好東西
(48:04~49:04) 這些東西真的要花時間 你看這個Google的TPU 從2016年吧開始做 做到現在做到第五代了 它才有一點樣子出來 也不能這樣說啦 但它真的是會需要一年一年的時間 去慢慢進步的東西 那這些其他科技巨頭的晶片 都是比較近期才開始開發的 可能20年22年之類的 才開始做 相對於Grok呢 Grok它是2016年就開始設計了 而且別忘了 Grok有一個很大的優勢就是 Grok它的創辦人 是Google TPU的創辦人 Google TPU的發明者 所以說它是 真的是一個AI的ASIC的專家 所以說我這邊做的一個小節 就是我覺得Grok對於NVIDIA 的威脅真的是蠻大的 而且可能是所有 在威脅NVIDIA的公司裡面 威脅最大的一個 大家對於Grok還是有一些質疑的 首先第一個最大的質疑 就是我剛剛其實有講到
(49:04~50:04) 就是Grok的Tokenomics 到底是不是make sense 他們到底是在亂燒VC的錢 還是真的成本就是這麼低 這邊我們當然很難去 真的去驗證啦 但是我自己是覺得 很多人呢很懷疑這一點 可能是覺得就是Grok 有點too good to be true的感覺 怎麼可能它的推論速度比NVIDIA 快這麼多能源消耗又少這麼多 而且還更便宜 尤其他們使用500多個晶片 才比NVIDIA 兩個H100 怎麼可能這樣子真的是更便宜 那我這邊也沒有額外資訊可以提供 所以我也沒有辦法給大家一個結論 另外一個大家質疑 Grok的一點就是 你要這麼快的推論到底要幹嘛 你的速度已經遠遠超過 人類可以閱讀的速度極限了 對吧那這樣有意義嗎 但其實這些人呢 他們是沒有看到 我覺得是big picture 那big picture就是其實有很多 AI的應用呢是會需要你
(50:04~51:04) 多次的進行infrains 來幫你解決一個問題的 就比如說AI的agents好了 這是最經典的例子 一個AI的agents你給他一個 高層次的任務他要自己 多次的prompt自己把這個任務 進行拆解然後去搞清楚 這個任務的所有細節跟 所有子任務的所有內容 他才可以幫你把這個任務給完成 那這個過程中呢 你問他一個問題他可能要 自己prompt自己好幾十次 那這個過程中如果你的推論 是很快的整個過程會變得 非常快然後另外一種呢就是 我們所謂的chain of thought prompting 就是很多人在研究怎麼樣讓 LLM有更強的這個 reasoning的能力 更強的這個推理的能力這樣 那大家發現呢我們如果 把LLM結合一個search的 演算法讓他可以先在 回答一個問題之前呢 先可能產生出很多個問題 很多個答案然後 自己再去慢慢找出哪個答案是 最適合進一步去
(51:04~52:04) 探索的然後再進一步探索 一樣產生很多個答案這樣 那這樣子的過程嚴格來說是 chain of thought那這樣子的過程 你在解決一個問題的時候 你也會需要多次的進行 推論然後還有一些AI的應用是 對於即時是有要求的 就是比如說你今天可能要 即時的進行翻譯或者是 即時的進行這個 語音轉文字之類的這種操作 那你就是對於即時性有 很大要求嘛那所以說呢我覺得 這個LLM的推論的速度 絕對是未來這個 兵家必爭之地因為有 很多應用會需要非常快速 非常即時的推論然後 那個時候來了的時候這個 Grok非常有可能是王者 那NVIDIA有沒有機會可以 避免這件事情發生呢 然後持續做在寶座之上呢 這個當然也是有可能的啦 我們知道NVIDIA 他最近好像有在思考 要不要成立一個新的ASIC 部門就是專門 做AI推論
(52:04~53:04) 晶片的這種部門那這邊 當然就是沒有什麼太多公開資訊 可以參考但是NVIDIA 感覺也是沒有要袖手旁觀 的意思好那我們知道這個 NVIDIA他上禮拜 還是上上禮拜他們開出他們的 財報這個成績真的是 非常的漂亮非常亮眼 這個Q4的revenue 20幾billion嘛 那他們的估值呢也衝 到了兩兆美元 所以說我相信大家 目前對於AI的這個寶都是 壓在NVIDIA身上 但是如果你是NVIDIA的分析師 或者是你是NVIDIA的投資人 我這集介紹的Grok就給你 來參考一下你可以把這個東西 可能model進去看你要 怎麼樣調整你的估值之類的 那我個人是覺得這個Grok真的是 感覺對NVIDIA是 有蠻大的威脅的 尤其是他們在他們很多的突破 都是NVIDIA沒辦法 短期之內做到的 沒辦法就是靠軟體的一些 改變做到的因為他們是整個
(53:04~54:04) 硬體軟體Networking 全部整套的一個重新設計 NVIDIA無論如何是沒有 他們的A100H100 或者是他們接下來要出的B100 讓它變成Deterministic 的hardware它還是Non-Deterministic 好那今天的節目就 錄到這邊啦那我今天 其實有一點感冒不知道大家 聽不聽得出來然後我可能 有一點點鼻音所以說剛剛 我如果有一些話講起來 聽起來很奇怪就請大家 見諒希望下禮拜我的 感冒可以趕快好上禮拜 科技浪可以休假一週我也是 真的是非常的開心 去見到了很多我想見的人 雖然說台北的天氣呢 真的是非常的糟糕 糟糕透頂 但是人才是重點啦見到 想見的人才是最重要的 另外也跟大家說一下就是 有些人怕我就是上禮拜 沒有更新我就是要休更 很久沒有科技浪沒有要休 更就是我們接下來 還是每個禮拜都會有那下一次
(54:04~54:50) 休假是什麼時候不知道但 絕對是在可能兩個月之後之類的 到時候有休假再跟大家說 最後如果你喜歡今天的節目 你可以幫我五星評分然後分享給 你的朋友們我們科技浪 在Podcast平台啊真的是 沒有演算法的一個平台 所以會需要大家的這個分享 來讓更多人知道科技浪同時 也感謝今天的乾爹戰亞ZenYum 的隱形鴨套想要看看 的朋友別忘了使用這個 科技浪的專屬連結那如果你是 廠商然後想要贊助科技浪的 也可以到本集資訊欄裡面 找到科技浪的信箱 直接寄信給我就行了 在寄信之前你也可以參考一下 科技浪的網站裡面有很多 科技浪的受眾輪廓或者是 流量的資訊好那這裡就先聊 到這邊最後祝大家有個愉快的一周
