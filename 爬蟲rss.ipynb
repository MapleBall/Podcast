{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e34e2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\stanl\\.conda\\envs\\bojyun\\lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import feedparser\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "def sanitize_filename(filename):\n",
    "    # 定義要移除的非法字符集合\n",
    "    illegal_chars = set('/\\\\?%*:|\"<>.')\n",
    "\n",
    "    # 創建一個映射表，將所有非法字符映射為空字串\n",
    "    translation_table = str.maketrans('', '', ''.join(illegal_chars))\n",
    "    \n",
    "    # 使用 translate 方法來應用映射表，移除非法字符\n",
    "    sanitized_filename = filename.translate(translation_table)\n",
    "    \n",
    "    return sanitized_filename\n",
    "\n",
    "def download_mp3(url, folder_path, filename):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(os.path.join(folder_path, filename), 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"檔案下載完成: {filename}\")\n",
    "    else:\n",
    "        print(\"無法下載檔案\")\n",
    "\n",
    "def get_rss_file(newsurl, test_n):\n",
    "    \n",
    "    def get_title_name(url):\n",
    "        response = requests.get(url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            title = soup.find_all(\"title\")[0].text\n",
    "            return sanitize_filename(title)\n",
    "\n",
    "    if newsurl[13:21] == \"firstory\":\n",
    "        response = requests.get(newsurl)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            title = soup.find_all(\"title\")[0].text\n",
    "            title = sanitize_filename(title)\n",
    "            folder_path = title\n",
    "\n",
    "            if not os.path.exists(folder_path):\n",
    "                os.makedirs(folder_path)\n",
    "            \n",
    "            n = len(soup.find_all(\"item\"))\n",
    "            print(title, \"一共\", n, \"集\")\n",
    "\n",
    "            if n <= 2 or test_n == \"all\":\n",
    "                test_n = n\n",
    "\n",
    "            for i in range(test_n):\n",
    "                time.sleep(random.randrange(1, 5))\n",
    "                mp3_url = soup.find_all(\"item\")[i].find('enclosure').get('url')\n",
    "                name = soup.find_all(\"item\")[i].find('itunes:title').text\n",
    "                sanitized_name = sanitize_filename(name)\n",
    "                download_mp3(mp3_url, folder_path, sanitized_name + \".mp3\")\n",
    "\n",
    "    else:\n",
    "        file = feedparser.parse(newsurl)\n",
    "        title = get_title_name(newsurl)\n",
    "        folder_path = title\n",
    "\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "        \n",
    "        n = len(file[\"entries\"])\n",
    "        print(title, \"一共\", n, \"集\")\n",
    "\n",
    "        if n <= 2 or test_n == \"all\":\n",
    "            test_n = n  # 下載所有集數\n",
    "        else:\n",
    "            test_n = int(test_n)  # 確保 test_n 是整數\n",
    "            if test_n > n:\n",
    "                test_n = n  # 如果指定下載的集數大於實際集數，則只下載實際集數\n",
    "\n",
    "        for i in range(test_n):\n",
    "            time.sleep(random.randrange(1, 5))\n",
    "            try:\n",
    "                mp3_url = file[\"entries\"][i][\"links\"][1][\"href\"]\n",
    "            except:\n",
    "                mp3_url = file[\"entries\"][i][\"links\"][0][\"href\"]\n",
    "            name = file[\"entries\"][i][\"title\"]\n",
    "            sanitized_name = sanitize_filename(name)\n",
    "            download_mp3(mp3_url, folder_path, sanitized_name + \".mp3\")\n",
    "                    \n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "# 測試字符是否為中文\n",
    "def is_chinese(char):\n",
    "    # 判斷字符的 Unicode 編碼值是否在中文範圍內\n",
    "    if '\\u4e00' <= char <= '\\u9fff':\n",
    "        return True\n",
    "    else:\n",
    "        return False      \n",
    "    \n",
    "def get_all_category():\n",
    "    all_cata=['all',\n",
    "     'Arts',\n",
    "     'Arts / Books',\n",
    "     'Arts / Design',\n",
    "     'Arts / Fashion & Beauty',\n",
    "     'Arts / Food',\n",
    "     'Arts / Performing Arts',\n",
    "     'Arts / Visual Arts',\n",
    "     'Business',\n",
    "     'Business / Careers',\n",
    "     'Business / Entrepreneurship',\n",
    "     'Business / Investing',\n",
    "     'Business / Management',\n",
    "     'Business / Marketing',\n",
    "     'Business / Non-Profit',\n",
    "     'Comedy',\n",
    "     'Comedy / Comedy Interviews',\n",
    "     'Comedy / Improv',\n",
    "     'Comedy / Stand-Up',\n",
    "     'Education',\n",
    "     'Education / Courses',\n",
    "     'Education / How To',\n",
    "     'Education / Language Learning',\n",
    "     'Education / Self-Improvement',\n",
    "     'Fiction',\n",
    "     'Fiction / Comedy Fiction',\n",
    "     'Fiction / Drama',\n",
    "     'Fiction / Science Fiction',\n",
    "     'Government',\n",
    "     'Health & Fitness',\n",
    "     'Health & Fitness / Alternative Health',\n",
    "     'Health & Fitness / Fitness',\n",
    "     'Health & Fitness / Medicine',\n",
    "     'Health & Fitness / Mental Health',\n",
    "     'Health & Fitness / Nutrition',\n",
    "     'Health & Fitness / Sexuality',\n",
    "     'History',\n",
    "     'Kids & Family',\n",
    "     'Kids & Family / Education for Kids',\n",
    "     'Kids & Family / Parenting',\n",
    "     'Kids & Family / Pets & Animals',\n",
    "     'Kids & Family / Stories for Kids',\n",
    "     'Leisure',\n",
    "     'Leisure / Animation & Manga',\n",
    "     'Leisure / Automotive',\n",
    "     'Leisure / Aviation',\n",
    "     'Leisure / Crafts',\n",
    "     'Leisure / Games',\n",
    "     'Leisure / Hobbies',\n",
    "     'Leisure / Home & Garden',\n",
    "     'Leisure / Video Games',\n",
    "     'Music',\n",
    "     'Music / Music Commentary',\n",
    "     'Music / Music History',\n",
    "     'Music / Music Interviews',\n",
    "     'News',\n",
    "     'News / Business News',\n",
    "     'News / Daily News',\n",
    "     'News / Entertainment News',\n",
    "     'News / News Commentary',\n",
    "     'News / Politics',\n",
    "     'News / Sports News',\n",
    "     'News / Tech News',\n",
    "     'Religion & Spirituality',\n",
    "     'Religion & Spirituality / Buddhism',\n",
    "     'Religion & Spirituality / Christianity',\n",
    "     'Religion & Spirituality / Hinduism',\n",
    "     'Religion & Spirituality / Islam',\n",
    "     'Religion & Spirituality / Judaism',\n",
    "     'Religion & Spirituality / Religion',\n",
    "     'Religion & Spirituality / Spirituality',\n",
    "     'Science',\n",
    "     'Science / Astronomy',\n",
    "     'Science / Chemistry',\n",
    "     'Science / Earth Sciences',\n",
    "     'Science / Life Sciences',\n",
    "     'Science / Mathematics',\n",
    "     'Science / Natural Sciences',\n",
    "     'Science / Nature',\n",
    "     'Science / Physics',\n",
    "     'Science / Social Sciences',\n",
    "     'Society & Culture',\n",
    "     'Society & Culture / Documentary',\n",
    "     'Society & Culture / Personal Journals',\n",
    "     'Society & Culture / Philosophy',\n",
    "     'Society & Culture / Places & Travel',\n",
    "     'Society & Culture / Relationships',\n",
    "     'Sports',\n",
    "     'Sports / Baseball',\n",
    "     'Sports / Basketball',\n",
    "     'Sports / Cricket',\n",
    "     'Sports / Fantasy Sports',\n",
    "     'Sports / Football',\n",
    "     'Sports / Golf',\n",
    "     'Sports / Hockey',\n",
    "     'Sports / Rugby',\n",
    "     'Sports / Running',\n",
    "     'Sports / Soccer',\n",
    "     'Sports / Swimming',\n",
    "     'Sports / Tennis',\n",
    "     'Sports / Volleyball',\n",
    "     'Sports / Wilderness',\n",
    "     'Sports / Wrestling',\n",
    "     'TV & Film',\n",
    "     'TV & Film / After Shows',\n",
    "     'TV & Film / Film History',\n",
    "     'TV & Film / Film Interviews',\n",
    "     'TV & Film / Film Reviews',\n",
    "     'TV & Film / TV Reviews',\n",
    "     'Technology',\n",
    "     'True Crime']\n",
    "    all_category=[]\n",
    "    for i in range(len(all_cata)):\n",
    "        all_category.append((all_cata[i].replace(\" / \",\"-\").replace(\" & \",\"-\").replace(\" \",\"-\").lower()))\n",
    "    return all_category\n",
    "\n",
    "\n",
    "def get_name_href(category_name):\n",
    "    #抓到總排行榜   全部的網址 跟名稱\n",
    "    rank_list_herf=[]\n",
    "    rank_list_name=[]\n",
    "    # 目標網站的URL\n",
    "\n",
    "    url=\"https://rephonic.com/charts/apple/tw/\"+category_name\n",
    "    # 發送GET請求獲取網頁內容\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # 檢查是否成功獲取網頁內容\n",
    "    if response.status_code == 200:\n",
    "        # 使用BeautifulSoup解析HTML\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # 在這裡編寫你的程式碼來處理解析後的網頁內容\n",
    "\n",
    "        # 以下是一個示例，尋找所有<a>標籤並獲取其連結和文字內容\n",
    "        for link in soup.find_all('a'):\n",
    "            href = link.get('href')\n",
    "            text = link.get_text()\n",
    "            if len(text)>0 and is_chinese(text)==True:\n",
    "                #print(f\"連結: {href}\\t文字內容: {text}\")\n",
    "                rank_list_herf.append(\"https://rephonic.com\"+href)\n",
    "                rank_list_name.append(text)\n",
    "            else:\n",
    "                pass\n",
    "    else:\n",
    "        print(\"無法獲取網頁內容\")\n",
    "    return  rank_list_herf,rank_list_name\n",
    "\n",
    "\n",
    "\n",
    "def get_rss(url):\n",
    "    # 目標網頁的URL\n",
    "    #url = \"https://rephonic.com/podcasts/li-jing-lei-de-chen-jing-shi-jian\"\n",
    "\n",
    "    # 發送GET請求獲取網頁內容\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # 檢查是否成功獲取網頁內容\n",
    "    if response.status_code == 200:\n",
    "        # 使用BeautifulSoup解析HTML\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # 尋找包含RSS網址的元素\n",
    "        find_rss=soup.find_all(\"\",text=re.compile(\"@context\"))\n",
    "\n",
    "        # 要解析的 JSON 字串\n",
    "        json_str = find_rss[0]\n",
    "        # 解析 JSON 字串\n",
    "        data = json.loads(json_str)\n",
    "        # 提取 identifier 後面的文字\n",
    "        identifier_text = data[\"identifier\"]\n",
    "        print(identifier_text)\n",
    "\n",
    "    else:\n",
    "        print(\"無法獲取網頁內容\")\n",
    "    return identifier_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a566d30b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\stanl\\.conda\\envs\\bojyun\\lib\\html\\parser.py:171: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  k = self.parse_starttag(i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "老高與小茉 Mr & Mrs Gao 一共 349 集\n",
      "檔案下載完成: 347 如果這些都是肉，那麼一切就都說通了，一個神奇角度的假說揭開遠古巨石文明的秘密  老高與小茉 Mr & Mrs Gao.mp3\n",
      "檔案下載完成: 346 【萬分抱歉，這次的影片又晚了】上古三大奇書之山海經，這本書裡寫的的東西你都沒見過，甚至都無法想像，但它們卻有可能真實存在，只是不是五千年前，而是六千五百萬年前 老高與小茉 Mr & Mrs Gao.mp3\n",
      "檔案下載完成: 345 【萬分抱歉，這次的影片真的太晚了】目前為止最長的影片，最奇幻的經歷，一個被埋沒了三十八年的神人  老高與小茉 Mr & Mrs Gao.mp3\n",
      "檔案下載完成: 344 這個國家有五千年歷史，人口有十四億，但平均年齡只有295歲  老高與小茉 Mr & Mrs Gao.mp3\n",
      "檔案下載完成: 343 讓你睡不著的深海巨獸  老高與小茉 Mr & Mrs Gao.mp3\n",
      "檔案下載完成: 【限时回放】190 碰到就玩完了，世界上最可怕的十大海洋生物  老高與小茉 Mr & Mrs Gao.mp3\n"
     ]
    }
   ],
   "source": [
    "# #貼上rss即可下載。＃新資料夾\n",
    "# get_rss_file('https://feeds.soundon.fm/podcasts/ecd31076-d12d-46dc-ba11-32d24b41cca5.xml')\n",
    "# #史塔克實驗室\n",
    "# get_rss_file(\"https://feeds.soundon.fm/podcasts/e4f101be-289a-4101-bb11-59fc61e5c88b.xml\")\n",
    "# #達特嘴哥地圖砲 \n",
    "# get_rss_file(\"https://open.firstory.me/rss/user/ckcdy2bijlk7n0918zfcwxyyr\")\n",
    "\n",
    "#科技浪 \n",
    "# get_rss_file(\"https://feeds.soundon.fm/podcasts/03f4a53e-80cf-4a20-ad2c-bdb31a76c7b3.xml\",\"all\")\n",
    "#老高 \n",
    "get_rss_file(\"https://anchor.fm/s/3ba51528/podcast/rss\",\"20\")\n",
    "#Joe & Jet 未過濾的 with Jason\n",
    "get_rss_file(\"https://feeds.soundon.fm/podcasts/78a91a6a-5c6b-43cc-aaac-7918f792e5ae.xml\",\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a143fd6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    #先得到所有分類的名字\n",
    "    category_name=get_all_category()\n",
    "    #使用者輸入想跑多少分類\n",
    "    category_n=input(\"你想要跑多少種分類？ 輸入數字，或者all抓取全部\")\n",
    "    if category_n==\"all\":       \n",
    "        category_n=len(category_name)\n",
    "    else:\n",
    "        category_n=int(category_n)\n",
    "    \n",
    "    for i in range(category_n):      #這邊的迴圈  是指定跑幾個分類    \n",
    "        #得到該分類的所有節目名稱跟網址\n",
    "        rank_list_herf,rank_list_name=get_name_href(category_name[i])\n",
    "        #使用者輸入想跑多少節目\n",
    "        rank_list_n=5        #(\"一個分類想要抓取多少節目？ 輸入數字，或者all抓取全部\")\n",
    "        if rank_list_n==\"all\":    \n",
    "            rank_list_n=len(rank_list_herf)\n",
    "        else:\n",
    "            rank_list_n=int(rank_list_n)\n",
    "        \n",
    "        for j in range(rank_list_n):     #這邊的迴圈  是指定跑幾個節目\n",
    "            \n",
    "            #得到該節目的rss\n",
    "            rss_url=get_rss(rank_list_herf[j])\n",
    "            #下載所有節目\n",
    "            get_rss_file(rss_url,3)    #後面的後面的數字   是測試用的時候。要下載幾集。  ＃要全部的集數。 輸入\"all\" \n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
