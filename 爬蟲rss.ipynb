{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e34e2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "def sanitize_filename(filename):\n",
    "    # 定義要移除的非法字符集合\n",
    "    illegal_chars = set('/\\\\?%*:|\"<>.')\n",
    "\n",
    "    # 創建一個映射表，將所有非法字符映射為空字串\n",
    "    translation_table = str.maketrans('', '', ''.join(illegal_chars))\n",
    "    \n",
    "    # 使用 translate 方法來應用映射表，移除非法字符\n",
    "    sanitized_filename = filename.translate(translation_table)\n",
    "    \n",
    "    return sanitized_filename\n",
    "\n",
    "def download_mp3(url, folder_path, filename):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(os.path.join(folder_path, filename), 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"檔案下載完成: {filename}\")\n",
    "    else:\n",
    "        print(\"無法下載檔案\")\n",
    "\n",
    "def get_rss_file(newsurl, test_n):\n",
    "    \n",
    "    def get_title_name(url):\n",
    "        response = requests.get(url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            title = soup.find_all(\"title\")[0].text\n",
    "            return sanitize_filename(title)\n",
    "\n",
    "    if newsurl[13:21] == \"firstory\":\n",
    "        response = requests.get(newsurl)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            title = soup.find_all(\"title\")[0].text\n",
    "            title = sanitize_filename(title)\n",
    "            folder_path = title\n",
    "\n",
    "            if not os.path.exists(folder_path):\n",
    "                os.makedirs(folder_path)\n",
    "            \n",
    "            n = len(soup.find_all(\"item\"))\n",
    "            print(title, \"一共\", n, \"集\")\n",
    "\n",
    "            if n <= 2 or test_n == \"all\":\n",
    "                test_n = n\n",
    "\n",
    "            for i in range(test_n):\n",
    "                time.sleep(random.randrange(1, 5))\n",
    "                mp3_url = soup.find_all(\"item\")[i].find('enclosure').get('url')\n",
    "                name = soup.find_all(\"item\")[i].find('itunes:title').text\n",
    "                sanitized_name = sanitize_filename(name)\n",
    "                download_mp3(mp3_url, folder_path, sanitized_name + \".mp3\")\n",
    "\n",
    "    else:\n",
    "        file = feedparser.parse(newsurl)\n",
    "        title = get_title_name(newsurl)\n",
    "        folder_path = title\n",
    "\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "        \n",
    "        n = len(file[\"entries\"])\n",
    "        print(title, \"一共\", n, \"集\")\n",
    "\n",
    "        if n <= 2 or test_n == \"all\":\n",
    "            test_n = n\n",
    "            \n",
    "        for i in range(test_n):\n",
    "            time.sleep(random.randrange(1, 5))\n",
    "            try:\n",
    "                mp3_url = file[\"entries\"][i][\"links\"][1][\"href\"]\n",
    "            except:\n",
    "                mp3_url = file[\"entries\"][i][\"links\"][0][\"href\"]\n",
    "            name = file[\"entries\"][i][\"title\"]\n",
    "            sanitized_name = sanitize_filename(name)\n",
    "            download_mp3(mp3_url, folder_path, sanitized_name + \".mp3\")\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "# 測試字符是否為中文\n",
    "def is_chinese(char):\n",
    "    # 判斷字符的 Unicode 編碼值是否在中文範圍內\n",
    "    if '\\u4e00' <= char <= '\\u9fff':\n",
    "        return True\n",
    "    else:\n",
    "        return False      \n",
    "    \n",
    "def get_all_category():\n",
    "    all_cata=['all',\n",
    "     'Arts',\n",
    "     'Arts / Books',\n",
    "     'Arts / Design',\n",
    "     'Arts / Fashion & Beauty',\n",
    "     'Arts / Food',\n",
    "     'Arts / Performing Arts',\n",
    "     'Arts / Visual Arts',\n",
    "     'Business',\n",
    "     'Business / Careers',\n",
    "     'Business / Entrepreneurship',\n",
    "     'Business / Investing',\n",
    "     'Business / Management',\n",
    "     'Business / Marketing',\n",
    "     'Business / Non-Profit',\n",
    "     'Comedy',\n",
    "     'Comedy / Comedy Interviews',\n",
    "     'Comedy / Improv',\n",
    "     'Comedy / Stand-Up',\n",
    "     'Education',\n",
    "     'Education / Courses',\n",
    "     'Education / How To',\n",
    "     'Education / Language Learning',\n",
    "     'Education / Self-Improvement',\n",
    "     'Fiction',\n",
    "     'Fiction / Comedy Fiction',\n",
    "     'Fiction / Drama',\n",
    "     'Fiction / Science Fiction',\n",
    "     'Government',\n",
    "     'Health & Fitness',\n",
    "     'Health & Fitness / Alternative Health',\n",
    "     'Health & Fitness / Fitness',\n",
    "     'Health & Fitness / Medicine',\n",
    "     'Health & Fitness / Mental Health',\n",
    "     'Health & Fitness / Nutrition',\n",
    "     'Health & Fitness / Sexuality',\n",
    "     'History',\n",
    "     'Kids & Family',\n",
    "     'Kids & Family / Education for Kids',\n",
    "     'Kids & Family / Parenting',\n",
    "     'Kids & Family / Pets & Animals',\n",
    "     'Kids & Family / Stories for Kids',\n",
    "     'Leisure',\n",
    "     'Leisure / Animation & Manga',\n",
    "     'Leisure / Automotive',\n",
    "     'Leisure / Aviation',\n",
    "     'Leisure / Crafts',\n",
    "     'Leisure / Games',\n",
    "     'Leisure / Hobbies',\n",
    "     'Leisure / Home & Garden',\n",
    "     'Leisure / Video Games',\n",
    "     'Music',\n",
    "     'Music / Music Commentary',\n",
    "     'Music / Music History',\n",
    "     'Music / Music Interviews',\n",
    "     'News',\n",
    "     'News / Business News',\n",
    "     'News / Daily News',\n",
    "     'News / Entertainment News',\n",
    "     'News / News Commentary',\n",
    "     'News / Politics',\n",
    "     'News / Sports News',\n",
    "     'News / Tech News',\n",
    "     'Religion & Spirituality',\n",
    "     'Religion & Spirituality / Buddhism',\n",
    "     'Religion & Spirituality / Christianity',\n",
    "     'Religion & Spirituality / Hinduism',\n",
    "     'Religion & Spirituality / Islam',\n",
    "     'Religion & Spirituality / Judaism',\n",
    "     'Religion & Spirituality / Religion',\n",
    "     'Religion & Spirituality / Spirituality',\n",
    "     'Science',\n",
    "     'Science / Astronomy',\n",
    "     'Science / Chemistry',\n",
    "     'Science / Earth Sciences',\n",
    "     'Science / Life Sciences',\n",
    "     'Science / Mathematics',\n",
    "     'Science / Natural Sciences',\n",
    "     'Science / Nature',\n",
    "     'Science / Physics',\n",
    "     'Science / Social Sciences',\n",
    "     'Society & Culture',\n",
    "     'Society & Culture / Documentary',\n",
    "     'Society & Culture / Personal Journals',\n",
    "     'Society & Culture / Philosophy',\n",
    "     'Society & Culture / Places & Travel',\n",
    "     'Society & Culture / Relationships',\n",
    "     'Sports',\n",
    "     'Sports / Baseball',\n",
    "     'Sports / Basketball',\n",
    "     'Sports / Cricket',\n",
    "     'Sports / Fantasy Sports',\n",
    "     'Sports / Football',\n",
    "     'Sports / Golf',\n",
    "     'Sports / Hockey',\n",
    "     'Sports / Rugby',\n",
    "     'Sports / Running',\n",
    "     'Sports / Soccer',\n",
    "     'Sports / Swimming',\n",
    "     'Sports / Tennis',\n",
    "     'Sports / Volleyball',\n",
    "     'Sports / Wilderness',\n",
    "     'Sports / Wrestling',\n",
    "     'TV & Film',\n",
    "     'TV & Film / After Shows',\n",
    "     'TV & Film / Film History',\n",
    "     'TV & Film / Film Interviews',\n",
    "     'TV & Film / Film Reviews',\n",
    "     'TV & Film / TV Reviews',\n",
    "     'Technology',\n",
    "     'True Crime']\n",
    "    all_category=[]\n",
    "    for i in range(len(all_cata)):\n",
    "        all_category.append((all_cata[i].replace(\" / \",\"-\").replace(\" & \",\"-\").replace(\" \",\"-\").lower()))\n",
    "    return all_category\n",
    "\n",
    "\n",
    "def get_name_href(category_name):\n",
    "    #抓到總排行榜   全部的網址 跟名稱\n",
    "    rank_list_herf=[]\n",
    "    rank_list_name=[]\n",
    "    # 目標網站的URL\n",
    "\n",
    "    url=\"https://rephonic.com/charts/apple/tw/\"+category_name\n",
    "    # 發送GET請求獲取網頁內容\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # 檢查是否成功獲取網頁內容\n",
    "    if response.status_code == 200:\n",
    "        # 使用BeautifulSoup解析HTML\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # 在這裡編寫你的程式碼來處理解析後的網頁內容\n",
    "\n",
    "        # 以下是一個示例，尋找所有<a>標籤並獲取其連結和文字內容\n",
    "        for link in soup.find_all('a'):\n",
    "            href = link.get('href')\n",
    "            text = link.get_text()\n",
    "            if len(text)>0 and is_chinese(text)==True:\n",
    "                #print(f\"連結: {href}\\t文字內容: {text}\")\n",
    "                rank_list_herf.append(\"https://rephonic.com\"+href)\n",
    "                rank_list_name.append(text)\n",
    "            else:\n",
    "                pass\n",
    "    else:\n",
    "        print(\"無法獲取網頁內容\")\n",
    "    return  rank_list_herf,rank_list_name\n",
    "\n",
    "\n",
    "\n",
    "def get_rss(url):\n",
    "    # 目標網頁的URL\n",
    "    #url = \"https://rephonic.com/podcasts/li-jing-lei-de-chen-jing-shi-jian\"\n",
    "\n",
    "    # 發送GET請求獲取網頁內容\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # 檢查是否成功獲取網頁內容\n",
    "    if response.status_code == 200:\n",
    "        # 使用BeautifulSoup解析HTML\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # 尋找包含RSS網址的元素\n",
    "        find_rss=soup.find_all(\"\",text=re.compile(\"@context\"))\n",
    "\n",
    "        # 要解析的 JSON 字串\n",
    "        json_str = find_rss[0]\n",
    "        # 解析 JSON 字串\n",
    "        data = json.loads(json_str)\n",
    "        # 提取 identifier 後面的文字\n",
    "        identifier_text = data[\"identifier\"]\n",
    "        print(identifier_text)\n",
    "\n",
    "    else:\n",
    "        print(\"無法獲取網頁內容\")\n",
    "    return identifier_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a566d30b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "科技浪 Techwav 一共 48 集\n",
      "檔案下載完成: EP46 - 比Transformer更強更快的架構？深度解析SSM！真的能取代Transformer嗎？.mp3\n",
      "檔案下載完成: SP - AWS 將在台投資數十億美元！什麽是區域級資料中心？ft AWS解決方案架構師主管 王仕榮.mp3\n",
      "檔案下載完成: EP45 - 特斯拉的自動駕駛技術指數成長！5年股價翻14倍有機會嗎？.mp3\n",
      "檔案下載完成: EP44 - 蘋果終於有AI了？深入解析Apple Intelligence！.mp3\n",
      "檔案下載完成: EP43 - COMPUTEX的重量級CEO演講！NVIDIA到底領先競爭者多少？護城河是什麽？.mp3\n",
      "檔案下載完成: EP42 - 介紹Meta的最强多模態AI模型！AI到底是如何同時理解文字、圖像與音訊的？多模態AI的未來如何？.mp3\n",
      "檔案下載完成: EP41 - AI PC的革命來了嗎？深入瞭解微軟的Copilot+PC.mp3\n",
      "檔案下載完成: EP40 - 深入瞭解GPT-4o的突破以及Google一年一度的開發者大會！.mp3\n",
      "檔案下載完成: EP39 - AlphaFold 3來了！Google Deepmind最引以爲傲的模型「AlphaFold」究竟是什麽？.mp3\n",
      "檔案下載完成: EP38 - AI一年半進步140倍，怎麽做到的？邊緣AI的未來.mp3\n",
      "檔案下載完成: EP37 - Meta重磅推出最强開源AI模型Llama 3！到底有多强？爲何這麽強？爲什麽要開源？.mp3\n",
      "檔案下載完成: EP36 - 特斯拉的全自動Robotaxi服務，將在今年8月亮相？馬斯克做得到嗎？.mp3\n",
      "檔案下載完成: EP35 - 深入解析Google最新AI技術：MoD！一個技術同時把LLM變得更快、更省算力、還更好？.mp3\n",
      "檔案下載完成: EP34 - 量子電腦究竟是什麽？會如何顛覆世界？.mp3\n",
      "檔案下載完成: EP33 - AI現在做的音樂到什麽程度？Nvidia的人形機器人計劃是在做什麽？.mp3\n",
      "檔案下載完成: EP32 - 深入分析Nvidia最新晶片Blackwell！爲何能有這樣的進步？資料精度下降的意義爲何？.mp3\n",
      "檔案下載完成: EP31 - 深入分析AI軟體工程師Devin！會如何影響軟體工程的職涯前景？星艦第三次試飛！.mp3\n",
      "檔案下載完成: EP30 - Claude 3有什麽特點？真的是地表最聰明模型嗎？真的有自我意識嗎？.mp3\n",
      "檔案下載完成: EP29 - 深入分析世界最快AI晶片：Groq的LPU（超越Nvidia！）.mp3\n",
      "檔案下載完成: EP28 - 深入瞭解兩大AI突破：Sora & Gemini 15 Pro.mp3\n",
      "檔案下載完成: EP27 - 科技時事雜談：全都變成Gemini、Sam的7兆、Vision Pro.mp3\n",
      "檔案下載完成: EP26 - 腦内植入晶片能做什麽？運作原理是什麽？揭秘腦内晶片公司Neuralink.mp3\n",
      "檔案下載完成: EP25 - AI的機會、風險在哪？AI時代下給年輕人的建議  專訪 Google Deepmind 傑出科學家--紀懷新（下集）.mp3\n",
      "檔案下載完成: EP24 - AI, Gemini, LLM的推理能力  專訪 Google Deepmind 傑出科學家--紀懷新（上集）.mp3\n",
      "檔案下載完成: EP23 - AI的「iphone時刻」來了嗎？.mp3\n",
      "檔案下載完成: EP22 - AI機器人跟傳統的差在哪？能取代人類的機器人要來了嗎？.mp3\n",
      "檔案下載完成: EP21 - 2023十大最關鍵AI發展.mp3\n",
      "檔案下載完成: EP20 - 有了會顛覆天氣預報產業的AI，台灣有跟上嗎？.mp3\n",
      "檔案下載完成: EP19 - 有錢人不該存在？谷歌如何靠自家晶片訓練Gemini.mp3\n",
      "檔案下載完成: EP18 - Google的王牌AI模型Gemini，真的這麽神嗎？（Gemini Part1）.mp3\n",
      "檔案下載完成: EP17 - 馬斯克爲何破口大駡、我們距離火星殖民比你想的更近.mp3\n",
      "檔案下載完成: SP - 校長教授怎麽看AI？Feat 東華大學趙涵捷校長、陳偉銘教授.mp3\n",
      "檔案下載完成: EP16 - OpenAI事件始末、重大突破Q到底是什麽？.mp3\n",
      "檔案下載完成: EP15 - VR吃鷄真的太好玩、大家都怎麽管AI？有效嗎？.mp3\n",
      "檔案下載完成: EP14 - OpenAI的重磅發表會-DevDay，你必須知道這些重點.mp3\n",
      "檔案下載完成: EP13 - AI與偏見，AI產生經濟價值了嗎？.mp3\n",
      "檔案下載完成: EP12 - 有AI後，學語言還有價值嗎？AI現況報告--Research發展.mp3\n",
      "檔案下載完成: EP11 - 關於拜登的晶片禁令，你要知道這些.mp3\n",
      "檔案下載完成: EP10 - GPT4-V 到底有多犯規？背後的技術是什麽？有什麽價值？.mp3\n",
      "檔案下載完成: EP9 - Meta想帶給我們什麽未來.mp3\n",
      "檔案下載完成: EP8 - 特斯拉的人形機器人.mp3\n",
      "檔案下載完成: EP7 - Close不對，Open AI.mp3\n",
      "檔案下載完成: EP6 - 蘋果與它的邊緣AI.mp3\n",
      "檔案下載完成: EP5 - 生產力工具+AI，AI浮水印.mp3\n",
      "檔案下載完成: EP4 - 爲何是AI+GPU？Nvidia怎麽這麽賺？.mp3\n",
      "檔案下載完成: EP3 - Excel+Python，AI醫療.mp3\n",
      "檔案下載完成: EP2 - AI是起點還是終點？.mp3\n",
      "檔案下載完成: EP1 - （試播集）全世界一起做了一個美夢.mp3\n"
     ]
    }
   ],
   "source": [
    "# #貼上rss即可下載。＃新資料夾\n",
    "# get_rss_file('https://feeds.soundon.fm/podcasts/ecd31076-d12d-46dc-ba11-32d24b41cca5.xml')\n",
    "# #史塔克實驗室\n",
    "# get_rss_file(\"https://feeds.soundon.fm/podcasts/e4f101be-289a-4101-bb11-59fc61e5c88b.xml\")\n",
    "# #達特嘴哥地圖砲 \n",
    "# get_rss_file(\"https://open.firstory.me/rss/user/ckcdy2bijlk7n0918zfcwxyyr\")\n",
    "\n",
    "#科技浪 \n",
    "get_rss_file(\"https://feeds.soundon.fm/podcasts/03f4a53e-80cf-4a20-ad2c-bdb31a76c7b3.xml\",\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a143fd6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    #先得到所有分類的名字\n",
    "    category_name=get_all_category()\n",
    "    #使用者輸入想跑多少分類\n",
    "    category_n=input(\"你想要跑多少種分類？ 輸入數字，或者all抓取全部\")\n",
    "    if category_n==\"all\":       \n",
    "        category_n=len(category_name)\n",
    "    else:\n",
    "        category_n=int(category_n)\n",
    "    \n",
    "    for i in range(category_n):      #這邊的迴圈  是指定跑幾個分類    \n",
    "        #得到該分類的所有節目名稱跟網址\n",
    "        rank_list_herf,rank_list_name=get_name_href(category_name[i])\n",
    "        #使用者輸入想跑多少節目\n",
    "        rank_list_n=5        #(\"一個分類想要抓取多少節目？ 輸入數字，或者all抓取全部\")\n",
    "        if rank_list_n==\"all\":    \n",
    "            rank_list_n=len(rank_list_herf)\n",
    "        else:\n",
    "            rank_list_n=int(rank_list_n)\n",
    "        \n",
    "        for j in range(rank_list_n):     #這邊的迴圈  是指定跑幾個節目\n",
    "            \n",
    "            #得到該節目的rss\n",
    "            rss_url=get_rss(rank_list_herf[j])\n",
    "            #下載所有節目\n",
    "            get_rss_file(rss_url,3)    #後面的後面的數字   是測試用的時候。要下載幾集。  ＃要全部的集數。 輸入\"all\" \n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
